2016-09-29 21:31:36 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:13] : starting spark-streaming-kafka
2016-09-29 21:31:56 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:19] : readme lines: 95
2016-09-29 21:33:59 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:13] : starting spark-streaming-kafka
2016-09-29 21:33:59 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-09-29 21:34:10 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-09-29 21:34:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-09-29 21:34:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-09-29 21:34:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-09-29 21:34:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-09-29 21:34:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-09-29 21:34:11 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 50769.
2016-09-29 21:34:11 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-09-29 21:34:11 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-09-29 21:34:11 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-aee5b490-cd8e-4cf7-96f3-8646033e713e
2016-09-29 21:34:11 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-09-29 21:34:11 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-09-29 21:34:11 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @12231ms
2016-09-29 21:34:11 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-09-29 21:34:11 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@20f12539{HTTP/1.1}{0.0.0.0:4040}
2016-09-29 21:34:11 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @12356ms
2016-09-29 21:34:11 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-09-29 21:34:11 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.102:4040
2016-09-29 21:34:11 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-09-29 21:34:11 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50770.
2016-09-29 21:34:11 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.102:50770
2016-09-29 21:34:11 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.102, 50770)
2016-09-29 21:34:11 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.102:50770 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.102, 50770)
2016-09-29 21:34:11 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.102, 50770)
2016-09-29 21:34:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-09-29 21:34:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-09-29 21:34:12 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.102:50770 (size: 10.2 KB, free: 912.3 MB)
2016-09-29 21:34:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:19
2016-09-29 21:34:12 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-09-29 21:34:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: count at SparkApplication.java:19
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (count at SparkApplication.java:19) with 2 output partitions
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (count at SparkApplication.java:19)
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (file:///usr/local/Cellar/apache-spark/1.6.1/README.md MapPartitionsRDD[1] at textFile at SparkApplication.java:19), which has no missing parents
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 912.2 MB)
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 1897.0 B, free 912.2 MB)
2016-09-29 21:34:12 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.102:50770 (size: 1897.0 B, free: 912.3 MB)
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (file:///usr/local/Cellar/apache-spark/1.6.1/README.md MapPartitionsRDD[1] at textFile at SparkApplication.java:19)
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-09-29 21:34:12 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5305 bytes)
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1041 bytes result sent to driver
2016-09-29 21:34:12 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5305 bytes)
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-09-29 21:34:12 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 190 ms on localhost (1/2)
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-09-29 21:34:12 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 954 bytes result sent to driver
2016-09-29 21:34:12 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 19 ms on localhost (2/2)
2016-09-29 21:34:12 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (count at SparkApplication.java:19) finished in 0.220 s
2016-09-29 21:34:12 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-09-29 21:34:12 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: count at SparkApplication.java:19, took 0.328578 s
2016-09-29 21:34:12 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:19] : readme lines: 95
2016-09-29 21:34:12 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-09-29 21:34:12 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@20f12539{HTTP/1.1}{0.0.0.0:4040}
2016-09-29 21:34:12 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.102:4040
2016-09-29 21:34:12 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-09-29 21:34:12 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-09-29 21:34:12 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-09-29 21:34:12 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-09-29 21:34:12 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-09-29 21:34:12 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-09-29 21:34:12 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-09-29 21:34:12 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-f08cb502-44fa-4585-abdf-f5476dd4d40e
