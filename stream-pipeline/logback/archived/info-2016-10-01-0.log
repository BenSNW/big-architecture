2016-10-01 11:52:20 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:26] : starting spark-streaming-kafka
2016-10-01 11:52:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 11:52:40 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 11:52:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 11:52:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 11:52:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 11:52:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 11:52:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 11:52:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 60582.
2016-10-01 11:52:41 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 11:52:41 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 11:52:41 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-5d27d0b5-a7b6-4f1f-bca5-d32a1a9ecc21
2016-10-01 11:52:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 11:52:41 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 11:52:41 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @21607ms
2016-10-01 11:52:41 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 11:52:41 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 11:52:41 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @21772ms
2016-10-01 11:52:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 11:52:41 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 11:52:41 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 11:52:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60583.
2016-10-01 11:52:41 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:60583
2016-10-01 11:52:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 60583)
2016-10-01 11:52:41 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:60583 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 60583)
2016-10-01 11:52:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 60583)
2016-10-01 11:52:42 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 11:52:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 11:52:43 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:60583 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 11:52:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:33
2016-10-01 11:52:43 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 11:52:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 11:52:43 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:60583 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 11:52:43 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 11:52:43 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 11:52:43 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 11:52:43 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 11:52:43 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 11:52:43 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 11:52:43 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 11:52:43 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 11:52:43 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 11:52:43 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 11:52:43 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 11:52:43 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 11:52:43 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 11:52:43 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 11:52:43 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 11:52:43 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:60583 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 11:52:43 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:60583 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 278 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 248 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.303 s
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:60583 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 97 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 94 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.099 s
2016-10-01 11:52:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.630888 s
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:60583 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 51 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 51 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.053 s
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:60583 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 39 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 38 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.041 s
2016-10-01 11:52:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.127217 s
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:60583 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 13 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2332 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 41 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 40 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.043 s
2016-10-01 11:52:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.061878 s
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:60583 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 40 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:60583 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:60583 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:60583 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:60583 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 60 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:60583 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.062 s
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:60583 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 32 ms on localhost (1/2)
2016-10-01 11:52:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 32 ms on localhost (2/2)
2016-10-01 11:52:44 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 11:52:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.035 s
2016-10-01 11:52:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.124808 s
2016-10-01 11:52:44 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 11:52:44 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 11:52:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 11:52:44 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 11:52:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 11:52:44 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 60584.
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 11:52:44 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-0dbae28d-7151-4040-bb9d-480aa0f13034
2016-10-01 11:52:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 11:52:44 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 11:52:44 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 11:52:44 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 11:52:44 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @24791ms
2016-10-01 11:52:44 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 11:52:44 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 11:52:44 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 11:52:44 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60585.
2016-10-01 11:52:44 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:60585
2016-10-01 11:52:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 60585)
2016-10-01 11:52:44 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:60585 with 912.3 MB RAM, BlockManagerId(driver, localhost, 60585)
2016-10-01 11:52:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 60585)
2016-10-01 11:52:44 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 11:52:44 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 11:52:46 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 11:52:46 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 11:52:46 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<>
2016-10-01 11:52:46 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 11:52:46 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 11:52:46 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 11:52:46 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:60585 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 11:52:46 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from count at SparkApplication.java:67
2016-10-01 11:52:46 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 11:52:46 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 462
2016-10-01 11:52:47 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 353.415652 ms
2016-10-01 11:52:47 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.637139 ms
2016-10-01 11:52:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: count at SparkApplication.java:67
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 2 (count at SparkApplication.java:67)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (count at SparkApplication.java:67) with 1 output partitions
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (count at SparkApplication.java:67)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at count at SparkApplication.java:67), which has no missing parents
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 9.3 KB, free 912.1 MB)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.9 KB, free 912.1 MB)
2016-10-01 11:52:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:60585 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at count at SparkApplication.java:67)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 11:52:47 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5867 bytes)
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.22305 ms
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1796 bytes result sent to driver
2016-10-01 11:52:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 87 ms on localhost (1/1)
2016-10-01 11:52:47 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (count at SparkApplication.java:67) finished in 0.094 s
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at count at SparkApplication.java:67), which has no missing parents
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 7.0 KB, free 912.1 MB)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.1 MB)
2016-10-01 11:52:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:60585 (size: 3.8 KB, free: 912.3 MB)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at SparkApplication.java:67)
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 11:52:47 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, NODE_LOCAL, 5275 bytes)
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 11:52:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1873 bytes result sent to driver
2016-10-01 11:52:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on localhost (1/1)
2016-10-01 11:52:47 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 11:52:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (count at SparkApplication.java:67) finished in 0.020 s
2016-10-01 11:52:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: count at SparkApplication.java:67, took 0.171061 s
2016-10-01 11:52:47 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.548707 ms
2016-10-01 11:52:47 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 11:52:47 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 11:52:47 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 11:52:47 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 11:52:47 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 11:52:47 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 11:52:47 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 11:52:47 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 11:52:47 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 11:52:47 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 11:52:47 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-f590b69d-465a-41e5-9331-9e7221f67d53
2016-10-01 13:12:12 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:26] : starting spark-streaming-kafka
2016-10-01 13:12:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 13:12:17 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 13:12:17 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 63388.
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 13:12:17 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-4ed17646-5c9b-4d61-91ad-ff65f1ba65e9
2016-10-01 13:12:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 13:12:17 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 13:12:18 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6572ms
2016-10-01 13:12:18 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 13:12:18 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@620421fd{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:18 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6742ms
2016-10-01 13:12:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 13:12:18 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 13:12:18 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 13:12:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63389.
2016-10-01 13:12:18 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:63389
2016-10-01 13:12:18 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 63389)
2016-10-01 13:12:18 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:63389 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 63389)
2016-10-01 13:12:18 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 63389)
2016-10-01 13:12:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 13:12:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 13:12:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:63389 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 13:12:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:33
2016-10-01 13:12:19 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 13:12:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:63389 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:63389 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:63389 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 315 ms on localhost (1/2)
2016-10-01 13:12:20 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 262 ms on localhost (2/2)
2016-10-01 13:12:20 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.364 s
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:63389 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 97 ms on localhost (1/2)
2016-10-01 13:12:20 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 94 ms on localhost (2/2)
2016-10-01 13:12:20 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.099 s
2016-10-01 13:12:20 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.707939 s
2016-10-01 13:12:20 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:63389 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1968 bytes result sent to driver
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 46 ms on localhost (1/2)
2016-10-01 13:12:20 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 46 ms on localhost (2/2)
2016-10-01 13:12:20 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.049 s
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:63389 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 36 ms on localhost (1/2)
2016-10-01 13:12:20 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 35 ms on localhost (2/2)
2016-10-01 13:12:20 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.037 s
2016-10-01 13:12:20 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.116735 s
2016-10-01 13:12:20 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:63389 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 18 ms on localhost (1/2)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 22 ms on localhost (2/2)
2016-10-01 13:12:20 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.023 s
2016-10-01 13:12:20 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.046469 s
2016-10-01 13:12:20 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:63389 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 13:12:20 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 2 ms
2016-10-01 13:12:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 28 ms on localhost (1/2)
2016-10-01 13:12:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 13:12:20 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 48 ms on localhost (2/2)
2016-10-01 13:12:20 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.049 s
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:12:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 13:12:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:63389 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 13:12:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:63389 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 13:12:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 13:12:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:63389 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:63389 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 13:12:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:63389 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 13:12:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:63389 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 13:12:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 13:12:21 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 17 ms on localhost (1/2)
2016-10-01 13:12:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 13:12:21 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 20 ms on localhost (2/2)
2016-10-01 13:12:21 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 13:12:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.023 s
2016-10-01 13:12:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.101974 s
2016-10-01 13:12:21 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@620421fd{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:21 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 13:12:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 13:12:21 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 13:12:21 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 13:12:21 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 63390.
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 13:12:21 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-01137c20-46bd-4cfb-b8fb-46ee66ac2127
2016-10-01 13:12:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 13:12:21 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 13:12:21 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 13:12:21 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@28adf099{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:21 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @9631ms
2016-10-01 13:12:21 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 13:12:21 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 13:12:21 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 13:12:21 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63391.
2016-10-01 13:12:21 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:63391
2016-10-01 13:12:21 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 63391)
2016-10-01 13:12:21 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:63391 with 912.3 MB RAM, BlockManagerId(driver, localhost, 63391)
2016-10-01 13:12:21 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 63391)
2016-10-01 13:12:21 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 13:12:21 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 13:12:22 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 13:12:22 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@28adf099{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:22 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 13:12:22 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 13:12:22 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 13:12:22 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 13:12:22 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 13:12:22 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 13:12:22 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 13:12:22 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 13:12:22 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-221ada61-827c-4254-8c49-25c9e9e7c2d2
2016-10-01 13:12:34 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:26] : starting spark-streaming-kafka
2016-10-01 13:12:34 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 13:12:39 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 13:12:39 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 13:12:39 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 13:12:39 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 13:12:39 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 13:12:39 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 13:12:40 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 63392.
2016-10-01 13:12:40 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 13:12:40 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 13:12:40 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-e572a918-dbfd-495a-b53b-b8bcec77b277
2016-10-01 13:12:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 13:12:40 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 13:12:40 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6337ms
2016-10-01 13:12:40 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 13:12:40 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:40 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6456ms
2016-10-01 13:12:40 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 13:12:40 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 13:12:40 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 13:12:40 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63393.
2016-10-01 13:12:40 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:63393
2016-10-01 13:12:40 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 63393)
2016-10-01 13:12:40 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:63393 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 63393)
2016-10-01 13:12:40 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 63393)
2016-10-01 13:12:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 13:12:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 13:12:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:63393 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 13:12:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:33
2016-10-01 13:12:41 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 13:12:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 13:12:41 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:63393 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 13:12:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 13:12:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 13:12:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 13:12:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 13:12:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 13:12:41 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 13:12:41 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 13:12:41 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 13:12:41 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 13:12:41 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 13:12:41 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 13:12:41 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 13:12:41 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 13:12:41 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 13:12:41 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 13:12:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:63393 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 13:12:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:63393 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 319 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 281 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.339 s
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:63393 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 74 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 73 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.077 s
2016-10-01 13:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.611406 s
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:63393 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 57 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 58 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.062 s
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:63393 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4574 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 50 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 53 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.054 s
2016-10-01 13:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.145107 s
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:63393 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 24 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 38 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.040 s
2016-10-01 13:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.055946 s
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:63393 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:63393 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:63393 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:63393 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:63393 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:63393 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 52 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 62 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.064 s
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:63393 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 20 ms on localhost (1/2)
2016-10-01 13:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 25 ms on localhost (2/2)
2016-10-01 13:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 13:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.028 s
2016-10-01 13:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.116113 s
2016-10-01 13:12:42 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:42 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 13:12:42 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 13:12:42 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 13:12:42 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 13:12:42 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 63394.
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 13:12:42 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-5912c661-4ea0-4145-9c47-350da1e080ad
2016-10-01 13:12:42 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 13:12:42 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 13:12:42 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 13:12:42 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:42 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8655ms
2016-10-01 13:12:42 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 13:12:42 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 13:12:42 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 13:12:42 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63395.
2016-10-01 13:12:42 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:63395
2016-10-01 13:12:42 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 63395)
2016-10-01 13:12:42 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:63395 with 912.3 MB RAM, BlockManagerId(driver, localhost, 63395)
2016-10-01 13:12:42 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 63395)
2016-10-01 13:12:42 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 13:12:42 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 13:12:44 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 13:12:44 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:12:44 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 13:12:44 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 13:12:44 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 13:12:44 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 13:12:44 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 13:12:44 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 13:12:44 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 13:12:44 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 13:12:44 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-519e1d78-9761-4167-8646-9a3b94936cbe
2016-10-01 13:14:02 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 13:14:02 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 13:14:07 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 13:14:07 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 63401.
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 13:14:07 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 13:14:07 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-7817f913-d507-4055-ab8d-05eb4a8f7828
2016-10-01 13:14:08 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 13:14:08 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 13:14:08 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6517ms
2016-10-01 13:14:08 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 13:14:08 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:14:08 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6646ms
2016-10-01 13:14:08 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 13:14:08 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 13:14:08 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 13:14:08 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63402.
2016-10-01 13:14:08 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:63402
2016-10-01 13:14:08 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 63402)
2016-10-01 13:14:08 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:63402 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 63402)
2016-10-01 13:14:08 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 63402)
2016-10-01 13:14:09 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:63402 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 13:14:09 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 13:14:09 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 13:14:09 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:54
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:51)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:54) with 2 output partitions
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:54)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51), which has no missing parents
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:63402 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:63402 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:63402 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2174 bytes result sent to driver
2016-10-01 13:14:09 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 256 ms on localhost (1/2)
2016-10-01 13:14:09 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 220 ms on localhost (2/2)
2016-10-01 13:14:09 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:51) finished in 0.280 s
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:63402 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54)
2016-10-01 13:14:09 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 13:14:09 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 13:14:09 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 13:14:09 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 74 ms on localhost (1/2)
2016-10-01 13:14:10 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 79 ms on localhost (2/2)
2016-10-01 13:14:10 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:54) finished in 0.080 s
2016-10-01 13:14:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:54, took 0.584317 s
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:55
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:54)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:55) with 2 output partitions
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:55)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:63402 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 46 ms on localhost (1/2)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 47 ms on localhost (2/2)
2016-10-01 13:14:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:54) finished in 0.050 s
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:63402 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4574 bytes result sent to driver
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4253 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 44 ms on localhost (1/2)
2016-10-01 13:14:10 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 43 ms on localhost (2/2)
2016-10-01 13:14:10 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:55) finished in 0.046 s
2016-10-01 13:14:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:55, took 0.127767 s
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:57
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:57) with 2 output partitions
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:57)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:63402 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 30 ms on localhost (1/2)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 37 ms on localhost (2/2)
2016-10-01 13:14:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:57) finished in 0.039 s
2016-10-01 13:14:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:57, took 0.059201 s
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:58
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:57)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:58) with 2 output partitions
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:58)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57), which has no missing parents
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:63402 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 2 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 44 ms on localhost (1/2)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 46 ms on localhost (2/2)
2016-10-01 13:14:10 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:63402 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:57) finished in 0.051 s
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:63402 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:63402 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:63402 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:63402 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:63402 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57)
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 13:14:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 26 ms on localhost (1/2)
2016-10-01 13:14:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 13:14:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 36 ms on localhost (2/2)
2016-10-01 13:14:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 13:14:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:58) finished in 0.038 s
2016-10-01 13:14:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:58, took 0.122242 s
2016-10-01 13:14:10 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:14:10 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 13:14:10 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 13:14:10 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 13:14:10 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 13:14:10 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 63403.
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 13:14:10 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-81b0fd15-7828-4287-a7ad-280997f004b6
2016-10-01 13:14:10 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 13:14:10 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 13:14:10 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 13:14:10 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:14:10 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8889ms
2016-10-01 13:14:10 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 13:14:10 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 13:14:10 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 13:14:10 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63404.
2016-10-01 13:14:10 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:63404
2016-10-01 13:14:10 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 63404)
2016-10-01 13:14:10 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:63404 with 912.3 MB RAM, BlockManagerId(driver, localhost, 63404)
2016-10-01 13:14:10 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 63404)
2016-10-01 13:14:10 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 13:14:10 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 13:14:11 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 13:14:11 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 13:14:11 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 13:14:11 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 13:14:11 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 13:14:11 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 13:14:11 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 13:14:11 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 13:14:11 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 13:14:11 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 13:14:11 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-488a8847-4102-4760-a06b-daf1d8aa4bfa
2016-10-01 16:45:12 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 16:45:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:45:17 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 16:45:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:45:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:45:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:45:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:45:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:45:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 64976.
2016-10-01 16:45:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:45:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:45:18 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-22bf368f-d2e7-4444-a76d-47ecd858d22e
2016-10-01 16:45:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:45:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:45:18 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6925ms
2016-10-01 16:45:18 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:45:18 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@655a80dd{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:45:18 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @7138ms
2016-10-01 16:45:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:45:18 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 16:45:18 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:45:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64979.
2016-10-01 16:45:18 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:64979
2016-10-01 16:45:18 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 64979)
2016-10-01 16:45:18 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:64979 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 64979)
2016-10-01 16:45:18 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 64979)
2016-10-01 16:45:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 16:45:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 16:45:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:64979 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 16:45:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 16:45:20 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:45:20 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:64979 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:64979 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:64979 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 16:45:20 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 284 ms on localhost (1/2)
2016-10-01 16:45:20 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 246 ms on localhost (2/2)
2016-10-01 16:45:20 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.308 s
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:64979 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 16:45:20 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 84 ms on localhost (1/2)
2016-10-01 16:45:20 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 89 ms on localhost (2/2)
2016-10-01 16:45:20 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.093 s
2016-10-01 16:45:20 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.648045 s
2016-10-01 16:45:20 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:64979 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 16:45:20 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:45:20 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:20 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:20 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 41 ms on localhost (1/2)
2016-10-01 16:45:21 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 39 ms on localhost (2/2)
2016-10-01 16:45:21 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.042 s
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:64979 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 41 ms on localhost (1/2)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 47 ms on localhost (2/2)
2016-10-01 16:45:21 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.048 s
2016-10-01 16:45:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.120646 s
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:64979 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2332 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 19 ms on localhost (1/2)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 22 ms on localhost (2/2)
2016-10-01 16:45:21 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.024 s
2016-10-01 16:45:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.041833 s
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:64979 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 2041 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 29 ms on localhost (1/2)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 34 ms on localhost (2/2)
2016-10-01 16:45:21 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.036 s
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:64979 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:64979 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:64979 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:64979 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:64979 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:64979 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:45:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 16:45:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 16:45:21 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 25 ms on localhost (1/2)
2016-10-01 16:45:21 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 23 ms on localhost (2/2)
2016-10-01 16:45:21 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 16:45:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.026 s
2016-10-01 16:45:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.093590 s
2016-10-01 16:45:21 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@655a80dd{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:45:21 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:45:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:45:21 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:45:21 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:45:21 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 64980.
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:45:21 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-0cd09c80-fda9-4e3f-97d3-bff7e0b8dda7
2016-10-01 16:45:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:45:21 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:45:21 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:45:21 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@29a1505c{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:45:21 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @9862ms
2016-10-01 16:45:21 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:45:21 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 16:45:21 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:45:21 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64981.
2016-10-01 16:45:21 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:64981
2016-10-01 16:45:21 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 64981)
2016-10-01 16:45:21 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:64981 with 912.3 MB RAM, BlockManagerId(driver, localhost, 64981)
2016-10-01 16:45:21 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 64981)
2016-10-01 16:45:21 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 16:45:21 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 16:45:23 WARN  [main] o.a.s.s.e.datasources.DataSource [Logging.scala:66] : Error while looking for metadata directory.
2016-10-01 16:45:23 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 16:45:23 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@29a1505c{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:45:23 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 16:45:23 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:45:23 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:45:23 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:45:23 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:45:23 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:45:23 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:45:23 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 16:45:23 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-20b89499-d3cd-4b15-9c72-bb6123068211
2016-10-01 16:48:50 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 16:48:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:48:55 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:48:55 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65218.
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:48:55 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-0f3c538a-c707-4161-803a-6781fe45d316
2016-10-01 16:48:55 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:48:55 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:48:56 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6662ms
2016-10-01 16:48:56 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:48:56 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:48:56 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6820ms
2016-10-01 16:48:56 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:48:56 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 16:48:56 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:48:56 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65219.
2016-10-01 16:48:56 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65219
2016-10-01 16:48:56 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65219)
2016-10-01 16:48:56 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65219 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65219)
2016-10-01 16:48:56 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65219)
2016-10-01 16:48:57 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 16:48:57 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 16:48:57 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65219 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 16:48:57 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 16:48:57 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:48:57 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 16:48:57 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 16:48:57 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 16:48:57 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 16:48:57 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 16:48:57 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 16:48:57 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65219 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65219 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65219 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 304 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 255 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.340 s
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65219 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 86 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 89 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.091 s
2016-10-01 16:48:58 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.705767 s
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65219 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 46 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 49 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.051 s
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65219 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 38 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 37 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.039 s
2016-10-01 16:48:58 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.122652 s
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65219 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 22 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 25 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.025 s
2016-10-01 16:48:58 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.042081 s
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65219 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:65219 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:65219 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:65219 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 28 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:65219 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 33 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.034 s
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:65219 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65219 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:48:58 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 22 ms on localhost (1/2)
2016-10-01 16:48:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 24 ms on localhost (2/2)
2016-10-01 16:48:58 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 16:48:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.026 s
2016-10-01 16:48:58 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.102785 s
2016-10-01 16:48:58 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:48:58 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:48:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:48:58 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:48:58 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:48:58 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:48:58 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65220.
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:48:58 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-b1f7022e-d96b-44c0-8e4d-be3abd49417e
2016-10-01 16:48:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:48:58 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:48:59 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:48:59 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:48:59 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @9597ms
2016-10-01 16:48:59 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:48:59 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 16:48:59 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:48:59 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65221.
2016-10-01 16:48:59 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65221
2016-10-01 16:48:59 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65221)
2016-10-01 16:48:59 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65221 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65221)
2016-10-01 16:48:59 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65221)
2016-10-01 16:48:59 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 16:48:59 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 16:49:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 16:49:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 16:49:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 16:49:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 16:49:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 16:49:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 16:49:01 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65221 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 16:49:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 16:49:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 16:49:01 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 303.102568 ms
2016-10-01 16:49:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 16:49:01 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65221 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 16:49:01 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 16:49:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:49:01 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 16:49:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.073345 ms
2016-10-01 16:49:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-01 16:49:01 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on localhost (1/1)
2016-10-01 16:49:01 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:49:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.197 s
2016-10-01 16:49:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.238668 s
2016-10-01 16:49:02 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.43921 ms
2016-10-01 16:49:02 WARN  [main] o.a.s.s.e.datasources.DataSource [Logging.scala:66] : Error while looking for metadata directory.
2016-10-01 16:49:02 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 16:49:02 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:49:02 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 16:49:02 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:49:02 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:49:02 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:49:02 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:49:02 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:49:02 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:49:02 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 16:49:02 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-b7d98ec9-4e7b-4657-a636-60b0d054ac9e
2016-10-01 16:50:18 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 16:50:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:50:26 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:50:26 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65240.
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:50:26 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-67ce3313-866f-4447-8041-4fd709cb5a0b
2016-10-01 16:50:26 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:50:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:50:26 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @8605ms
2016-10-01 16:50:26 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:50:26 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@2294a8e{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:50:26 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8739ms
2016-10-01 16:50:26 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:50:26 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 16:50:27 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:50:27 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65243.
2016-10-01 16:50:27 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65243
2016-10-01 16:50:27 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65243)
2016-10-01 16:50:27 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65243 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65243)
2016-10-01 16:50:27 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65243)
2016-10-01 16:50:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 16:50:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 16:50:27 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65243 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 16:50:27 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 16:50:28 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65243 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65243 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65243 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 16:50:28 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 208 ms on localhost (1/2)
2016-10-01 16:50:28 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 249 ms on localhost (2/2)
2016-10-01 16:50:28 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.266 s
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65243 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 16:50:28 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 69 ms on localhost (1/2)
2016-10-01 16:50:28 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 74 ms on localhost (2/2)
2016-10-01 16:50:28 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.075 s
2016-10-01 16:50:28 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.549812 s
2016-10-01 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65243 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 16:50:28 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 43 ms on localhost (1/2)
2016-10-01 16:50:28 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 45 ms on localhost (2/2)
2016-10-01 16:50:28 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.046 s
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65243 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 16:50:28 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 38 ms on localhost (1/2)
2016-10-01 16:50:28 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 42 ms on localhost (2/2)
2016-10-01 16:50:28 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.043 s
2016-10-01 16:50:28 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.122245 s
2016-10-01 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65243 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 16:50:28 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 16:50:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 47 ms on localhost (1/2)
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 16:50:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 49 ms on localhost (2/2)
2016-10-01 16:50:29 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.052 s
2016-10-01 16:50:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.069847 s
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65243 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 16:50:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 35 ms on localhost (1/2)
2016-10-01 16:50:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 35 ms on localhost (2/2)
2016-10-01 16:50:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.037 s
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65243 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 16:50:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 22 ms on localhost (1/2)
2016-10-01 16:50:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 16:50:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 29 ms on localhost (2/2)
2016-10-01 16:50:29 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.032 s
2016-10-01 16:50:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.092878 s
2016-10-01 16:50:29 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@2294a8e{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:50:29 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:50:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:50:29 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:50:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:50:29 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65244.
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:50:29 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-f5dd231f-2612-43c7-8214-77edd2fe1ab6
2016-10-01 16:50:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:50:29 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:50:29 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:50:29 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@534c6767{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:50:29 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @10969ms
2016-10-01 16:50:29 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:50:29 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 16:50:29 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:50:29 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65245.
2016-10-01 16:50:29 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65245
2016-10-01 16:50:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65245)
2016-10-01 16:50:29 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65245 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65245)
2016-10-01 16:50:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65245)
2016-10-01 16:50:29 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 16:50:29 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 16:50:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 16:50:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 16:50:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 16:50:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 16:50:30 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 16:50:30 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 16:50:30 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65245 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 16:50:30 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 16:50:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 16:50:31 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 205.20861 ms
2016-10-01 16:50:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 16:50:31 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65245 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 16:50:31 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 16:50:31 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:50:31 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 16:50:31 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.741824 ms
2016-10-01 16:50:31 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-01 16:50:31 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 197 ms on localhost (1/1)
2016-10-01 16:50:31 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.197 s
2016-10-01 16:50:31 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.232362 s
2016-10-01 16:50:31 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.575384 ms
2016-10-01 16:50:31 WARN  [main] o.a.s.s.e.datasources.DataSource [Logging.scala:66] : Error while looking for metadata directory.
2016-10-01 16:50:31 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 16:50:31 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@534c6767{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:50:31 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 16:50:31 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:50:31 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:50:31 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:50:31 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:50:31 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:50:31 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:50:31 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 16:50:31 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-8554a500-179c-4e2b-ba4a-6ea7a4c2518a
2016-10-01 16:54:02 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 16:54:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:54:12 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:54:12 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65261.
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:54:12 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-8dde60c8-14ce-49d8-b73c-5639f43d36d6
2016-10-01 16:54:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:54:12 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:54:13 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @11010ms
2016-10-01 16:54:13 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:54:13 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:54:13 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @11185ms
2016-10-01 16:54:13 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:54:13 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 16:54:13 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:54:13 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65262.
2016-10-01 16:54:13 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65262
2016-10-01 16:54:13 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65262)
2016-10-01 16:54:13 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65262 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65262)
2016-10-01 16:54:13 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65262)
2016-10-01 16:54:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 16:54:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 16:54:14 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65262 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 16:54:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 16:54:14 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:54:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 16:54:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 16:54:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 16:54:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 16:54:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 16:54:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65262 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65262 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65262 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 287 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 241 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.310 s
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65262 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 73 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 78 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.079 s
2016-10-01 16:54:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.639080 s
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65262 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1968 bytes result sent to driver
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 40 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 43 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.044 s
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65262 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4253 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 43 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 46 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.047 s
2016-10-01 16:54:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.124334 s
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65262 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2332 bytes result sent to driver
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 26 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 30 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.031 s
2016-10-01 16:54:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.047644 s
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65262 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 27 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 31 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.032 s
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65262 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:65262 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:65262 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:65262 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:65262 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:65262 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:54:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-01 16:54:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-01 16:54:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 20 ms on localhost (1/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 23 ms on localhost (2/2)
2016-10-01 16:54:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 16:54:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.023 s
2016-10-01 16:54:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.086922 s
2016-10-01 16:54:15 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:54:15 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:54:15 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:54:15 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:54:15 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:54:15 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:54:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:54:15 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65263.
2016-10-01 16:54:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:54:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:54:16 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-636120c4-a305-4097-ae5a-0dd214af92cb
2016-10-01 16:54:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:54:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:54:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:54:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@24db6ce{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:54:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @13919ms
2016-10-01 16:54:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:54:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 16:54:16 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:54:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65264.
2016-10-01 16:54:16 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65264
2016-10-01 16:54:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65264)
2016-10-01 16:54:16 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65264 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65264)
2016-10-01 16:54:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65264)
2016-10-01 16:54:16 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 16:54:16 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 16:54:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 16:54:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 16:54:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 16:54:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 16:54:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 16:54:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 16:54:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65264 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 16:54:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 16:54:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 16:54:18 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 298.038621 ms
2016-10-01 16:54:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 16:54:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65264 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 16:54:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 16:54:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:54:18 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 16:54:18 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 144.651981 ms
2016-10-01 16:54:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-01 16:54:18 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on localhost (1/1)
2016-10-01 16:54:18 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:54:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.198 s
2016-10-01 16:54:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.232619 s
2016-10-01 16:54:18 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.028481 ms
2016-10-01 16:54:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 16:54:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 16:54:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:65264 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 16:54:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:76
2016-10-01 16:54:18 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:54:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:76
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:76) with 1 output partitions
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:76)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:76), which has no missing parents
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:65264 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:76)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 16:54:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 16:54:19 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 16:54:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 16:54:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 30 ms on localhost (1/1)
2016-10-01 16:54:19 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:76) finished in 0.032 s
2016-10-01 16:54:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:76, took 0.042513 s
2016-10-01 16:54:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 16:54:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 16:54:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 16:54:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 16:54:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 16:54:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:65264 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 16:54:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:79
2016-10-01 16:54:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 16:54:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:79
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:79) with 1 output partitions
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:79)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:79), which has no missing parents
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:65264 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:79)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 16:54:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 16:54:19 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 16:54:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 16:54:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 23 ms on localhost (1/1)
2016-10-01 16:54:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:79) finished in 0.023 s
2016-10-01 16:54:19 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 16:54:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:79, took 0.031773 s
2016-10-01 16:54:19 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 16:54:19 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@24db6ce{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:54:19 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:54:19 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:54:19 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:54:19 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:54:19 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:54:19 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:54:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 16:54:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-ddf86461-de4d-4bf9-abef-12c849d5467f
2016-10-01 16:57:11 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 16:57:11 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:57:16 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:57:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65306.
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:57:16 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-6105624a-ef88-4b07-acfa-b73d71121685
2016-10-01 16:57:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:57:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:57:16 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6149ms
2016-10-01 16:57:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:57:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:57:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6279ms
2016-10-01 16:57:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:57:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 16:57:17 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:57:17 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65307.
2016-10-01 16:57:17 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65307
2016-10-01 16:57:17 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65307)
2016-10-01 16:57:17 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65307 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65307)
2016-10-01 16:57:17 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65307)
2016-10-01 16:57:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 16:57:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 16:57:17 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65307 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 16:57:17 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 16:57:18 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:57:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65307 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65307 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65307 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2174 bytes result sent to driver
2016-10-01 16:57:18 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 205 ms on localhost (1/2)
2016-10-01 16:57:18 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 245 ms on localhost (2/2)
2016-10-01 16:57:18 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.263 s
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65307 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2346 bytes result sent to driver
2016-10-01 16:57:18 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 72 ms on localhost (1/2)
2016-10-01 16:57:18 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 70 ms on localhost (2/2)
2016-10-01 16:57:18 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.074 s
2016-10-01 16:57:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.613456 s
2016-10-01 16:57:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65307 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 16:57:18 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 46 ms on localhost (1/2)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1968 bytes result sent to driver
2016-10-01 16:57:18 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 50 ms on localhost (2/2)
2016-10-01 16:57:18 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.053 s
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65307 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 16:57:18 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 41 ms on localhost (1/2)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 16:57:18 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 47 ms on localhost (2/2)
2016-10-01 16:57:18 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.048 s
2016-10-01 16:57:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.138003 s
2016-10-01 16:57:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65307 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 16:57:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 16:57:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 16:57:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 16:57:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2337 bytes result sent to driver
2016-10-01 16:57:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 25 ms on localhost (1/2)
2016-10-01 16:57:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 27 ms on localhost (2/2)
2016-10-01 16:57:19 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.028 s
2016-10-01 16:57:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.043429 s
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65307 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:65307 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:65307 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:65307 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 2 ms
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:65307 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:65307 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 16:57:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 49 ms on localhost (1/2)
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 16:57:19 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 47 ms on localhost (2/2)
2016-10-01 16:57:19 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.051 s
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65307 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 16:57:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-01 16:57:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 22 ms on localhost (1/2)
2016-10-01 16:57:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 16:57:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 27 ms on localhost (2/2)
2016-10-01 16:57:19 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 16:57:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.030 s
2016-10-01 16:57:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.118303 s
2016-10-01 16:57:19 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:57:19 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:57:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:57:19 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:57:19 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 16:57:19 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65308.
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 16:57:19 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-1ba3ffff-85cf-4e1c-8924-2a648dbc3ad3
2016-10-01 16:57:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 16:57:19 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 16:57:19 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 16:57:19 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:57:19 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8567ms
2016-10-01 16:57:19 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 16:57:19 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 16:57:19 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 16:57:19 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65309.
2016-10-01 16:57:19 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65309
2016-10-01 16:57:19 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65309)
2016-10-01 16:57:19 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65309 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65309)
2016-10-01 16:57:19 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65309)
2016-10-01 16:57:19 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 16:57:19 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 16:57:20 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 16:57:20 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 16:57:20 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 16:57:20 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 16:57:20 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 16:57:20 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 16:57:20 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65309 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 16:57:20 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 16:57:20 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 16:57:21 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 225.513515 ms
2016-10-01 16:57:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65309 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 172.907324 ms
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-01 16:57:21 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 221 ms on localhost (1/1)
2016-10-01 16:57:21 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.223 s
2016-10-01 16:57:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.255501 s
2016-10-01 16:57:21 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.032301 ms
2016-10-01 16:57:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 16:57:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:65309 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 16:57:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:76
2016-10-01 16:57:21 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 16:57:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:76
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:76) with 1 output partitions
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:76)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:76), which has no missing parents
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:65309 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:76)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1098
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 16:57:21 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 19 ms on localhost (1/1)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:76) finished in 0.020 s
2016-10-01 16:57:21 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 16:57:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:76, took 0.026950 s
2016-10-01 16:57:21 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 16:57:21 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 16:57:21 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 16:57:21 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 16:57:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 16:57:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:65309 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 16:57:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:79
2016-10-01 16:57:21 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195402 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 16:57:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:79
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:79) with 1 output partitions
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:79)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:79), which has no missing parents
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:65309 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:79)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 16:57:21 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1098, partition values: [empty row]
2016-10-01 16:57:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1651 bytes result sent to driver
2016-10-01 16:57:21 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 16 ms on localhost (1/1)
2016-10-01 16:57:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:79) finished in 0.018 s
2016-10-01 16:57:21 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 16:57:21 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:79, took 0.032379 s
2016-10-01 16:57:22 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 16:57:22 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 16:57:22 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 16:57:22 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 16:57:22 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 16:57:22 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 16:57:22 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 16:57:22 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 16:57:22 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 16:57:22 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 16:57:22 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-c43fe9cc-ed02-467a-9d9a-9175d9faf307
2016-10-01 17:10:29 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 17:10:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:10:37 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 17:10:37 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:10:37 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:10:37 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:10:37 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:10:37 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:10:38 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65417.
2016-10-01 17:10:38 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:10:38 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:10:38 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-9fb6ba1f-b889-4827-aa68-0936eabef335
2016-10-01 17:10:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:10:38 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:10:38 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @10242ms
2016-10-01 17:10:38 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:10:38 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@68b49146{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:10:38 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @10415ms
2016-10-01 17:10:38 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:10:38 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 17:10:39 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:10:39 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65418.
2016-10-01 17:10:39 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65418
2016-10-01 17:10:39 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65418)
2016-10-01 17:10:39 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65418 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65418)
2016-10-01 17:10:39 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65418)
2016-10-01 17:10:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 17:10:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 17:10:40 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65418 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 17:10:40 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 17:10:40 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:10:40 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 17:10:40 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65418 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65418 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65418 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2174 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 247 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 298 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.318 s
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65418 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 92 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 97 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.099 s
2016-10-01 17:10:41 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.723070 s
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65418 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 44 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 44 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.050 s
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65418 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 29 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4574 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 33 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.035 s
2016-10-01 17:10:41 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.116733 s
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65418 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2337 bytes result sent to driver
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 17 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 15 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.018 s
2016-10-01 17:10:41 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.033345 s
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65418 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 19 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 21 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.021 s
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65418 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:10:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-01 17:10:41 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-01 17:10:41 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 22 ms on localhost (1/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 24 ms on localhost (2/2)
2016-10-01 17:10:41 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 17:10:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.025 s
2016-10-01 17:10:41 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.075195 s
2016-10-01 17:10:41 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@68b49146{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:10:41 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:10:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:10:41 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:10:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:10:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65419.
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:10:41 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-39332ce8-494e-4413-83df-f2ce80fc7dea
2016-10-01 17:10:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:10:41 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:10:41 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:10:41 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@782f20e9{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:10:41 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @13349ms
2016-10-01 17:10:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:10:41 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 17:10:41 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:10:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65420.
2016-10-01 17:10:41 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65420
2016-10-01 17:10:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65420)
2016-10-01 17:10:41 INFO  [dispatcher-event-loop-1] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65420 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65420)
2016-10-01 17:10:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65420)
2016-10-01 17:10:41 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 17:10:42 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 17:10:43 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:10:43 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:10:43 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 17:10:43 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:10:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 17:10:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 17:10:43 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65420 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:10:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 17:10:43 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:10:44 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 219.17059 ms
2016-10-01 17:10:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65420 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 25.468772 ms
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-01 17:10:44 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 233 ms on localhost (1/1)
2016-10-01 17:10:44 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.235 s
2016-10-01 17:10:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.275770 s
2016-10-01 17:10:44 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 21.184593 ms
2016-10-01 17:10:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 17:10:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:65420 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 17:10:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:75
2016-10-01 17:10:44 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:10:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:75
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:75) with 1 output partitions
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:75)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:75), which has no missing parents
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:65420 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:75)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 17:10:44 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 24 ms on localhost (1/1)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:75) finished in 0.025 s
2016-10-01 17:10:44 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:10:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:75, took 0.033542 s
2016-10-01 17:10:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:10:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:10:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 17:10:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:10:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 17:10:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:65420 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:10:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:78
2016-10-01 17:10:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:10:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:78
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:78) with 1 output partitions
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:78)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:78), which has no missing parents
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:65420 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:78)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 17:10:44 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 17:10:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 17:10:44 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 24 ms on localhost (1/1)
2016-10-01 17:10:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:78) finished in 0.024 s
2016-10-01 17:10:44 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 17:10:44 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:78, took 0.034858 s
2016-10-01 17:10:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-01 17:10:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:65420 (size: 14.9 KB, free: 912.2 MB)
2016-10-01 17:10:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:82
2016-10-01 17:10:45 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:10:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:82
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:82) with 1 output partitions
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:82)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82), which has no missing parents
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:65420 (size: 2.6 KB, free: 912.2 MB)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+262
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1685 bytes result sent to driver
2016-10-01 17:10:45 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 11 ms on localhost (1/1)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:82) finished in 0.013 s
2016-10-01 17:10:45 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:10:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:82, took 0.021789 s
2016-10-01 17:10:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:10:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:10:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string, age: string, id: string, name: string ... 2 more fields>
2016-10-01 17:10:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:10:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-01 17:10:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:65420 (size: 14.6 KB, free: 912.2 MB)
2016-10-01 17:10:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:85
2016-10-01 17:10:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194566 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:10:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:85
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:85) with 1 output partitions
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:85)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85), which has no missing parents
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.3 KB, free 911.5 MB)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:65420 (size: 4.1 KB, free: 912.2 MB)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85)
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-262, partition values: [empty row]
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.631842 ms
2016-10-01 17:10:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1442 bytes result sent to driver
2016-10-01 17:10:45 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 29 ms on localhost (1/1)
2016-10-01 17:10:45 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:10:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:85) finished in 0.031 s
2016-10-01 17:10:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:85, took 0.045322 s
2016-10-01 17:10:45 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 27.944593 ms
2016-10-01 17:10:45 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 17:10:45 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@782f20e9{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:10:45 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:10:45 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:10:45 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:10:45 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:10:45 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:10:45 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:10:45 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 17:10:45 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-4fe92f8f-5836-4089-912b-e9587fcacc01
2016-10-01 17:11:41 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 17:11:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:11:49 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:11:49 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65425.
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:11:49 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:11:49 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-5f0170e3-e120-4fec-90ed-dcf385daa67d
2016-10-01 17:11:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:11:50 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:11:50 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @8816ms
2016-10-01 17:11:50 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:11:50 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@18956be5{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:11:50 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8975ms
2016-10-01 17:11:50 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:11:50 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 17:11:50 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:11:50 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65426.
2016-10-01 17:11:50 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65426
2016-10-01 17:11:50 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65426)
2016-10-01 17:11:50 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65426 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65426)
2016-10-01 17:11:50 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65426)
2016-10-01 17:11:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65426 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 17:11:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 17:11:51 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:11:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65426 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65426 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65426 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2174 bytes result sent to driver
2016-10-01 17:11:51 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 217 ms on localhost (1/2)
2016-10-01 17:11:51 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 261 ms on localhost (2/2)
2016-10-01 17:11:51 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.280 s
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65426 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 17:11:51 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 17:11:51 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:51 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 17:11:51 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 83 ms on localhost (1/2)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 82 ms on localhost (2/2)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.087 s
2016-10-01 17:11:52 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:11:52 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.557234 s
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65426 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 2 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 65 ms on localhost (1/2)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 65 ms on localhost (2/2)
2016-10-01 17:11:52 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.067 s
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65426 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4253 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 39 ms on localhost (1/2)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 44 ms on localhost (2/2)
2016-10-01 17:11:52 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.045 s
2016-10-01 17:11:52 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.142203 s
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65426 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 29 ms on localhost (1/2)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2337 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 39 ms on localhost (2/2)
2016-10-01 17:11:52 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.040 s
2016-10-01 17:11:52 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.055374 s
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65426 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:65426 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:65426 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:65426 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:65426 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:65426 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 43 ms on localhost (1/2)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 66 ms on localhost (2/2)
2016-10-01 17:11:52 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.067 s
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65426 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:11:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 35 ms on localhost (1/2)
2016-10-01 17:11:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 17:11:52 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 40 ms on localhost (2/2)
2016-10-01 17:11:52 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 17:11:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.043 s
2016-10-01 17:11:52 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.134942 s
2016-10-01 17:11:52 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@18956be5{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:11:52 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:11:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:11:52 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:11:52 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:11:52 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65427.
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:11:52 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-7b0d5c84-2988-43c7-bf02-dc0e7b25fe6c
2016-10-01 17:11:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:11:52 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:11:52 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:11:52 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@4846c8fe{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:11:52 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @11175ms
2016-10-01 17:11:52 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:11:52 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 17:11:52 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:11:52 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65428.
2016-10-01 17:11:52 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65428
2016-10-01 17:11:52 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65428)
2016-10-01 17:11:52 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65428 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65428)
2016-10-01 17:11:52 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65428)
2016-10-01 17:11:52 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 17:11:52 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 17:11:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:11:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65428 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:11:54 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 222.923509 ms
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65428 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 21.493982 ms
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1445 bytes result sent to driver
2016-10-01 17:11:54 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 202 ms on localhost (1/1)
2016-10-01 17:11:54 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.202 s
2016-10-01 17:11:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.235682 s
2016-10-01 17:11:54 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.818473 ms
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:65428 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:75
2016-10-01 17:11:54 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:75
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:75) with 1 output partitions
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:75)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:75), which has no missing parents
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:65428 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:75)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1703 bytes result sent to driver
2016-10-01 17:11:54 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 27 ms on localhost (1/1)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:75) finished in 0.027 s
2016-10-01 17:11:54 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:11:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:75, took 0.035947 s
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:65428 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:78
2016-10-01 17:11:54 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:78
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:78) with 1 output partitions
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:78)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:78), which has no missing parents
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:65428 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:78)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 17:11:54 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 21 ms on localhost (1/1)
2016-10-01 17:11:54 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:78) finished in 0.021 s
2016-10-01 17:11:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:78, took 0.032624 s
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-01 17:11:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.8 KB, free 911.7 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:65428 (size: 14.8 KB, free: 912.2 MB)
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:82
2016-10-01 17:11:54 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:11:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:82
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:82) with 1 output partitions
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:82)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82), which has no missing parents
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:65428 (size: 2.6 KB, free: 912.2 MB)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82)
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-01 17:11:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+260
2016-10-01 17:11:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1685 bytes result sent to driver
2016-10-01 17:11:54 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 10 ms on localhost (1/1)
2016-10-01 17:11:54 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:11:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:82) finished in 0.011 s
2016-10-01 17:11:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:82, took 0.017032 s
2016-10-01 17:11:55 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:11:55 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:11:55 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string, age: string, id: string, name: string ... 2 more fields>
2016-10-01 17:11:55 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:11:55 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-01 17:11:55 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-01 17:11:55 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:65428 (size: 14.6 KB, free: 912.2 MB)
2016-10-01 17:11:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:85
2016-10-01 17:11:55 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194564 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:11:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:85
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:85) with 1 output partitions
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:85)
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85), which has no missing parents
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.3 KB, free 911.5 MB)
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-01 17:11:55 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:65428 (size: 4.1 KB, free: 912.2 MB)
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85)
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-01 17:11:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-01 17:11:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-01 17:11:55 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-260, partition values: [empty row]
2016-10-01 17:11:55 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.755428 ms
2016-10-01 17:11:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1539 bytes result sent to driver
2016-10-01 17:11:55 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 23 ms on localhost (1/1)
2016-10-01 17:11:55 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:11:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:85) finished in 0.023 s
2016-10-01 17:11:55 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:85, took 0.030602 s
2016-10-01 17:11:55 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.910207 ms
2016-10-01 17:11:55 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 17:11:55 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@4846c8fe{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:11:55 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 17:11:55 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:11:55 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:11:55 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:11:55 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:11:55 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:11:55 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:11:55 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 17:11:55 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-297dd642-627c-441c-bd79-0fe29e9cfdbc
2016-10-01 17:12:34 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 17:12:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:12:40 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:12:40 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65432.
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:12:40 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-e503f7bd-21c3-43ee-b833-471700c5ab9f
2016-10-01 17:12:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:12:40 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:12:40 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6294ms
2016-10-01 17:12:40 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:12:40 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@18956be5{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:12:40 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6434ms
2016-10-01 17:12:40 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:12:40 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 17:12:41 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:12:41 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65433.
2016-10-01 17:12:41 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:65433
2016-10-01 17:12:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 65433)
2016-10-01 17:12:41 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:65433 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 65433)
2016-10-01 17:12:41 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 65433)
2016-10-01 17:12:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 17:12:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 17:12:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:65433 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 17:12:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 17:12:41 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:65433 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:65433 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:65433 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 262 ms on localhost (1/2)
2016-10-01 17:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 219 ms on localhost (2/2)
2016-10-01 17:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.283 s
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:65433 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 79 ms on localhost (1/2)
2016-10-01 17:12:42 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 77 ms on localhost (2/2)
2016-10-01 17:12:42 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.081 s
2016-10-01 17:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.575595 s
2016-10-01 17:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:65433 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 50 ms on localhost (1/2)
2016-10-01 17:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 49 ms on localhost (2/2)
2016-10-01 17:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.052 s
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:65433 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4253 bytes result sent to driver
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 53 ms on localhost (1/2)
2016-10-01 17:12:42 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 55 ms on localhost (2/2)
2016-10-01 17:12:42 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.056 s
2016-10-01 17:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.138616 s
2016-10-01 17:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:65433 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 21 ms on localhost (1/2)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2337 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 25 ms on localhost (2/2)
2016-10-01 17:12:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.027 s
2016-10-01 17:12:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.045271 s
2016-10-01 17:12:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:65433 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:65433 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:65433 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:65433 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 2041 bytes result sent to driver
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:65433 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 45 ms on localhost (1/2)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:65433 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 17:12:42 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 51 ms on localhost (2/2)
2016-10-01 17:12:42 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.052 s
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:65433 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 17:12:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:12:42 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:12:42 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:12:43 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 17:12:43 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 24 ms on localhost (1/2)
2016-10-01 17:12:43 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 17:12:43 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 27 ms on localhost (2/2)
2016-10-01 17:12:43 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 17:12:43 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.030 s
2016-10-01 17:12:43 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.108831 s
2016-10-01 17:12:43 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@18956be5{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:12:43 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 17:12:43 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:12:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:12:43 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:12:43 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:12:43 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:12:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 65434.
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:12:43 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-04c177b1-95cc-4edd-8a91-9449efbc5d49
2016-10-01 17:12:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:12:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:12:43 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:12:43 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:12:43 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8639ms
2016-10-01 17:12:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:12:43 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 17:12:43 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:12:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65435.
2016-10-01 17:12:43 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:65435
2016-10-01 17:12:43 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 65435)
2016-10-01 17:12:43 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:65435 with 912.3 MB RAM, BlockManagerId(driver, localhost, 65435)
2016-10-01 17:12:43 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 65435)
2016-10-01 17:12:43 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 17:12:43 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 17:12:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:12:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:12:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 17:12:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:12:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 17:12:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 17:12:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:65435 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:12:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:71
2016-10-01 17:12:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:12:44 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 213.970346 ms
2016-10-01 17:12:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:71
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:71) with 1 output partitions
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:71)
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71), which has no missing parents
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 17:12:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:65435 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:71)
2016-10-01 17:12:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 17:12:44 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 17:12:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:12:44 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 128.870249 ms
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-01 17:12:45 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 172 ms on localhost (1/1)
2016-10-01 17:12:45 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:71) finished in 0.173 s
2016-10-01 17:12:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:71, took 0.203710 s
2016-10-01 17:12:45 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.26329 ms
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:65435 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:75
2016-10-01 17:12:45 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:75
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:75) with 1 output partitions
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:75)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:75), which has no missing parents
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:65435 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:75)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 17:12:45 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on localhost (1/1)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:75) finished in 0.020 s
2016-10-01 17:12:45 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:12:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:75, took 0.028208 s
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:65435 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:78
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:78
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:78) with 1 output partitions
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:78)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:78), which has no missing parents
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:65435 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:78)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 17:12:45 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on localhost (1/1)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:78) finished in 0.016 s
2016-10-01 17:12:45 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 17:12:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:78, took 0.025361 s
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:65435 (size: 14.9 KB, free: 912.2 MB)
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:82
2016-10-01 17:12:45 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:82
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:82) with 1 output partitions
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:82)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82), which has no missing parents
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:65435 (size: 2.6 KB, free: 912.2 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+255
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1650 bytes result sent to driver
2016-10-01 17:12:45 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 13 ms on localhost (1/1)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:82) finished in 0.013 s
2016-10-01 17:12:45 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:12:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:82, took 0.021545 s
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-01 17:12:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:65435 (size: 14.6 KB, free: 912.2 MB)
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:85
2016-10-01 17:12:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194559 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:12:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:85
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:85) with 1 output partitions
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:85)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85), which has no missing parents
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:65435 (size: 4.1 KB, free: 912.2 MB)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85)
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-01 17:12:45 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-255, partition values: [empty row]
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.968984 ms
2016-10-01 17:12:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-01 17:12:45 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 20 ms on localhost (1/1)
2016-10-01 17:12:45 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:12:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:85) finished in 0.021 s
2016-10-01 17:12:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:85, took 0.027579 s
2016-10-01 17:12:45 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.488057 ms
2016-10-01 17:12:46 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 17:12:46 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3bddc676{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:12:46 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 17:12:46 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:12:46 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:12:46 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:12:46 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:12:46 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:12:46 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:12:46 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 17:12:46 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-87e4fd8b-2ded-44c3-8aff-16bb07fb5ec1
2016-10-01 17:56:05 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 17:56:05 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:56:12 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:56:12 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 49460.
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:56:12 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-9c28a1d0-cd9c-499a-8b20-eb67ae3af8f8
2016-10-01 17:56:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:56:12 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:56:13 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @8249ms
2016-10-01 17:56:13 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:56:13 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:56:13 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8417ms
2016-10-01 17:56:13 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:56:13 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 17:56:13 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:56:13 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49461.
2016-10-01 17:56:13 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:49461
2016-10-01 17:56:13 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 49461)
2016-10-01 17:56:13 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:49461 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 49461)
2016-10-01 17:56:13 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 49461)
2016-10-01 17:56:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 17:56:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 17:56:14 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:49461 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 17:56:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 17:56:14 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:56:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 17:56:14 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:49461 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 17:56:14 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 17:56:14 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:56:14 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 17:56:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:49461 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:49461 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 303 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 262 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.331 s
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:49461 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 91 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 91 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.095 s
2016-10-01 17:56:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.665709 s
2016-10-01 17:56:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:49461 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 43 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 46 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.052 s
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:49461 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4340 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 30 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 36 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.036 s
2016-10-01 17:56:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.121014 s
2016-10-01 17:56:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:49461 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2332 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 15 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 18 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.019 s
2016-10-01 17:56:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.033754 s
2016-10-01 17:56:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:49461 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 21 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 25 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.026 s
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:49461 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 16 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 22 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.024 s
2016-10-01 17:56:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.072639 s
2016-10-01 17:56:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:59)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 192.168.8.100:49461 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2292 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 12 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2337 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 20 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:59) finished in 0.021 s
2016-10-01 17:56:15 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:59, took 0.038646 s
2016-10-01 17:56:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTextFile at SparkApplication.java:60
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (mapToPair at SparkApplication.java:59)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTextFile at SparkApplication.java:60) with 2 output partitions
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 18 (saveAsTextFile at SparkApplication.java:60)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 17)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 17)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 17 (MapPartitionsRDD[12] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 192.168.8.100:49461 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[12] at mapToPair at SparkApplication.java:59)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 17.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 17.0 (TID 16, localhost, partition 0, ANY, 5137 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 17.0 (TID 17, localhost, partition 1, ANY, 5137 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 17.0 (TID 16)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 17.0 (TID 17)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 17.0 (TID 17). 1881 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 17.0 (TID 17) in 18 ms on localhost (1/2)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 17.0 (TID 16). 1881 bytes result sent to driver
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 17.0 (TID 16) in 20 ms on localhost (2/2)
2016-10-01 17:56:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 17.0, whose tasks have all completed, from pool 
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 17 (mapToPair at SparkApplication.java:59) finished in 0.021 s
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 18)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 18 (MapPartitionsRDD[16] at saveAsTextFile at SparkApplication.java:60), which has no missing parents
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 49.9 KB, free 912.1 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 18.1 KB, free 912.0 MB)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 192.168.8.100:49461 (size: 18.1 KB, free: 912.2 MB)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[16] at saveAsTextFile at SparkApplication.java:60)
2016-10-01 17:56:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 18.0 with 2 tasks
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 18.0 (TID 18, localhost, partition 0, ANY, 5148 bytes)
2016-10-01 17:56:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 18.0 (TID 19, localhost, partition 1, ANY, 5148 bytes)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 18.0 (TID 18)
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 18.0 (TID 19)
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 17:56:15 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 17:56:16 INFO  [Executor task launch worker-1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610011756_0018_m_000000_18' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/wordCount.txt/_temporary/0/task_201610011756_0018_m_000000
2016-10-01 17:56:16 INFO  [Executor task launch worker-0] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610011756_0018_m_000001_19' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/wordCount.txt/_temporary/0/task_201610011756_0018_m_000001
2016-10-01 17:56:16 INFO  [Executor task launch worker-1] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610011756_0018_m_000000_18: Committed
2016-10-01 17:56:16 INFO  [Executor task launch worker-0] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610011756_0018_m_000001_19: Committed
2016-10-01 17:56:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 18.0 (TID 19). 1553 bytes result sent to driver
2016-10-01 17:56:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 18.0 (TID 18). 1553 bytes result sent to driver
2016-10-01 17:56:16 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 18.0 (TID 19) in 100 ms on localhost (1/2)
2016-10-01 17:56:16 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 18.0 (TID 18) in 102 ms on localhost (2/2)
2016-10-01 17:56:16 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 18.0, whose tasks have all completed, from pool 
2016-10-01 17:56:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 18 (saveAsTextFile at SparkApplication.java:60) finished in 0.103 s
2016-10-01 17:56:16 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTextFile at SparkApplication.java:60, took 0.170620 s
2016-10-01 17:56:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:56:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 17:56:16 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:56:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:56:16 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:56:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:56:16 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 17:56:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 49468.
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 17:56:16 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-17137272-6ff2-4371-a83b-365e3a9e1c9f
2016-10-01 17:56:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 17:56:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 17:56:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 17:56:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@60f70249{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:56:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @11464ms
2016-10-01 17:56:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 17:56:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 17:56:16 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 17:56:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49469.
2016-10-01 17:56:16 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:49469
2016-10-01 17:56:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 49469)
2016-10-01 17:56:16 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:49469 with 912.3 MB RAM, BlockManagerId(driver, localhost, 49469)
2016-10-01 17:56:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 49469)
2016-10-01 17:56:16 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 17:56:16 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 17:56:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:56:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:56:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 17:56:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:56:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 17:56:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 17:56:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:49469 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:56:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:73
2016-10-01 17:56:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:56:18 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 306.169383 ms
2016-10-01 17:56:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:73
2016-10-01 17:56:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:73) with 1 output partitions
2016-10-01 17:56:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:73)
2016-10-01 17:56:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:56:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:73), which has no missing parents
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:49469 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:73)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.233033 ms
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-01 17:56:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 85 ms on localhost (1/1)
2016-10-01 17:56:19 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:73) finished in 0.085 s
2016-10-01 17:56:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:73, took 0.111571 s
2016-10-01 17:56:19 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 19.325017 ms
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:49469 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:77
2016-10-01 17:56:19 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:77
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:77) with 1 output partitions
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:77)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:77), which has no missing parents
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:49469 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:77)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 17:56:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 28 ms on localhost (1/1)
2016-10-01 17:56:19 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:77) finished in 0.029 s
2016-10-01 17:56:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:77, took 0.042314 s
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:49469 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:80
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:80
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:80) with 1 output partitions
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:80)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:80), which has no missing parents
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:49469 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:80)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 17:56:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 19 ms on localhost (1/1)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:80) finished in 0.020 s
2016-10-01 17:56:19 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 17:56:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:80, took 0.028478 s
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:49469 (size: 14.9 KB, free: 912.2 MB)
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:84
2016-10-01 17:56:19 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:84
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:84) with 1 output partitions
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:84)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:84), which has no missing parents
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:49469 (size: 2.6 KB, free: 912.2 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:84)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+243
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1650 bytes result sent to driver
2016-10-01 17:56:19 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (1/1)
2016-10-01 17:56:19 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:84) finished in 0.013 s
2016-10-01 17:56:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:84, took 0.024270 s
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-01 17:56:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:49469 (size: 14.6 KB, free: 912.2 MB)
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:87
2016-10-01 17:56:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 17:56:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:87
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:87) with 1 output partitions
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:87)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:87), which has no missing parents
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:49469 (size: 4.1 KB, free: 912.2 MB)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:87)
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.824641 ms
2016-10-01 17:56:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-01 17:56:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 30 ms on localhost (1/1)
2016-10-01 17:56:19 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 17:56:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:87) finished in 0.031 s
2016-10-01 17:56:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:87, took 0.046433 s
2016-10-01 17:56:19 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 20.019484 ms
2016-10-01 17:56:19 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 17:56:19 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@60f70249{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 17:56:19 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 17:56:19 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 17:56:19 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 17:56:19 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 17:56:19 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 17:56:19 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 17:56:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 17:56:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-8be7fa3d-01ea-4268-ae8d-cfe40ab51c95
2016-10-01 18:02:21 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 18:02:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:02:26 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:02:26 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 49523.
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:02:26 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-e57ae37c-337b-4c6d-b48c-cef747a978c9
2016-10-01 18:02:26 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:02:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:02:27 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @6194ms
2016-10-01 18:02:27 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:02:27 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:02:27 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @6315ms
2016-10-01 18:02:27 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:02:27 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 18:02:27 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:02:27 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49524.
2016-10-01 18:02:27 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:49524
2016-10-01 18:02:27 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 49524)
2016-10-01 18:02:27 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:49524 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 49524)
2016-10-01 18:02:27 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 49524)
2016-10-01 18:02:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 18:02:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 18:02:27 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:49524 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 18:02:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 18:02:28 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:02:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:49524 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:49524 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:49524 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2174 bytes result sent to driver
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 18:02:28 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 223 ms on localhost (1/2)
2016-10-01 18:02:28 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 265 ms on localhost (2/2)
2016-10-01 18:02:28 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.284 s
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:49524 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 18:02:28 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 18:02:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2433 bytes result sent to driver
2016-10-01 18:02:28 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2463 bytes result sent to driver
2016-10-01 18:02:28 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 82 ms on localhost (1/2)
2016-10-01 18:02:28 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 89 ms on localhost (2/2)
2016-10-01 18:02:28 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.091 s
2016-10-01 18:02:28 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.590407 s
2016-10-01 18:02:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 18:02:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:49524 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 59 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 57 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.061 s
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:49524 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4661 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 46 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4253 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 48 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.051 s
2016-10-01 18:02:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.142942 s
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:49524 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2245 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 44 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2250 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 50 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.051 s
2016-10-01 18:02:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.070062 s
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:49524 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 39 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 41 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.044 s
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:49524 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 36 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 42 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.046 s
2016-10-01 18:02:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.118027 s
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:59)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 192.168.8.100:49524 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2337 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 27 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2205 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 29 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:59) finished in 0.032 s
2016-10-01 18:02:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:59, took 0.051518 s
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTextFile at SparkApplication.java:60
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (mapToPair at SparkApplication.java:59)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTextFile at SparkApplication.java:60) with 2 output partitions
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 18 (saveAsTextFile at SparkApplication.java:60)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 17)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 17)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 17 (MapPartitionsRDD[12] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.7 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 192.168.8.100:49524 (size: 2.7 KB, free: 912.3 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[12] at mapToPair at SparkApplication.java:59)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 17.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 17.0 (TID 16, localhost, partition 0, ANY, 5137 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 17.0 (TID 17, localhost, partition 1, ANY, 5137 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 17.0 (TID 16)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 17.0 (TID 17)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 17.0 (TID 17). 1881 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 17.0 (TID 17) in 26 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 17.0 (TID 16). 1881 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 17.0 (TID 16) in 30 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 17.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 17 (mapToPair at SparkApplication.java:59) finished in 0.031 s
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 18)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 18 (MapPartitionsRDD[16] at saveAsTextFile at SparkApplication.java:60), which has no missing parents
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 49.9 KB, free 912.1 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 18.1 KB, free 912.0 MB)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 192.168.8.100:49524 (size: 18.1 KB, free: 912.2 MB)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[16] at saveAsTextFile at SparkApplication.java:60)
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 18.0 with 2 tasks
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 18.0 (TID 18, localhost, partition 0, ANY, 5148 bytes)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 18.0 (TID 19, localhost, partition 1, ANY, 5148 bytes)
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 18.0 (TID 18)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 18.0 (TID 19)
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610011802_0018_m_000000_18' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/output/_temporary/0/task_201610011802_0018_m_000000
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610011802_0018_m_000001_19' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/output/_temporary/0/task_201610011802_0018_m_000001
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610011802_0018_m_000000_18: Committed
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610011802_0018_m_000001_19: Committed
2016-10-01 18:02:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 18.0 (TID 19). 1553 bytes result sent to driver
2016-10-01 18:02:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 18.0 (TID 18). 1553 bytes result sent to driver
2016-10-01 18:02:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 18.0 (TID 19) in 98 ms on localhost (1/2)
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 18.0 (TID 18) in 101 ms on localhost (2/2)
2016-10-01 18:02:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 18.0, whose tasks have all completed, from pool 
2016-10-01 18:02:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 18 (saveAsTextFile at SparkApplication.java:60) finished in 0.102 s
2016-10-01 18:02:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTextFile at SparkApplication.java:60, took 0.177912 s
2016-10-01 18:02:29 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:02:29 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:02:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:02:29 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:02:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:02:29 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 49525.
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:02:29 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-a1e668cf-2f8b-45f7-b438-e407ee6f1f42
2016-10-01 18:02:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:02:29 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:02:29 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:02:29 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7035da8d{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:02:29 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @9018ms
2016-10-01 18:02:29 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:02:29 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 18:02:29 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:02:29 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49526.
2016-10-01 18:02:29 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:49526
2016-10-01 18:02:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 49526)
2016-10-01 18:02:29 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:49526 with 912.3 MB RAM, BlockManagerId(driver, localhost, 49526)
2016-10-01 18:02:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 49526)
2016-10-01 18:02:29 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 18:02:30 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 18:02:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 18:02:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 18:02:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 18:02:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 18:02:31 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 18:02:31 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 18:02:31 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:49526 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 18:02:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:73
2016-10-01 18:02:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 18:02:31 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 205.009308 ms
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:73
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:73) with 1 output partitions
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:73)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:73), which has no missing parents
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:49526 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:73)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.977186 ms
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-01 18:02:32 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 78 ms on localhost (1/1)
2016-10-01 18:02:32 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:73) finished in 0.080 s
2016-10-01 18:02:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:73, took 0.120758 s
2016-10-01 18:02:32 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.342465 ms
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:49526 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:76
2016-10-01 18:02:32 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:76
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:76) with 1 output partitions
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:76)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:76), which has no missing parents
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:49526 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:76)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 18:02:32 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on localhost (1/1)
2016-10-01 18:02:32 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:76) finished in 0.021 s
2016-10-01 18:02:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:76, took 0.027664 s
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:49526 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:79
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:79
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:79) with 1 output partitions
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:79)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:79), which has no missing parents
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:49526 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:79)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 18:02:32 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 22 ms on localhost (1/1)
2016-10-01 18:02:32 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:79) finished in 0.022 s
2016-10-01 18:02:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:79, took 0.031755 s
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:49526 (size: 14.9 KB, free: 912.2 MB)
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:82
2016-10-01 18:02:32 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:82
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:82) with 1 output partitions
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:82)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82), which has no missing parents
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:49526 (size: 2.6 KB, free: 912.2 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:82)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+243
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1737 bytes result sent to driver
2016-10-01 18:02:32 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 32 ms on localhost (1/1)
2016-10-01 18:02:32 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:82) finished in 0.033 s
2016-10-01 18:02:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:82, took 0.040024 s
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-01 18:02:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:49526 (size: 14.6 KB, free: 912.2 MB)
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:85
2016-10-01 18:02:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 18:02:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:85
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:85) with 1 output partitions
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:85)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85), which has no missing parents
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:49526 (size: 4.1 KB, free: 912.2 MB)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:85)
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-01 18:02:32 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.31223 ms
2016-10-01 18:02:32 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-01 18:02:32 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 21 ms on localhost (1/1)
2016-10-01 18:02:32 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:02:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:85) finished in 0.021 s
2016-10-01 18:02:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:85, took 0.027784 s
2016-10-01 18:02:32 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.03398 ms
2016-10-01 18:02:33 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 18:02:33 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7035da8d{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:02:33 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 18:02:33 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:02:33 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:02:33 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:02:33 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:02:33 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:02:33 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:02:33 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 18:02:33 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-f953dfe7-230a-487b-b774-62e0b2c15ea0
2016-10-01 18:09:19 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 18:09:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:09:25 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 18:09:25 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:09:25 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:09:25 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:09:25 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:09:25 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:09:26 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 49871.
2016-10-01 18:09:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:09:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:09:26 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-5690219c-e237-4dae-bd93-77c4e87fc719
2016-10-01 18:09:26 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:09:26 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:09:26 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @7624ms
2016-10-01 18:09:26 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:09:26 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@3e9956ce{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:09:26 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @7782ms
2016-10-01 18:09:26 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:09:26 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 18:09:26 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:09:26 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49872.
2016-10-01 18:09:26 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:49872
2016-10-01 18:09:26 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 49872)
2016-10-01 18:09:26 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:49872 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 49872)
2016-10-01 18:09:27 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 49872)
2016-10-01 18:09:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 18:09:28 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 18:09:28 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:49872 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 18:09:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 18:09:28 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:09:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 18:09:28 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:49872 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 18:09:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:49872 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:49872 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 292 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 381 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.420 s
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:49872 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2478 bytes result sent to driver
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2505 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 78 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 82 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.082 s
2016-10-01 18:09:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.793107 s
2016-10-01 18:09:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:49872 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 38 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 37 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.041 s
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:49872 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4578 bytes result sent to driver
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4763 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 29 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 32 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.032 s
2016-10-01 18:09:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.100653 s
2016-10-01 18:09:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 157 bytes
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:49872 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2225 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 21 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2225 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 26 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.027 s
2016-10-01 18:09:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.044534 s
2016-10-01 18:09:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:49872 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 28 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 32 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.033 s
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:49872 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 16 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 18 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.020 s
2016-10-01 18:09:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.077308 s
2016-10-01 18:09:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 157 bytes
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:59)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 192.168.8.100:49872 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59)
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:09:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2327 bytes result sent to driver
2016-10-01 18:09:29 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2220 bytes result sent to driver
2016-10-01 18:09:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 15 ms on localhost (1/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 19 ms on localhost (2/2)
2016-10-01 18:09:29 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-01 18:09:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:59) finished in 0.021 s
2016-10-01 18:09:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:59, took 0.038044 s
2016-10-01 18:09:29 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@3e9956ce{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:09:29 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:09:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:09:29 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:09:29 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:09:29 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:09:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:09:29 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 18:09:29 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-fe37ed35-c9cb-4d59-b29c-80b68dd2fda0
2016-10-01 18:14:38 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 18:14:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:14:44 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:14:44 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 49990.
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:14:44 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:14:44 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-334706af-f718-4992-ab13-df741b0142dd
2016-10-01 18:14:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:14:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:14:45 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @7092ms
2016-10-01 18:14:45 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:14:45 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@1c3a900c{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:14:45 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @7256ms
2016-10-01 18:14:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:14:45 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 18:14:45 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:14:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49991.
2016-10-01 18:14:45 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:49991
2016-10-01 18:14:45 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 49991)
2016-10-01 18:14:45 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:49991 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 49991)
2016-10-01 18:14:45 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 49991)
2016-10-01 18:14:46 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 18:14:46 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 18:14:46 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:49991 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 18:14:46 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 18:14:46 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:14:46 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:53
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:50)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:53) with 2 output partitions
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:53)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50), which has no missing parents
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 18:14:46 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:49991 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:50)
2016-10-01 18:14:46 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:49991 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:49991 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 227 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 267 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:50) finished in 0.291 s
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:49991 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:53)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2396 bytes result sent to driver
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2402 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 82 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 86 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:53) finished in 0.088 s
2016-10-01 18:14:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:53, took 0.654560 s
2016-10-01 18:14:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:54
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:53)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:54) with 2 output partitions
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:54)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:49991 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:53)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 36 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 38 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:53) finished in 0.040 s
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:49991 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:53)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4099 bytes result sent to driver
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 3957 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 29 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 31 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:54) finished in 0.032 s
2016-10-01 18:14:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:54, took 0.106314 s
2016-10-01 18:14:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 160 bytes
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:56)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:49991 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:56)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2225 bytes result sent to driver
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2225 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 16 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 19 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:56) finished in 0.020 s
2016-10-01 18:14:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:56, took 0.036168 s
2016-10-01 18:14:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:57
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:56)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:57) with 2 output partitions
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:57)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:49991 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:56)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 25 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 31 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:56) finished in 0.032 s
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:49991 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:49991 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:56)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:49991 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:49991 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:49991 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:49991 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 19 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 18 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:57) finished in 0.020 s
2016-10-01 18:14:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:57, took 0.079147 s
2016-10-01 18:14:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 160 bytes
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:59)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.2 MB)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 192.168.8.100:49991 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:59)
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:14:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2250 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 11 ms on localhost (1/2)
2016-10-01 18:14:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2225 bytes result sent to driver
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 16 ms on localhost (2/2)
2016-10-01 18:14:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-01 18:14:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:59) finished in 0.017 s
2016-10-01 18:14:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:59, took 0.032487 s
2016-10-01 18:14:47 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@1c3a900c{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:14:47 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:14:47 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:14:47 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:14:47 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:14:47 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:14:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:14:47 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 18:14:47 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-39a2fe55-5ab6-4d1b-8dfe-11029a2cbb00
2016-10-01 18:16:27 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 18:16:27 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:16:45 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:16:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 50020.
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:16:45 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-2dbc9c7d-849c-40d8-bacb-316e2a883de9
2016-10-01 18:16:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:16:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:16:45 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @18885ms
2016-10-01 18:16:45 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:16:45 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:16:45 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @19014ms
2016-10-01 18:16:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:16:45 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 18:16:45 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:16:46 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50021.
2016-10-01 18:16:46 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:50021
2016-10-01 18:16:46 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 50021)
2016-10-01 18:16:46 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:50021 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 50021)
2016-10-01 18:16:46 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 50021)
2016-10-01 18:16:46 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 18:16:46 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 18:16:46 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:50021 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 18:16:46 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 18:16:47 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:16:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:54
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:51)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:54) with 2 output partitions
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:54)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:50021 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:50021 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:50021 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 248 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 212 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:51) finished in 0.270 s
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:50021 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2402 bytes result sent to driver
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2396 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 71 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 70 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:54) finished in 0.074 s
2016-10-01 18:16:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:54, took 0.542788 s
2016-10-01 18:16:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:55
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:54)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:55) with 2 output partitions
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:55)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:50021 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 42 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 45 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:54) finished in 0.046 s
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:50021 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 3957 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 48 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 4186 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 49 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:55) finished in 0.052 s
2016-10-01 18:16:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:55, took 0.127202 s
2016-10-01 18:16:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:57
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 160 bytes
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:57) with 2 output partitions
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:57)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:50021 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2225 bytes result sent to driver
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2225 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 23 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 26 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:57) finished in 0.027 s
2016-10-01 18:16:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:57, took 0.040766 s
2016-10-01 18:16:47 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:58
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:57)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:58) with 2 output partitions
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:58)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:50021 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 2 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1881 bytes result sent to driver
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1881 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 27 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 30 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:57) finished in 0.031 s
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:50021 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57)
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:16:47 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:16:47 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-01 18:16:47 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 18:16:47 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 22 ms on localhost (1/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 24 ms on localhost (2/2)
2016-10-01 18:16:47 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 18:16:47 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:58) finished in 0.024 s
2016-10-01 18:16:47 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:58, took 0.079588 s
2016-10-01 18:16:48 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:60
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 160 bytes
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:60) with 2 output partitions
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:60)
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:60), which has no missing parents
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:16:48 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 192.168.8.100:50021 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:60)
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-01 18:16:48 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:16:48 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:16:48 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-01 18:16:48 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-01 18:16:48 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:48 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:16:48 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:48 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:16:48 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2312 bytes result sent to driver
2016-10-01 18:16:48 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 22 ms on localhost (1/2)
2016-10-01 18:16:48 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2250 bytes result sent to driver
2016-10-01 18:16:48 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 24 ms on localhost (2/2)
2016-10-01 18:16:48 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-01 18:16:48 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:60) finished in 0.026 s
2016-10-01 18:16:48 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:60, took 0.041637 s
2016-10-01 18:16:48 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:16:48 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 18:16:48 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:16:48 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:16:48 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:16:48 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:16:48 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:16:48 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:16:48 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 18:16:48 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-1ce4dac9-5652-49fb-9039-e514d42eae73
2016-10-01 18:17:26 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-01 18:17:26 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:17:33 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:17:33 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 50025.
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:17:33 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-370e4449-7d64-4fe0-b1b5-a99732b531ff
2016-10-01 18:17:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:17:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:17:33 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @8044ms
2016-10-01 18:17:33 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:17:33 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@608ff5d8{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:17:33 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @8175ms
2016-10-01 18:17:33 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:17:33 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://192.168.8.100:4040
2016-10-01 18:17:34 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:17:34 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50026.
2016-10-01 18:17:34 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 192.168.8.100:50026
2016-10-01 18:17:34 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 192.168.8.100, 50026)
2016-10-01 18:17:34 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 192.168.8.100:50026 with 912.3 MB RAM, BlockManagerId(driver, 192.168.8.100, 50026)
2016-10-01 18:17:34 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 192.168.8.100, 50026)
2016-10-01 18:17:34 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-01 18:17:34 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-01 18:17:34 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 192.168.8.100:50026 (size: 10.2 KB, free: 912.3 MB)
2016-10-01 18:17:34 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-01 18:17:34 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:17:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:54
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:51)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:54) with 2 output partitions
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:54)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 192.168.8.100:50026 (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 192.168.8.100:50026 (size: 5.7 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 192.168.8.100:50026 (size: 4.9 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2261 bytes result sent to driver
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2261 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 232 ms on localhost (1/2)
2016-10-01 18:17:35 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 280 ms on localhost (2/2)
2016-10-01 18:17:35 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:51) finished in 0.302 s
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 192.168.8.100:50026 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 71 ms on localhost (1/2)
2016-10-01 18:17:35 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 76 ms on localhost (2/2)
2016-10-01 18:17:35 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:54) finished in 0.077 s
2016-10-01 18:17:35 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:54, took 0.581381 s
2016-10-01 18:17:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:55
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:54)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:55) with 2 output partitions
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:55)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 192.168.8.100:50026 (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1881 bytes result sent to driver
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1881 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 45 ms on localhost (1/2)
2016-10-01 18:17:35 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 45 ms on localhost (2/2)
2016-10-01 18:17:35 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:54) finished in 0.047 s
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 192.168.8.100:50026 (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3896 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 43 ms on localhost (1/2)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 49 ms on localhost (2/2)
2016-10-01 18:17:35 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:55) finished in 0.050 s
2016-10-01 18:17:35 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:55, took 0.128067 s
2016-10-01 18:17:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:57
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 160 bytes
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:57) with 2 output partitions
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:57)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 192.168.8.100:50026 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2342 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 29 ms on localhost (1/2)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 34 ms on localhost (2/2)
2016-10-01 18:17:35 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:57) finished in 0.034 s
2016-10-01 18:17:35 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:57, took 0.052408 s
2016-10-01 18:17:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:58
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:57)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:58) with 2 output partitions
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:58)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 192.168.8.100:50026 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1954 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 43 ms on localhost (1/2)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1954 bytes result sent to driver
2016-10-01 18:17:35 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 52 ms on localhost (2/2)
2016-10-01 18:17:35 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:57) finished in 0.053 s
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 192.168.8.100:50026 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 192.168.8.100:50026 (size: 2.4 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 192.168.8.100:50026 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57)
2016-10-01 18:17:35 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 192.168.8.100:50026 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 192.168.8.100:50026 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 192.168.8.100:50026 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:35 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-01 18:17:36 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 26 ms on localhost (1/2)
2016-10-01 18:17:36 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 28 ms on localhost (2/2)
2016-10-01 18:17:36 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:58) finished in 0.029 s
2016-10-01 18:17:36 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:58, took 0.111782 s
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:60
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 160 bytes
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 160 bytes
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:60) with 2 output partitions
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:60)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:60), which has no missing parents
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.2 MB)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.2 MB)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 192.168.8.100:50026 (size: 2.8 KB, free: 912.3 MB)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:60)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2240 bytes result sent to driver
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2312 bytes result sent to driver
2016-10-01 18:17:36 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 19 ms on localhost (1/2)
2016-10-01 18:17:36 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 18 ms on localhost (2/2)
2016-10-01 18:17:36 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:60) finished in 0.021 s
2016-10-01 18:17:36 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:60, took 0.034249 s
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTextFile at SparkApplication.java:61
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (mapToPair at SparkApplication.java:60)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTextFile at SparkApplication.java:61) with 2 output partitions
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 18 (saveAsTextFile at SparkApplication.java:61)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 17)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 17)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 17 (MapPartitionsRDD[12] at mapToPair at SparkApplication.java:60), which has no missing parents
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 192.168.8.100:50026 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[12] at mapToPair at SparkApplication.java:60)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 17.0 with 2 tasks
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 17.0 (TID 16, localhost, partition 0, ANY, 5137 bytes)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 17.0 (TID 17, localhost, partition 1, ANY, 5137 bytes)
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 17.0 (TID 16)
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 17.0 (TID 17)
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 17.0 (TID 17). 1881 bytes result sent to driver
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 17.0 (TID 16). 1881 bytes result sent to driver
2016-10-01 18:17:36 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 17.0 (TID 17) in 17 ms on localhost (1/2)
2016-10-01 18:17:36 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 17.0 (TID 16) in 19 ms on localhost (2/2)
2016-10-01 18:17:36 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 17.0, whose tasks have all completed, from pool 
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 17 (mapToPair at SparkApplication.java:60) finished in 0.020 s
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 18)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 18 (MapPartitionsRDD[16] at saveAsTextFile at SparkApplication.java:61), which has no missing parents
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 49.9 KB, free 912.1 MB)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 18.1 KB, free 912.1 MB)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 192.168.8.100:50026 (size: 18.1 KB, free: 912.3 MB)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 18 (MapPartitionsRDD[16] at saveAsTextFile at SparkApplication.java:61)
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 18.0 with 2 tasks
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 18.0 (TID 18, localhost, partition 0, ANY, 5148 bytes)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 18.0 (TID 19, localhost, partition 1, ANY, 5148 bytes)
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 18.0 (TID 18)
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 18.0 (TID 19)
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610011817_0018_m_000001_19' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/output/_temporary/0/task_201610011817_0018_m_000001
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610011817_0018_m_000000_18' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/output/_temporary/0/task_201610011817_0018_m_000000
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610011817_0018_m_000000_18: Committed
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610011817_0018_m_000001_19: Committed
2016-10-01 18:17:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 18.0 (TID 18). 1553 bytes result sent to driver
2016-10-01 18:17:36 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 18.0 (TID 18) in 85 ms on localhost (1/2)
2016-10-01 18:17:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 18.0 (TID 19). 1553 bytes result sent to driver
2016-10-01 18:17:36 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 18.0 (TID 19) in 85 ms on localhost (2/2)
2016-10-01 18:17:36 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 18.0, whose tasks have all completed, from pool 
2016-10-01 18:17:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 18 (saveAsTextFile at SparkApplication.java:61) finished in 0.087 s
2016-10-01 18:17:36 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTextFile at SparkApplication.java:61, took 0.144964 s
2016-10-01 18:17:36 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@608ff5d8{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:17:36 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://192.168.8.100:4040
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:17:36 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:17:36 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:17:36 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-01 18:17:36 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 50029.
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-01 18:17:36 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-3e06903c-ed71-429d-90c6-11422114b62c
2016-10-01 18:17:36 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-01 18:17:36 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-01 18:17:36 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-01 18:17:36 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@606376b{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:17:36 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @10710ms
2016-10-01 18:17:36 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-01 18:17:36 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-01 18:17:36 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-01 18:17:36 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50030.
2016-10-01 18:17:36 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:50030
2016-10-01 18:17:36 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 50030)
2016-10-01 18:17:36 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:50030 with 912.3 MB RAM, BlockManagerId(driver, localhost, 50030)
2016-10-01 18:17:36 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 50030)
2016-10-01 18:17:36 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-01 18:17:36 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-01 18:17:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 18:17:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 18:17:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-01 18:17:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 18:17:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-01 18:17:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-01 18:17:38 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:50030 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 18:17:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:74
2016-10-01 18:17:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 18:17:39 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 359.611207 ms
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:74
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:74) with 1 output partitions
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:74)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:74), which has no missing parents
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:50030 (size: 3.6 KB, free: 912.3 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:74)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.31072 ms
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-01 18:17:39 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 80 ms on localhost (1/1)
2016-10-01 18:17:39 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:74) finished in 0.082 s
2016-10-01 18:17:39 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:74, took 0.114381 s
2016-10-01 18:17:39 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.209937 ms
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:50030 (size: 14.9 KB, free: 912.3 MB)
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:77
2016-10-01 18:17:39 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:77
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:77) with 1 output partitions
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:77)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:77), which has no missing parents
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:50030 (size: 2.6 KB, free: 912.3 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:77)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-01 18:17:39 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 14 ms on localhost (1/1)
2016-10-01 18:17:39 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:77) finished in 0.015 s
2016-10-01 18:17:39 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:77, took 0.022803 s
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:50030 (size: 14.6 KB, free: 912.3 MB)
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:80
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:80
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:80) with 1 output partitions
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:80)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:80), which has no missing parents
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:50030 (size: 4.0 KB, free: 912.2 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:80)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-01 18:17:39 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 21 ms on localhost (1/1)
2016-10-01 18:17:39 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:80) finished in 0.021 s
2016-10-01 18:17:39 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:80, took 0.030870 s
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:50030 (size: 14.9 KB, free: 912.2 MB)
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:83
2016-10-01 18:17:39 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:83
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:83) with 1 output partitions
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:83)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:83), which has no missing parents
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:50030 (size: 2.6 KB, free: 912.2 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:83)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+243
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1650 bytes result sent to driver
2016-10-01 18:17:39 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 16 ms on localhost (1/1)
2016-10-01 18:17:39 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:83) finished in 0.017 s
2016-10-01 18:17:39 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:83, took 0.024225 s
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-01 18:17:39 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:50030 (size: 14.6 KB, free: 912.2 MB)
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:86
2016-10-01 18:17:39 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-01 18:17:39 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:86
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:86) with 1 output partitions
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:86)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:86), which has no missing parents
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:50030 (size: 4.1 KB, free: 912.2 MB)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:86)
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.763519 ms
2016-10-01 18:17:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-01 18:17:39 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 25 ms on localhost (1/1)
2016-10-01 18:17:39 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-01 18:17:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:86) finished in 0.025 s
2016-10-01 18:17:39 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:86, took 0.034111 s
2016-10-01 18:17:39 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.255758 ms
2016-10-01 18:17:39 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-01 18:17:39 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@606376b{HTTP/1.1}{0.0.0.0:4040}
2016-10-01 18:17:39 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-01 18:17:39 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-01 18:17:39 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-01 18:17:39 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-01 18:17:39 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-01 18:17:39 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-01 18:17:39 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-01 18:17:39 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-dcb3be0e-5960-474c-9449-f61cf45db1b7
