2016-11-02 11:44:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-11-02 11:44:32 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-02 11:44:32 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-11-02 11:44:32 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-11-02 11:44:32 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-11-02 11:44:32 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-11-02 11:44:32 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-11-02 11:44:33 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 51634.
2016-11-02 11:44:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-11-02 11:44:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-11-02 11:44:33 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-aa7669be-fd11-46e8-b4ba-127174035404
2016-11-02 11:44:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-11-02 11:44:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-11-02 11:44:33 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2200ms
2016-11-02 11:44:33 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-11-02 11:44:33 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@36308f3a{HTTP/1.1}{0.0.0.0:4040}
2016-11-02 11:44:33 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2396ms
2016-11-02 11:44:33 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-11-02 11:44:33 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-11-02 11:44:34 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-11-02 11:44:34 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51635.
2016-11-02 11:44:34 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:51635
2016-11-02 11:44:34 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 51635)
2016-11-02 11:44:34 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:51635 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 51635)
2016-11-02 11:44:34 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 51635)
2016-11-02 11:44:34 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-11-02 11:44:34 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-11-02 11:44:36 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 132.3 KB, free 912.2 MB)
2016-11-02 11:44:36 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-11-02 11:44:36 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:51635 (size: 14.9 KB, free: 912.3 MB)
2016-11-02 11:44:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkDataset.java:30
2016-11-02 11:44:36 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-11-02 11:44:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkDataset.java:30
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkDataset.java:30) with 2 output partitions
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkDataset.java:30)
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkDataset.java:30), which has no missing parents
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-11-02 11:44:36 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:51635 (size: 2.6 KB, free: 912.3 MB)
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkDataset.java:30)
2016-11-02 11:44:36 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-11-02 11:44:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-11-02 11:44:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-11-02 11:44:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-11-02 11:44:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-11-02 11:44:36 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-11-02 11:44:36 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-11-02 11:44:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-11-02 11:44:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-11-02 11:44:36 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-11-02 11:44:36 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-11-02 11:44:36 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-11-02 11:44:37 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-11-02 11:44:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-11-02 11:44:37 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 310 ms on localhost (1/2)
2016-11-02 11:44:37 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 264 ms on localhost (2/2)
2016-11-02 11:44:37 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-11-02 11:44:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkDataset.java:30) finished in 0.337 s
2016-11-02 11:44:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkDataset.java:30, took 0.447285 s
2016-11-02 11:44:37 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:51635 in memory (size: 14.9 KB, free: 912.3 MB)
2016-11-02 11:44:37 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:51635 in memory (size: 2.6 KB, free: 912.3 MB)
2016-11-02 11:44:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:44:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:44:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-11-02 11:44:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:44:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.9 KB, free 912.2 MB)
2016-11-02 11:44:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-11-02 11:44:37 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:51635 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:44:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkDataset.java:33
2016-11-02 11:44:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:44:38 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 222.955345 ms
2016-11-02 11:44:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:33
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkDataset.java:33) with 1 output partitions
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkDataset.java:33)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkDataset.java:33), which has no missing parents
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:51635 (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkDataset.java:33)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.532721 ms
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-11-02 11:44:38 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 85 ms on localhost (1/1)
2016-11-02 11:44:38 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkDataset.java:33) finished in 0.086 s
2016-11-02 11:44:38 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkDataset.java:33, took 0.119923 s
2016-11-02 11:44:38 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.449193 ms
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:44:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:44:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:51635 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:44:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkDataset.java:35
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:44:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:35
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkDataset.java:35) with 1 output partitions
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkDataset.java:35)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkDataset.java:35), which has no missing parents
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:51635 (size: 4.0 KB, free: 912.3 MB)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkDataset.java:35)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.705003 ms
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1423 bytes result sent to driver
2016-11-02 11:44:38 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 34 ms on localhost (1/1)
2016-11-02 11:44:38 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkDataset.java:35) finished in 0.036 s
2016-11-02 11:44:38 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkDataset.java:35, took 0.049772 s
2016-11-02 11:44:38 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.451298 ms
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:44:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.9 KB, free 911.9 MB)
2016-11-02 11:44:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.8 MB)
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:51635 (size: 14.6 KB, free: 912.2 MB)
2016-11-02 11:44:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkDataset.java:36
2016-11-02 11:44:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:44:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkDataset.java:36
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkDataset.java:36)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkDataset.java:36) with 1 output partitions
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkDataset.java:36)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkDataset.java:36), which has no missing parents
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:51635 (size: 7.8 KB, free: 912.2 MB)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkDataset.java:36)
2016-11-02 11:44:38 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-11-02 11:44:38 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-11-02 11:44:38 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:44:39 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:51635 in memory (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:44:39 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-11-02 11:44:39 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-11-02 11:44:39 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:51635 in memory (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:44:39 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:51635 in memory (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:44:39 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-11-02 11:44:39 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-11-02 11:44:39 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:51635 in memory (size: 4.0 KB, free: 912.3 MB)
2016-11-02 11:44:39 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 20.508676 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 36.97314 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.505434 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 39.288412 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.26253 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1865 bytes result sent to driver
2016-11-02 11:44:39 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 432 ms on localhost (1/1)
2016-11-02 11:44:39 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkDataset.java:36) finished in 0.434 s
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkDataset.java:36), which has no missing parents
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 912.1 MB)
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 912.1 MB)
2016-11-02 11:44:39 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:51635 (size: 8.0 KB, free: 912.3 MB)
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkDataset.java:36)
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-11-02 11:44:39 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 9 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.337592 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.638622 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.706966 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.756602 ms
2016-11-02 11:44:39 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-11-02 11:44:39 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 171 ms on localhost (1/1)
2016-11-02 11:44:39 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-11-02 11:44:39 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkDataset.java:36) finished in 0.173 s
2016-11-02 11:44:39 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkDataset.java:36, took 0.714787 s
2016-11-02 11:44:39 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.30962 ms
2016-11-02 11:44:39 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.276407 ms
2016-11-02 11:44:39 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.23789 ms
2016-11-02 11:44:39 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:44:39 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-11-02 11:44:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:44:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:44:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-11-02 11:44:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:44:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:44:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:44:40 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:51635 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:44:40 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkDataset.java:40
2016-11-02 11:44:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:44:40 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:40
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkDataset.java:40) with 1 output partitions
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkDataset.java:40)
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkDataset.java:40), which has no missing parents
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 912.0 MB)
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.0 MB)
2016-11-02 11:44:40 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:51635 (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkDataset.java:40)
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-11-02 11:44:40 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:44:40 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-11-02 11:44:40 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:44:40 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-11-02 11:44:40 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 13 ms on localhost (1/1)
2016-11-02 11:44:40 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-11-02 11:44:40 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkDataset.java:40) finished in 0.014 s
2016-11-02 11:44:40 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkDataset.java:40, took 0.028864 s
2016-11-02 11:44:40 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:44:40 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-11-02 11:44:40 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.372877 ms
2016-11-02 11:44:40 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:44:40 ERROR [main] o.a.s.s.e.c.CreateDataSourceTableAsSelectCommand [Logging.scala:91] : Failed to write to table person in ErrorIfExists mode
org.apache.spark.sql.AnalysisException: path file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:88)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:246)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:378)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at hx.stream.spark.SparkDataset.main(SparkDataset.java:65)
2016-11-02 11:44:40 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-11-02 11:44:40 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@36308f3a{HTTP/1.1}{0.0.0.0:4040}
2016-11-02 11:44:40 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-11-02 11:44:40 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-11-02 11:44:40 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-11-02 11:44:40 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-11-02 11:44:40 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-11-02 11:44:40 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-11-02 11:44:40 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-11-02 11:44:40 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-11-02 11:44:40 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-36b0e274-0fc3-4155-a2cb-951d1242a23e
2016-11-02 11:49:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-11-02 11:49:01 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-02 11:49:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-11-02 11:49:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-11-02 11:49:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-11-02 11:49:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-11-02 11:49:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-11-02 11:49:02 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 51701.
2016-11-02 11:49:02 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-11-02 11:49:02 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-11-02 11:49:02 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-903d810c-671b-42b9-a5cb-a8f93fe2fffb
2016-11-02 11:49:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-11-02 11:49:02 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-11-02 11:49:02 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1920ms
2016-11-02 11:49:02 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-11-02 11:49:02 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@593417cb{HTTP/1.1}{0.0.0.0:4040}
2016-11-02 11:49:02 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2102ms
2016-11-02 11:49:02 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-11-02 11:49:02 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-11-02 11:49:02 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-11-02 11:49:02 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51702.
2016-11-02 11:49:02 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:51702
2016-11-02 11:49:02 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 51702)
2016-11-02 11:49:02 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:51702 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 51702)
2016-11-02 11:49:02 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 51702)
2016-11-02 11:49:03 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-11-02 11:49:03 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-11-02 11:49:04 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 132.3 KB, free 912.2 MB)
2016-11-02 11:49:04 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-11-02 11:49:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:51702 (size: 14.9 KB, free: 912.3 MB)
2016-11-02 11:49:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkDataset.java:30
2016-11-02 11:49:05 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-11-02 11:49:05 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkDataset.java:30
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkDataset.java:30) with 2 output partitions
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkDataset.java:30)
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkDataset.java:30), which has no missing parents
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-11-02 11:49:05 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:51702 (size: 2.6 KB, free: 912.3 MB)
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkDataset.java:30)
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-11-02 11:49:05 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-11-02 11:49:05 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-11-02 11:49:05 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-11-02 11:49:05 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-11-02 11:49:05 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-11-02 11:49:05 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-11-02 11:49:05 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-11-02 11:49:05 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-11-02 11:49:05 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-11-02 11:49:05 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-11-02 11:49:05 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-11-02 11:49:05 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-11-02 11:49:05 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-11-02 11:49:05 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-11-02 11:49:05 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 240 ms on localhost (1/2)
2016-11-02 11:49:05 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 296 ms on localhost (2/2)
2016-11-02 11:49:05 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-11-02 11:49:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkDataset.java:30) finished in 0.313 s
2016-11-02 11:49:05 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkDataset.java:30, took 0.424783 s
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:49:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:49:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:51702 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:49:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkDataset.java:33
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:51702 in memory (size: 14.9 KB, free: 912.3 MB)
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:51702 in memory (size: 2.6 KB, free: 912.3 MB)
2016-11-02 11:49:06 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 259.425347 ms
2016-11-02 11:49:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:33
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkDataset.java:33) with 1 output partitions
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkDataset.java:33)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkDataset.java:33), which has no missing parents
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:51702 (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkDataset.java:33)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.538576 ms
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-11-02 11:49:06 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 74 ms on localhost (1/1)
2016-11-02 11:49:06 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkDataset.java:33) finished in 0.077 s
2016-11-02 11:49:06 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkDataset.java:33, took 0.107665 s
2016-11-02 11:49:06 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.271964 ms
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:49:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:49:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:51702 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:49:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkDataset.java:35
2016-11-02 11:49:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:49:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:35
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkDataset.java:35) with 1 output partitions
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkDataset.java:35)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkDataset.java:35), which has no missing parents
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:51702 (size: 4.0 KB, free: 912.3 MB)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkDataset.java:35)
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-11-02 11:49:06 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.354454 ms
2016-11-02 11:49:06 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1336 bytes result sent to driver
2016-11-02 11:49:06 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 29 ms on localhost (1/1)
2016-11-02 11:49:06 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-11-02 11:49:06 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkDataset.java:35) finished in 0.031 s
2016-11-02 11:49:06 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkDataset.java:35, took 0.051988 s
2016-11-02 11:49:06 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.615914 ms
2016-11-02 11:49:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:49:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:49:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-11-02 11:49:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:49:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.9 KB, free 911.9 MB)
2016-11-02 11:49:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.8 MB)
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:51702 (size: 14.6 KB, free: 912.2 MB)
2016-11-02 11:49:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkDataset.java:36
2016-11-02 11:49:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:49:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkDataset.java:36
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkDataset.java:36)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkDataset.java:36) with 1 output partitions
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkDataset.java:36)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkDataset.java:36), which has no missing parents
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:51702 (size: 7.8 KB, free: 912.2 MB)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkDataset.java:36)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:51702 in memory (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:51702 in memory (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:49:07 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-11-02 11:49:07 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:51702 in memory (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:49:07 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-11-02 11:49:07 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:51702 in memory (size: 4.0 KB, free: 912.3 MB)
2016-11-02 11:49:07 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.583965 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 41.70701 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 35.101404 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 73.049716 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.131754 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1865 bytes result sent to driver
2016-11-02 11:49:07 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 475 ms on localhost (1/1)
2016-11-02 11:49:07 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkDataset.java:36) finished in 0.476 s
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkDataset.java:36), which has no missing parents
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 912.1 MB)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 912.1 MB)
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:51702 (size: 8.0 KB, free: 912.3 MB)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkDataset.java:36)
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-11-02 11:49:07 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 28.803593 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 19.484199 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.155084 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.509018 ms
2016-11-02 11:49:07 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-11-02 11:49:07 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 157 ms on localhost (1/1)
2016-11-02 11:49:07 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-11-02 11:49:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkDataset.java:36) finished in 0.157 s
2016-11-02 11:49:07 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkDataset.java:36, took 0.715536 s
2016-11-02 11:49:07 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.307219 ms
2016-11-02 11:49:07 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.04981 ms
2016-11-02 11:49:07 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.29438 ms
2016-11-02 11:49:08 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:49:08 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-11-02 11:49:08 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:49:08 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:49:08 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-11-02 11:49:08 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:49:08 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:49:08 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:49:08 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:51702 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:49:08 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkDataset.java:40
2016-11-02 11:49:08 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:49:08 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:40
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkDataset.java:40) with 1 output partitions
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkDataset.java:40)
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkDataset.java:40), which has no missing parents
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 912.0 MB)
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.0 MB)
2016-11-02 11:49:08 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:51702 (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkDataset.java:40)
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-11-02 11:49:08 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:49:08 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-11-02 11:49:08 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:49:08 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-11-02 11:49:08 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 15 ms on localhost (1/1)
2016-11-02 11:49:08 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-11-02 11:49:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkDataset.java:40) finished in 0.016 s
2016-11-02 11:49:08 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkDataset.java:40, took 0.029070 s
2016-11-02 11:49:08 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:49:08 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.646653 ms
2016-11-02 11:49:08 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-11-02 11:49:08 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:49:08 ERROR [main] o.a.s.s.e.c.CreateDataSourceTableAsSelectCommand [Logging.scala:91] : Failed to write to table person in ErrorIfExists mode
org.apache.spark.sql.AnalysisException: path file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:88)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:246)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:378)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at hx.stream.spark.SparkDataset.main(SparkDataset.java:65)
2016-11-02 11:49:08 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-11-02 11:49:08 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@593417cb{HTTP/1.1}{0.0.0.0:4040}
2016-11-02 11:49:08 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-11-02 11:49:08 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-11-02 11:49:08 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-11-02 11:49:08 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-11-02 11:49:08 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-11-02 11:49:08 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-11-02 11:49:08 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-11-02 11:49:08 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-11-02 11:49:08 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-aef858f1-0f1a-4f65-a553-fceae536ef54
2016-11-02 11:50:17 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-11-02 11:50:17 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-11-02 11:50:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-11-02 11:50:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-11-02 11:50:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-11-02 11:50:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-11-02 11:50:17 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-11-02 11:50:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 51731.
2016-11-02 11:50:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-11-02 11:50:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-11-02 11:50:18 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-0257c8eb-3a5d-43d2-8526-c3c6314fb684
2016-11-02 11:50:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-11-02 11:50:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-11-02 11:50:18 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2776ms
2016-11-02 11:50:18 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-11-02 11:50:18 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7c853486{HTTP/1.1}{0.0.0.0:4040}
2016-11-02 11:50:18 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @3007ms
2016-11-02 11:50:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-11-02 11:50:18 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-11-02 11:50:18 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-11-02 11:50:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51733.
2016-11-02 11:50:18 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:51733
2016-11-02 11:50:18 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 51733)
2016-11-02 11:50:18 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:51733 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 51733)
2016-11-02 11:50:18 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 51733)
2016-11-02 11:50:19 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-11-02 11:50:19 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-11-02 11:50:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 132.3 KB, free 912.2 MB)
2016-11-02 11:50:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-11-02 11:50:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:51733 (size: 14.9 KB, free: 912.3 MB)
2016-11-02 11:50:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkDataset.java:30
2016-11-02 11:50:21 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-11-02 11:50:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkDataset.java:30
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkDataset.java:30) with 2 output partitions
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkDataset.java:30)
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkDataset.java:30), which has no missing parents
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-11-02 11:50:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:51733 (size: 2.6 KB, free: 912.3 MB)
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkDataset.java:30)
2016-11-02 11:50:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-11-02 11:50:21 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-11-02 11:50:21 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-11-02 11:50:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-11-02 11:50:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-11-02 11:50:21 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-11-02 11:50:21 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-11-02 11:50:21 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-11-02 11:50:21 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-11-02 11:50:21 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-11-02 11:50:21 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-11-02 11:50:21 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-11-02 11:50:21 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-11-02 11:50:22 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-11-02 11:50:22 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-11-02 11:50:22 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 286 ms on localhost (1/2)
2016-11-02 11:50:22 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 239 ms on localhost (2/2)
2016-11-02 11:50:22 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-11-02 11:50:22 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkDataset.java:30) finished in 0.310 s
2016-11-02 11:50:22 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkDataset.java:30, took 0.404796 s
2016-11-02 11:50:22 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:51733 in memory (size: 14.9 KB, free: 912.3 MB)
2016-11-02 11:50:22 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:51733 in memory (size: 2.6 KB, free: 912.3 MB)
2016-11-02 11:50:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:50:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:50:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-11-02 11:50:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:50:22 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.9 KB, free 912.2 MB)
2016-11-02 11:50:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:51733 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkDataset.java:33
2016-11-02 11:50:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:50:23 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 234.279836 ms
2016-11-02 11:50:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:33
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkDataset.java:33) with 1 output partitions
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkDataset.java:33)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkDataset.java:33), which has no missing parents
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:51733 (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkDataset.java:33)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 19.567059 ms
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-11-02 11:50:23 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 81 ms on localhost (1/1)
2016-11-02 11:50:23 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkDataset.java:33) finished in 0.082 s
2016-11-02 11:50:23 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkDataset.java:33, took 0.119561 s
2016-11-02 11:50:23 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 40.447174 ms
2016-11-02 11:50:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:50:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:50:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-11-02 11:50:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:50:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:50:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:51733 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkDataset.java:35
2016-11-02 11:50:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:50:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:35
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkDataset.java:35) with 1 output partitions
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkDataset.java:35)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkDataset.java:35), which has no missing parents
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:51733 (size: 4.0 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkDataset.java:35)
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.910617 ms
2016-11-02 11:50:23 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1423 bytes result sent to driver
2016-11-02 11:50:23 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 36 ms on localhost (1/1)
2016-11-02 11:50:23 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-11-02 11:50:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkDataset.java:35) finished in 0.037 s
2016-11-02 11:50:23 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkDataset.java:35, took 0.053330 s
2016-11-02 11:50:23 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.044703 ms
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:51733 in memory (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:51733 in memory (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-11-02 11:50:23 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:51733 in memory (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:50:23 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-11-02 11:50:23 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-11-02 11:50:23 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:51733 in memory (size: 4.0 KB, free: 912.3 MB)
2016-11-02 11:50:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:50:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:50:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-11-02 11:50:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:50:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.9 KB, free 912.2 MB)
2016-11-02 11:50:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-11-02 11:50:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:51733 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:50:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkDataset.java:36
2016-11-02 11:50:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:50:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-11-02 11:50:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkDataset.java:36
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkDataset.java:36)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkDataset.java:36) with 1 output partitions
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkDataset.java:36)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkDataset.java:36), which has no missing parents
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 912.1 MB)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 912.1 MB)
2016-11-02 11:50:24 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:51733 (size: 7.8 KB, free: 912.3 MB)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkDataset.java:36)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-11-02 11:50:24 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 30.555175 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 99.019027 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.027327 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.407454 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.223129 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-11-02 11:50:24 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 328 ms on localhost (1/1)
2016-11-02 11:50:24 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkDataset.java:36) finished in 0.331 s
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkDataset.java:36), which has no missing parents
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 912.1 MB)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 912.1 MB)
2016-11-02 11:50:24 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:51733 (size: 8.0 KB, free: 912.3 MB)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkDataset.java:36)
2016-11-02 11:50:24 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-11-02 11:50:24 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 10 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 42.824425 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 26.10752 ms
2016-11-02 11:50:24 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 23.047282 ms
2016-11-02 11:50:25 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.046871 ms
2016-11-02 11:50:25 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-11-02 11:50:25 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 196 ms on localhost (1/1)
2016-11-02 11:50:25 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkDataset.java:36) finished in 0.197 s
2016-11-02 11:50:25 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkDataset.java:36, took 0.656395 s
2016-11-02 11:50:25 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.904701 ms
2016-11-02 11:50:25 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.785235 ms
2016-11-02 11:50:25 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.658739 ms
2016-11-02 11:50:25 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:50:25 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-11-02 11:50:25 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-11-02 11:50:25 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-11-02 11:50:25 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-11-02 11:50:25 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-11-02 11:50:25 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 131.9 KB, free 912.0 MB)
2016-11-02 11:50:25 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.0 MB)
2016-11-02 11:50:25 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:51733 (size: 14.6 KB, free: 912.3 MB)
2016-11-02 11:50:25 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkDataset.java:40
2016-11-02 11:50:25 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-11-02 11:50:25 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkDataset.java:40
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkDataset.java:40) with 1 output partitions
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkDataset.java:40)
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkDataset.java:40), which has no missing parents
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 912.0 MB)
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.0 MB)
2016-11-02 11:50:25 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:51733 (size: 4.1 KB, free: 912.3 MB)
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkDataset.java:40)
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-11-02 11:50:25 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-11-02 11:50:25 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-11-02 11:50:25 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-11-02 11:50:25 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-11-02 11:50:25 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 13 ms on localhost (1/1)
2016-11-02 11:50:25 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-11-02 11:50:25 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkDataset.java:40) finished in 0.014 s
2016-11-02 11:50:25 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkDataset.java:40, took 0.023286 s
2016-11-02 11:50:25 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:50:36 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 36.800478 ms
2016-11-02 11:51:44 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-11-02 11:51:53 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-11-02 11:51:54 ERROR [main] o.a.s.s.e.c.CreateDataSourceTableAsSelectCommand [Logging.scala:91] : Failed to write to table person in ErrorIfExists mode
org.apache.spark.sql.AnalysisException: path file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:88)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:246)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:378)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)
	at hx.stream.spark.SparkDataset.main(SparkDataset.java:65)
