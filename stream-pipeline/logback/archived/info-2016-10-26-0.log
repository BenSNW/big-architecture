2016-10-26 13:02:13 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-26 13:02:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 13:02:15 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 13:02:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 13:02:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 13:02:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 13:02:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 13:02:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 13:02:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 51853.
2016-10-26 13:02:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 13:02:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 13:02:16 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-65953c6c-0110-4716-9849-076ac84fb720
2016-10-26 13:02:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 13:02:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 13:02:16 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @3670ms
2016-10-26 13:02:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 13:02:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 13:02:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @3842ms
2016-10-26 13:02:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 13:02:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 13:02:16 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 13:02:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51854.
2016-10-26 13:02:16 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:51854
2016-10-26 13:02:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 51854)
2016-10-26 13:02:16 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:51854 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 51854)
2016-10-26 13:02:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 51854)
2016-10-26 13:02:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 13:02:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 13:02:17 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:51854 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 13:02:17 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-26 13:02:18 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 13:02:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:54
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:51)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:54) with 2 output partitions
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:54)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51), which has no missing parents
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:51854 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:51854 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:51854 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 13:02:18 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 310 ms on localhost (1/2)
2016-10-26 13:02:18 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 259 ms on localhost (2/2)
2016-10-26 13:02:18 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:51) finished in 0.339 s
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:51854 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54)
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 13:02:18 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-26 13:02:18 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 13:02:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 13:02:18 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 88 ms on localhost (1/2)
2016-10-26 13:02:18 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 93 ms on localhost (2/2)
2016-10-26 13:02:18 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 13:02:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:54) finished in 0.093 s
2016-10-26 13:02:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:54, took 0.648408 s
2016-10-26 13:02:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:55
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:54)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:55) with 2 output partitions
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:55)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54), which has no missing parents
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:51854 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 43 ms on localhost (1/2)
2016-10-26 13:02:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 43 ms on localhost (2/2)
2016-10-26 13:02:19 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:54) finished in 0.046 s
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:51854 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3896 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 35 ms on localhost (1/2)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 37 ms on localhost (2/2)
2016-10-26 13:02:19 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:55) finished in 0.038 s
2016-10-26 13:02:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:55, took 0.114882 s
2016-10-26 13:02:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:57
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:57) with 2 output partitions
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:57)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:51854 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 18 ms on localhost (1/2)
2016-10-26 13:02:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 17 ms on localhost (2/2)
2016-10-26 13:02:19 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:57) finished in 0.019 s
2016-10-26 13:02:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:57, took 0.041279 s
2016-10-26 13:02:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:58
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:57)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:58) with 2 output partitions
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:58)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57), which has no missing parents
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:51854 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:51854 in memory (size: 2.8 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 26 ms on localhost (1/2)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:51854 in memory (size: 3.2 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 27 ms on localhost (2/2)
2016-10-26 13:02:19 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:51854 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:57) finished in 0.030 s
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:51854 in memory (size: 2.5 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:51854 in memory (size: 2.0 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.2 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.2 MB)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:51854 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 19 ms on localhost (1/2)
2016-10-26 13:02:19 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 21 ms on localhost (2/2)
2016-10-26 13:02:19 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:58) finished in 0.023 s
2016-10-26 13:02:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:58, took 0.087227 s
2016-10-26 13:02:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:60
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (sortByKey at SparkApplication.java:60) with 2 output partitions
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 14 (sortByKey at SparkApplication.java:60)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 13)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:60), which has no missing parents
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 5.0 KB, free 912.2 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.2 MB)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:51854 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at sortByKey at SparkApplication.java:60)
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 14.0 with 2 tasks
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 14.0 (TID 14, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 14.0 (TID 15, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 14.0 (TID 14)
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 14.0 (TID 15)
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:02:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15). 2225 bytes result sent to driver
2016-10-26 13:02:19 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14). 2240 bytes result sent to driver
2016-10-26 13:02:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 14.0 (TID 15) in 15 ms on localhost (1/2)
2016-10-26 13:02:19 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 14.0 (TID 14) in 20 ms on localhost (2/2)
2016-10-26 13:02:19 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 14.0, whose tasks have all completed, from pool 
2016-10-26 13:02:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 14 (sortByKey at SparkApplication.java:60) finished in 0.021 s
2016-10-26 13:02:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: sortByKey at SparkApplication.java:60, took 0.038651 s
2016-10-26 13:02:19 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 13:02:19 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 13:02:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 13:02:19 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 13:02:19 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 13:02:19 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 13:02:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 13:02:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 13:02:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-658be59a-6d3d-491b-9a07-d44add15fb2a
2016-10-26 13:03:01 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:27] : starting spark-streaming-kafka
2016-10-26 13:03:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 13:03:01 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 13:03:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 13:03:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 13:03:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 13:03:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 13:03:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 13:03:02 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 51859.
2016-10-26 13:03:02 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 13:03:02 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 13:03:02 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-703882cc-384a-4213-955b-a60071588e02
2016-10-26 13:03:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 13:03:02 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 13:03:02 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1868ms
2016-10-26 13:03:02 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 13:03:02 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@5734ee87{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 13:03:02 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2001ms
2016-10-26 13:03:02 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 13:03:02 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 13:03:02 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 13:03:02 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51860.
2016-10-26 13:03:02 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:51860
2016-10-26 13:03:02 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 51860)
2016-10-26 13:03:02 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:51860 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 51860)
2016-10-26 13:03:02 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 51860)
2016-10-26 13:03:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 13:03:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 13:03:03 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:51860 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 13:03:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:34
2016-10-26 13:03:03 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 13:03:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:54
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:51)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:54) with 2 output partitions
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:54)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51), which has no missing parents
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 13:03:03 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:51860 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:51)
2016-10-26 13:03:03 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 13:03:03 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 13:03:03 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 13:03:03 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 13:03:03 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 13:03:03 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 13:03:03 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 13:03:03 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:51860 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 13:03:03 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:51860 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2175 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 274 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 244 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:51) finished in 0.297 s
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:51860 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:54)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 75 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 81 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:54) finished in 0.082 s
2016-10-26 13:03:04 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:54, took 0.592707 s
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:55
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:54)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:55) with 2 output partitions
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:55)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54), which has no missing parents
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:51860 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:54)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 46 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 46 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:54) finished in 0.048 s
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54), which has no missing parents
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:51860 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:54)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3809 bytes result sent to driver
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4186 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 32 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 33 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:55) finished in 0.034 s
2016-10-26 13:03:04 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:55, took 0.114529 s
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:57
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:57) with 2 output partitions
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:57)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:51860 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:57)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 26 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 29 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:57) finished in 0.030 s
2016-10-26 13:03:04 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:57, took 0.044657 s
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:58
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:57)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:58) with 2 output partitions
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:58)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57), which has no missing parents
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:51860 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:57)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 28 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 27 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:57) finished in 0.031 s
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:51860 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:57)
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 13:03:04 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 23 ms on localhost (1/2)
2016-10-26 13:03:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 27 ms on localhost (2/2)
2016-10-26 13:03:04 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 13:03:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:58) finished in 0.030 s
2016-10-26 13:03:04 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:58, took 0.088565 s
2016-10-26 13:03:04 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@5734ee87{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 13:03:04 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 13:03:04 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 13:03:04 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 13:03:04 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 13:03:04 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 51861.
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 13:03:04 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-940c67ce-5198-4a59-98d0-27d7c62e4db5
2016-10-26 13:03:04 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 13:03:04 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 13:03:04 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 13:03:04 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@534c6767{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 13:03:04 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @4157ms
2016-10-26 13:03:04 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 13:03:04 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 13:03:04 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 13:03:04 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51862.
2016-10-26 13:03:04 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:51862
2016-10-26 13:03:04 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 51862)
2016-10-26 13:03:04 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:51862 with 912.3 MB RAM, BlockManagerId(driver, localhost, 51862)
2016-10-26 13:03:04 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 51862)
2016-10-26 13:03:04 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 13:03:04 INFO  [main] o.a.spark.sql.internal.SharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 13:03:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 13:03:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 13:03:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-26 13:03:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 13:03:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 131.2 KB, free 912.2 MB)
2016-10-26 13:03:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.6 KB, free 912.2 MB)
2016-10-26 13:03:06 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:51862 (size: 14.6 KB, free: 912.3 MB)
2016-10-26 13:03:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:76
2016-10-26 13:03:06 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 13:03:07 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 265.920544 ms
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:76
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:76) with 1 output partitions
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:76)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:76), which has no missing parents
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.2 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:51862 (size: 3.6 KB, free: 912.3 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:76)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.905846 ms
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1358 bytes result sent to driver
2016-10-26 13:03:07 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 234 ms on localhost (1/1)
2016-10-26 13:03:07 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:76) finished in 0.236 s
2016-10-26 13:03:07 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:76, took 0.279997 s
2016-10-26 13:03:07 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.108593 ms
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 131.6 KB, free 912.0 MB)
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:51862 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:79
2016-10-26 13:03:07 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:79
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:79) with 1 output partitions
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:79)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:79), which has no missing parents
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:51862 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:79)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-26 13:03:07 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 26 ms on localhost (1/1)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:79) finished in 0.026 s
2016-10-26 13:03:07 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 13:03:07 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:79, took 0.038570 s
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 131.2 KB, free 911.9 MB)
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.9 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:51862 (size: 14.6 KB, free: 912.3 MB)
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:82
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:82
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:82) with 1 output partitions
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:82)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:82), which has no missing parents
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:51862 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:82)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-26 13:03:07 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 22 ms on localhost (1/1)
2016-10-26 13:03:07 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:82) finished in 0.022 s
2016-10-26 13:03:07 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:82, took 0.032762 s
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 131.6 KB, free 911.7 MB)
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:51862 (size: 14.9 KB, free: 912.2 MB)
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:85
2016-10-26 13:03:07 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:85
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:85) with 1 output partitions
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:85)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:85), which has no missing parents
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:51862 (size: 2.6 KB, free: 912.2 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:85)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+243
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1650 bytes result sent to driver
2016-10-26 13:03:07 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 11 ms on localhost (1/1)
2016-10-26 13:03:07 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:85) finished in 0.011 s
2016-10-26 13:03:07 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:85, took 0.018831 s
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 131.2 KB, free 911.6 MB)
2016-10-26 13:03:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.6 KB, free 911.6 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:51862 (size: 14.6 KB, free: 912.2 MB)
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:88
2016-10-26 13:03:07 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 13:03:07 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:88
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:88) with 1 output partitions
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:88)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:88), which has no missing parents
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:51862 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:88)
2016-10-26 13:03:07 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 13:03:07 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-26 13:03:07 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 13:03:08 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.395161 ms
2016-10-26 13:03:08 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-26 13:03:08 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 21 ms on localhost (1/1)
2016-10-26 13:03:08 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 13:03:08 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:88) finished in 0.022 s
2016-10-26 13:03:08 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:88, took 0.029857 s
2016-10-26 13:03:08 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.194469 ms
2016-10-26 13:03:08 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 13:03:08 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@534c6767{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 13:03:08 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 13:03:08 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 13:03:08 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 13:03:08 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 13:03:08 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 13:03:08 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 13:03:08 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 13:03:08 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 13:03:08 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-49677a4f-9d60-409e-a7dd-b5978a786fd2
2016-10-26 15:23:06 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:23:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:23:06 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:24:09 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:24:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:24:10 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:24:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:24:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:24:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:24:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:24:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:24:11 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 53877.
2016-10-26 15:24:11 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:24:11 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:24:11 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-03ab32db-7586-41c7-9271-e9a024febddf
2016-10-26 15:24:11 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:24:11 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:24:11 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2215ms
2016-10-26 15:24:11 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:24:11 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:24:11 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2392ms
2016-10-26 15:24:11 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:24:11 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 15:24:11 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:24:11 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53878.
2016-10-26 15:24:11 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:53878
2016-10-26 15:24:11 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 53878)
2016-10-26 15:24:11 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:53878 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 53878)
2016-10-26 15:24:11 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 53878)
2016-10-26 15:24:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 15:24:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 15:24:12 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:53878 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 15:24:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:36
2016-10-26 15:24:13 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:24:13 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:53)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:56)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53), which has no missing parents
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:53878 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:53878 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:53878 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 15:24:13 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 241 ms on localhost (1/2)
2016-10-26 15:24:13 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 302 ms on localhost (2/2)
2016-10-26 15:24:13 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:53) finished in 0.325 s
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:53878 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-26 15:24:13 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 15:24:13 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 15:24:13 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 84 ms on localhost (1/2)
2016-10-26 15:24:13 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 81 ms on localhost (2/2)
2016-10-26 15:24:13 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:56) finished in 0.086 s
2016-10-26 15:24:13 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:56, took 0.609759 s
2016-10-26 15:24:13 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:57
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:56)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:57) with 2 output partitions
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:57)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:53878 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56)
2016-10-26 15:24:13 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:24:13 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 83 ms on localhost (1/2)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 83 ms on localhost (2/2)
2016-10-26 15:24:14 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:56) finished in 0.087 s
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:53878 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3896 bytes result sent to driver
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 34 ms on localhost (1/2)
2016-10-26 15:24:14 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 36 ms on localhost (2/2)
2016-10-26 15:24:14 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:57) finished in 0.038 s
2016-10-26 15:24:14 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:57, took 0.161053 s
2016-10-26 15:24:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:59)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:53878 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 17 ms on localhost (1/2)
2016-10-26 15:24:14 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 19 ms on localhost (2/2)
2016-10-26 15:24:14 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:59) finished in 0.020 s
2016-10-26 15:24:14 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:59, took 0.037615 s
2016-10-26 15:24:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:60
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:59)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:60) with 2 output partitions
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:60)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:53878 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 16 ms on localhost (1/2)
2016-10-26 15:24:14 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 19 ms on localhost (2/2)
2016-10-26 15:24:14 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:59) finished in 0.019 s
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:53878 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59)
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:24:14 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 19 ms on localhost (1/2)
2016-10-26 15:24:14 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-26 15:24:14 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 18 ms on localhost (2/2)
2016-10-26 15:24:14 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 15:24:14 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:60) finished in 0.021 s
2016-10-26 15:24:14 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:60, took 0.061357 s
2016-10-26 15:24:14 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@26425897{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:24:14 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:24:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:24:14 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:24:14 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:24:14 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:24:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:24:14 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 15:24:14 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-60d8b5fe-8b2c-4770-9e51-11d1aeebe9e9
2016-10-26 15:33:12 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:33:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:33:12 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:33:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:33:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:33:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:33:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:33:12 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:33:13 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54006.
2016-10-26 15:33:13 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:33:13 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:33:13 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-7928cd46-6254-49b6-9984-cdf4c6384d17
2016-10-26 15:33:13 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:33:13 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:33:13 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2439ms
2016-10-26 15:33:13 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:33:13 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@5f2cab59{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:33:13 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2624ms
2016-10-26 15:33:13 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:33:13 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 15:33:14 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:33:14 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54007.
2016-10-26 15:33:14 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54007
2016-10-26 15:33:14 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54007)
2016-10-26 15:33:14 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54007 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54007)
2016-10-26 15:33:14 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54007)
2016-10-26 15:33:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 15:33:14 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 15:33:14 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54007 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 15:33:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:36
2016-10-26 15:33:15 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:33:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:53)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:56)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53), which has no missing parents
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54007 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:54007 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:54007 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 15:33:15 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 279 ms on localhost (1/2)
2016-10-26 15:33:15 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 241 ms on localhost (2/2)
2016-10-26 15:33:15 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:53) finished in 0.308 s
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54007 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56)
2016-10-26 15:33:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 15:33:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 15:33:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 15:33:15 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 7 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2324 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 101 ms on localhost (1/2)
2016-10-26 15:33:16 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 108 ms on localhost (2/2)
2016-10-26 15:33:16 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:56) finished in 0.110 s
2016-10-26 15:33:16 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:56, took 0.608610 s
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:57
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:56)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:57) with 2 output partitions
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:57)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54007 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 37 ms on localhost (1/2)
2016-10-26 15:33:16 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 36 ms on localhost (2/2)
2016-10-26 15:33:16 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:56) finished in 0.039 s
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54007 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3809 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 35 ms on localhost (1/2)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 39 ms on localhost (2/2)
2016-10-26 15:33:16 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:57) finished in 0.039 s
2016-10-26 15:33:16 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:57, took 0.108506 s
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:59)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54007 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 14 ms on localhost (1/2)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 18 ms on localhost (2/2)
2016-10-26 15:33:16 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:59) finished in 0.019 s
2016-10-26 15:33:16 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:59, took 0.034198 s
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:60
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:59)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:60) with 2 output partitions
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:60)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54007 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 16 ms on localhost (1/2)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 20 ms on localhost (2/2)
2016-10-26 15:33:16 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:59) finished in 0.021 s
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54007 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59)
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:33:16 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-26 15:33:16 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-26 15:33:16 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 20 ms on localhost (1/2)
2016-10-26 15:33:16 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 22 ms on localhost (2/2)
2016-10-26 15:33:16 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 15:33:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:60) finished in 0.023 s
2016-10-26 15:33:16 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:60, took 0.070481 s
2016-10-26 15:33:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@5f2cab59{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:33:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:33:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:33:16 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:33:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:33:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54008.
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:33:16 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-892dfccb-24be-4177-9ada-a5edc6994ac8
2016-10-26 15:33:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:33:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:33:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:33:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@75ae4a1f{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:33:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @5307ms
2016-10-26 15:33:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:33:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 15:33:16 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:33:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54009.
2016-10-26 15:33:16 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:54009
2016-10-26 15:33:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 54009)
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:54009 with 912.3 MB RAM, BlockManagerId(driver, localhost, 54009)
2016-10-26 15:33:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 54009)
2016-10-26 15:33:16 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 15:33:16 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/tmp').
2016-10-26 15:33:16 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is '/tmp'.
2016-10-26 15:33:16 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 15:33:16 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@75ae4a1f{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:33:16 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:33:16 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:33:16 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:33:16 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:33:16 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:33:16 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:33:16 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 15:33:16 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-b5acd4e7-7c98-4fe0-b6c4-dcb08e68b8fe
2016-10-26 15:36:41 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:36:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:36:41 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:36:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:36:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:36:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:36:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:36:41 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:36:42 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54035.
2016-10-26 15:36:42 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:36:42 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:36:42 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-43c2ad71-cdbf-470e-b6fb-6da65b219e7d
2016-10-26 15:36:42 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:36:42 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:36:42 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2310ms
2016-10-26 15:36:42 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:36:42 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:36:42 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2472ms
2016-10-26 15:36:42 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:36:42 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 15:36:43 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:36:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54036.
2016-10-26 15:36:43 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54036
2016-10-26 15:36:43 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54036)
2016-10-26 15:36:43 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54036 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54036)
2016-10-26 15:36:43 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54036)
2016-10-26 15:36:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 15:36:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 15:36:43 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54036 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 15:36:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:36
2016-10-26 15:36:44 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:36:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:53)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:56)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53), which has no missing parents
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54036 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:54036 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:54036 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 15:36:44 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 298 ms on localhost (1/2)
2016-10-26 15:36:44 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 253 ms on localhost (2/2)
2016-10-26 15:36:44 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:53) finished in 0.321 s
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54036 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56)
2016-10-26 15:36:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 15:36:44 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 15:36:44 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 15:36:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 92 ms on localhost (1/2)
2016-10-26 15:36:45 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 91 ms on localhost (2/2)
2016-10-26 15:36:45 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:56) finished in 0.094 s
2016-10-26 15:36:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:56, took 0.643789 s
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:57
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:56)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:57) with 2 output partitions
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:57)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54036 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 36 ms on localhost (1/2)
2016-10-26 15:36:45 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 39 ms on localhost (2/2)
2016-10-26 15:36:45 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:56) finished in 0.040 s
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54036 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3809 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 32 ms on localhost (1/2)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 34 ms on localhost (2/2)
2016-10-26 15:36:45 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:57) finished in 0.036 s
2016-10-26 15:36:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:57, took 0.107768 s
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:59)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54036 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2342 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 18 ms on localhost (1/2)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2322 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 24 ms on localhost (2/2)
2016-10-26 15:36:45 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:59) finished in 0.026 s
2016-10-26 15:36:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:59, took 0.045450 s
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:60
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:59)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:60) with 2 output partitions
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:60)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54036 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 17 ms on localhost (1/2)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 29 ms on localhost (2/2)
2016-10-26 15:36:45 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:59) finished in 0.030 s
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54036 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59)
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:36:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-26 15:36:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-26 15:36:45 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 19 ms on localhost (1/2)
2016-10-26 15:36:45 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 20 ms on localhost (2/2)
2016-10-26 15:36:45 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 15:36:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:60) finished in 0.021 s
2016-10-26 15:36:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:60, took 0.074925 s
2016-10-26 15:36:45 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:36:45 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:36:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:36:45 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:36:45 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:36:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54037.
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:36:45 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-ae1bb65d-3573-4746-a77d-238a681127a0
2016-10-26 15:36:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:36:45 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:36:45 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:36:45 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7e55540f{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:36:45 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @5185ms
2016-10-26 15:36:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:36:45 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 15:36:45 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:36:45 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54038.
2016-10-26 15:36:45 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:54038
2016-10-26 15:36:45 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 54038)
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:54038 with 912.3 MB RAM, BlockManagerId(driver, localhost, 54038)
2016-10-26 15:36:45 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 54038)
2016-10-26 15:36:45 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 15:36:45 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'hive-warehouse'.
2016-10-26 15:36:45 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 15:36:45 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7e55540f{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:36:45 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:36:45 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:36:45 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:36:45 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:36:45 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:36:45 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:36:45 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 15:36:45 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-e3d83bb2-7d8d-43e8-896f-0f2c91164742
2016-10-26 15:41:50 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:41:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:41:50 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:41:51 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54094.
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:41:51 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-3f431ab5-9045-4ee9-9fc2-de7f4eab0c8f
2016-10-26 15:41:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:41:51 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:41:51 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2692ms
2016-10-26 15:41:52 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:41:52 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:41:52 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2872ms
2016-10-26 15:41:52 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:41:52 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 15:41:52 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:41:52 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54095.
2016-10-26 15:41:52 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54095
2016-10-26 15:41:52 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54095)
2016-10-26 15:41:52 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54095 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54095)
2016-10-26 15:41:52 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54095)
2016-10-26 15:41:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 15:41:53 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 15:41:53 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54095 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 15:41:53 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:36
2016-10-26 15:41:53 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:41:53 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:53)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:56)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53), which has no missing parents
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 15:41:53 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54095 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53)
2016-10-26 15:41:53 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 15:41:53 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:41:53 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:41:53 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:41:53 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 15:41:53 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 15:41:53 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 15:41:53 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:41:53 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:41:53 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 15:41:53 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 15:41:53 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 15:41:53 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 15:41:53 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:41:53 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 15:41:53 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:54095 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 15:41:53 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:54095 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 302 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 252 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:53) finished in 0.329 s
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54095 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 95 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 90 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:56) finished in 0.098 s
2016-10-26 15:41:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:56, took 0.662723 s
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:57
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:56)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:57) with 2 output partitions
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:57)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54095 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 41 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 42 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:56) finished in 0.043 s
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54095 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 33 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3809 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 34 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:57) finished in 0.037 s
2016-10-26 15:41:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:57, took 0.112910 s
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:59)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54095 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 29 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 28 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:59) finished in 0.032 s
2016-10-26 15:41:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:59, took 0.052077 s
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:60
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:59)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:60) with 2 output partitions
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:60)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54095 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 19 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 19 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:59) finished in 0.022 s
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54095 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59)
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:41:54 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 20 ms on localhost (1/2)
2016-10-26 15:41:54 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 23 ms on localhost (2/2)
2016-10-26 15:41:54 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 15:41:54 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:60) finished in 0.023 s
2016-10-26 15:41:54 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:60, took 0.071499 s
2016-10-26 15:41:54 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:41:54 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:41:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:41:54 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:41:54 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:41:54 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54096.
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:41:54 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-7a836c94-a1c6-4e2a-a233-2e6294763299
2016-10-26 15:41:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:41:54 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:41:54 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:41:54 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@4aaaf1f7{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:41:54 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @5615ms
2016-10-26 15:41:54 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:41:54 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 15:41:54 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:41:54 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54097.
2016-10-26 15:41:54 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:54097
2016-10-26 15:41:54 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 54097)
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:54097 with 912.3 MB RAM, BlockManagerId(driver, localhost, 54097)
2016-10-26 15:41:54 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 54097)
2016-10-26 15:41:54 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 15:41:54 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'hive-warehouse'.
2016-10-26 15:41:54 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 15:41:54 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@4aaaf1f7{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:41:54 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:41:54 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:41:54 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:41:54 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:41:54 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:41:54 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:41:54 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 15:41:54 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-d4f43f7c-2a31-45bb-8430-defe76e0114d
2016-10-26 15:54:57 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:54:57 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:54:57 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:54:57 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:54:57 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:54:57 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:54:57 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:54:57 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:54:58 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54309.
2016-10-26 15:54:58 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:54:58 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:54:58 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-e3cd1fb8-1a8b-485e-a5e8-20375e3308fd
2016-10-26 15:54:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:54:58 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:54:58 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2236ms
2016-10-26 15:54:58 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:54:58 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:54:58 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2395ms
2016-10-26 15:54:58 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:54:58 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 15:54:58 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:54:58 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54310.
2016-10-26 15:54:58 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54310
2016-10-26 15:54:58 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54310)
2016-10-26 15:54:59 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54310 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54310)
2016-10-26 15:54:59 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54310)
2016-10-26 15:54:59 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 15:54:59 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 15:54:59 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54310 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 15:54:59 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:36
2016-10-26 15:55:00 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:55:00 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:53)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:56)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53), which has no missing parents
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54310 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:54310 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:54310 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 15:55:00 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 290 ms on localhost (1/2)
2016-10-26 15:55:00 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 255 ms on localhost (2/2)
2016-10-26 15:55:00 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:53) finished in 0.319 s
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54310 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56)
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 15:55:00 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 15:55:00 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 15:55:00 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 15:55:00 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 83 ms on localhost (1/2)
2016-10-26 15:55:00 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 87 ms on localhost (2/2)
2016-10-26 15:55:00 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:55:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:56) finished in 0.088 s
2016-10-26 15:55:00 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:56, took 0.627771 s
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:57
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:56)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:57) with 2 output partitions
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:57)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54310 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 40 ms on localhost (1/2)
2016-10-26 15:55:01 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 43 ms on localhost (2/2)
2016-10-26 15:55:01 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:56) finished in 0.045 s
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54310 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3896 bytes result sent to driver
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4273 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 36 ms on localhost (1/2)
2016-10-26 15:55:01 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 39 ms on localhost (2/2)
2016-10-26 15:55:01 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:57) finished in 0.040 s
2016-10-26 15:55:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:57, took 0.117790 s
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:59)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54310 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 18 ms on localhost (1/2)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2342 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 20 ms on localhost (2/2)
2016-10-26 15:55:01 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:59) finished in 0.022 s
2016-10-26 15:55:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:59, took 0.036305 s
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:60
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:59)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:60) with 2 output partitions
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:60)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54310 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 23 ms on localhost (1/2)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 37 ms on localhost (2/2)
2016-10-26 15:55:01 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:59) finished in 0.038 s
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54310 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59)
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1640 bytes result sent to driver
2016-10-26 15:55:01 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-26 15:55:01 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 20 ms on localhost (1/2)
2016-10-26 15:55:01 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 18 ms on localhost (2/2)
2016-10-26 15:55:01 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 15:55:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:60) finished in 0.021 s
2016-10-26 15:55:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:60, took 0.085677 s
2016-10-26 15:55:01 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:55:01 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:55:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:55:01 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:55:01 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:55:01 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54311.
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:55:01 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-b0a1348d-e5c5-43aa-96e8-de972dc8400d
2016-10-26 15:55:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:55:01 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:55:01 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:55:01 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@21bd20ee{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:55:01 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @5114ms
2016-10-26 15:55:01 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:55:01 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 15:55:01 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:55:01 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54312.
2016-10-26 15:55:01 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:54312
2016-10-26 15:55:01 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 54312)
2016-10-26 15:55:01 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:54312 with 912.3 MB RAM, BlockManagerId(driver, localhost, 54312)
2016-10-26 15:55:01 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 54312)
2016-10-26 15:55:01 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 15:55:01 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'hive-warehouse'.
2016-10-26 15:55:02 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 15:55:02 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 15:55:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 15:55:03 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 15:55:05 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 15:55:07 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 15:55:07 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 15:55:07 WARN  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:6666] : Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2016-10-26 15:55:07 WARN  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:568] : Failed to get database default, returning NoSuchObjectException
2016-10-26 15:55:07 WARN  [main] hive.ql.metadata.Hive [Hive.java:168] : Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:171)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:258)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:359)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:263)
	at org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)
	at org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)
	at org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:46)
	at org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:492)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:472)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:78)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	... 27 common frames omitted
Caused by: java.lang.reflect.InvocationTargetException: null
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 33 common frames omitted
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./hive-warehouse
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:197)
	at org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:141)
	at org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:146)
	at org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:159)
	at org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:177)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:600)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 38 common frames omitted
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:./hive-warehouse
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:203)
	... 51 common frames omitted
2016-10-26 15:55:07 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 15:55:07 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 15:55:07 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 15:55:07 WARN  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:568] : Failed to get database default, returning NoSuchObjectException
2016-10-26 15:55:07 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 15:55:07 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@21bd20ee{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:55:07 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 15:55:07 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:55:07 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:55:07 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:55:07 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:55:07 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:55:07 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:55:07 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 15:55:07 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-93d8135e-ad6f-4eef-ae08-bacf4a7e6788
2016-10-26 15:55:52 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:29] : starting spark-streaming-kafka
2016-10-26 15:55:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:55:53 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:55:53 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54323.
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:55:53 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-d3e8d41d-6e46-4c9a-8008-a2138da8f3da
2016-10-26 15:55:53 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:55:53 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:55:53 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1865ms
2016-10-26 15:55:53 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:55:54 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:55:54 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @1996ms
2016-10-26 15:55:54 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:55:54 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 15:55:54 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:55:54 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54324.
2016-10-26 15:55:54 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54324
2016-10-26 15:55:54 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54324)
2016-10-26 15:55:54 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54324 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54324)
2016-10-26 15:55:54 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54324)
2016-10-26 15:55:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 15:55:54 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 15:55:54 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54324 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 15:55:54 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:36
2016-10-26 15:55:55 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:55:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:56
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:53)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:56) with 2 output partitions
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:56)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54324 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:53)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:54324 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:54324 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2175 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 196 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 236 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:53) finished in 0.253 s
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54324 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:56)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 6 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2307 bytes result sent to driver
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2411 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 71 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 75 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:56) finished in 0.076 s
2016-10-26 15:55:55 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:56, took 0.518895 s
2016-10-26 15:55:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:57
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:56)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:57) with 2 output partitions
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:57)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54324 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:56)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 41 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 43 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:56) finished in 0.046 s
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54324 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:56)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3896 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 36 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4186 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 42 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:57) finished in 0.044 s
2016-10-26 15:55:55 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:57, took 0.118171 s
2016-10-26 15:55:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:59
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:59) with 2 output partitions
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:59)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54324 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:59)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 20 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2322 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 27 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:59) finished in 0.029 s
2016-10-26 15:55:55 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:59, took 0.043867 s
2016-10-26 15:55:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:60
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:59)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:60) with 2 output partitions
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:60)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54324 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:59)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1969 bytes result sent to driver
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 20 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 22 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:59) finished in 0.023 s
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59), which has no missing parents
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54324 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:59)
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 15:55:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 17 ms on localhost (1/2)
2016-10-26 15:55:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1640 bytes result sent to driver
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 21 ms on localhost (2/2)
2016-10-26 15:55:55 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 15:55:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:60) finished in 0.022 s
2016-10-26 15:55:55 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:60, took 0.069697 s
2016-10-26 15:55:55 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7c2b6087{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:55:55 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:55:55 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:55:55 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:55:55 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:55:55 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:55:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 15:55:56 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54325.
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 15:55:56 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-5148b4b1-f35a-42f9-8422-82b4ae3ac336
2016-10-26 15:55:56 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 15:55:56 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 15:55:56 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 15:55:56 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7e047377{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:55:56 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @4096ms
2016-10-26 15:55:56 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 15:55:56 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 15:55:56 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 15:55:56 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54326.
2016-10-26 15:55:56 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:54326
2016-10-26 15:55:56 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 54326)
2016-10-26 15:55:56 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:54326 with 912.3 MB RAM, BlockManagerId(driver, localhost, 54326)
2016-10-26 15:55:56 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 54326)
2016-10-26 15:55:56 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 15:55:56 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 15:55:56 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 15:55:57 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 15:55:57 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 15:55:57 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 15:55:59 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 15:56:00 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 15:56:00 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 15:56:01 WARN  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:568] : Failed to get database default, returning NoSuchObjectException
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 15:56:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 15:56:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/1f4b2976-58ce-4375-bb0b-449024d0f675_resources
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/1f4b2976-58ce-4375-bb0b-449024d0f675
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/1f4b2976-58ce-4375-bb0b-449024d0f675
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/1f4b2976-58ce-4375-bb0b-449024d0f675/_tmp_space.db
2016-10-26 15:56:01 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/11ef9327-9356-49f5-a5b9-7066ecc33b8a_resources
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/11ef9327-9356-49f5-a5b9-7066ecc33b8a
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/11ef9327-9356-49f5-a5b9-7066ecc33b8a
2016-10-26 15:56:01 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/11ef9327-9356-49f5-a5b9-7066ecc33b8a/_tmp_space.db
2016-10-26 15:56:01 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 15:56:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 15:56:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 15:56:02 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:492)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:472)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:78)

2016-10-26 15:56:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 15:56:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 15:56:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-26 15:56:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 15:56:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 15:56:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 15:56:02 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:54326 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 15:56:02 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:82
2016-10-26 15:56:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 15:56:03 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 229.126747 ms
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:82
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:82) with 1 output partitions
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:82)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:82), which has no missing parents
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.1 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:54326 (size: 3.6 KB, free: 912.3 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:82)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.325487 ms
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-26 15:56:03 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 73 ms on localhost (1/1)
2016-10-26 15:56:03 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:82) finished in 0.074 s
2016-10-26 15:56:03 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:82, took 0.105680 s
2016-10-26 15:56:03 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 21.688243 ms
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.8 KB, free 912.0 MB)
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:54326 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:85
2016-10-26 15:56:03 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:85
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:85) with 1 output partitions
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:85)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:85), which has no missing parents
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:54326 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:85)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-26 15:56:03 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 20 ms on localhost (1/1)
2016-10-26 15:56:03 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:85) finished in 0.020 s
2016-10-26 15:56:03 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:85, took 0.029163 s
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:54326 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:88
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:88
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:88) with 1 output partitions
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:88)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:88), which has no missing parents
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:54326 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:88)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-26 15:56:03 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 26 ms on localhost (1/1)
2016-10-26 15:56:03 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:88) finished in 0.028 s
2016-10-26 15:56:03 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:88, took 0.039032 s
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.8 KB, free 911.7 MB)
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:54326 (size: 14.9 KB, free: 912.2 MB)
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:91
2016-10-26 15:56:03 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:91
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:91) with 1 output partitions
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:91)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:91), which has no missing parents
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:54326 (size: 2.6 KB, free: 912.2 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:91)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+243
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1650 bytes result sent to driver
2016-10-26 15:56:03 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 12 ms on localhost (1/1)
2016-10-26 15:56:03 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:91) finished in 0.013 s
2016-10-26 15:56:03 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:91, took 0.019720 s
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 133.4 KB, free 911.6 MB)
2016-10-26 15:56:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.5 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:54326 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:94
2016-10-26 15:56:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 15:56:03 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:94
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:94) with 1 output partitions
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:94)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:94), which has no missing parents
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:54326 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:94)
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.180537 ms
2016-10-26 15:56:03 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-26 15:56:03 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 24 ms on localhost (1/1)
2016-10-26 15:56:03 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 15:56:03 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:94) finished in 0.026 s
2016-10-26 15:56:03 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:94, took 0.032542 s
2016-10-26 15:56:03 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.420728 ms
2016-10-26 15:56:03 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 15:56:03 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7e047377{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 15:56:03 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 15:56:03 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 15:56:04 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 15:56:04 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 15:56:04 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 15:56:04 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 15:56:04 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 15:56:04 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 15:56:04 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-1f8b541f-9379-40f8-8c10-2e112d16697b
2016-10-26 16:19:05 INFO  [main] hx.stream.spark.SparkApplication [SparkApplication.java:30] : starting spark-streaming-kafka
2016-10-26 16:19:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:19:06 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:19:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:19:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:19:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:19:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:19:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:19:07 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54697.
2016-10-26 16:19:07 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:19:07 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:19:07 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-46debfde-bb78-4c41-8d51-6c0568430466
2016-10-26 16:19:07 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:19:07 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:19:07 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2581ms
2016-10-26 16:19:08 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:19:08 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@44bddc96{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:19:08 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2744ms
2016-10-26 16:19:08 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:19:08 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:19:08 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:19:08 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54698.
2016-10-26 16:19:08 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54698
2016-10-26 16:19:08 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54698)
2016-10-26 16:19:08 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54698 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54698)
2016-10-26 16:19:08 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54698)
2016-10-26 16:19:09 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 912.2 MB)
2016-10-26 16:19:09 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.2 KB, free 912.2 MB)
2016-10-26 16:19:09 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54698 (size: 10.2 KB, free: 912.3 MB)
2016-10-26 16:19:09 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from textFile at SparkApplication.java:37
2016-10-26 16:19:09 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:19:09 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:57
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 3 (mapToPair at SparkApplication.java:54)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (sortByKey at SparkApplication.java:57) with 2 output partitions
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (sortByKey at SparkApplication.java:57)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 0)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 0)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:54), which has no missing parents
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.2 KB, free 912.2 MB)
2016-10-26 16:19:09 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54698 (size: 3.2 KB, free: 912.3 MB)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at SparkApplication.java:54)
2016-10-26 16:19:09 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 16:19:09 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5380 bytes)
2016-10-26 16:19:09 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5380 bytes)
2016-10-26 16:19:09 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:19:09 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 16:19:09 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:0+1679
2016-10-26 16:19:09 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/usr/local/Cellar/apache-spark/1.6.1/README.md:1679+1680
2016-10-26 16:19:09 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:19:09 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:19:09 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:19:09 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:19:09 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:19:09 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:19:09 INFO  [Executor task launch worker-0] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_0 stored as values in memory (estimated size 5.7 KB, free 912.2 MB)
2016-10-26 16:19:09 INFO  [Executor task launch worker-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block rdd_1_1 stored as values in memory (estimated size 4.9 KB, free 912.2 MB)
2016-10-26 16:19:09 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_0 in memory on 172.16.106.190:54698 (size: 5.7 KB, free: 912.3 MB)
2016-10-26 16:19:09 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added rdd_1_1 in memory on 172.16.106.190:54698 (size: 4.9 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 2262 bytes result sent to driver
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 2262 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 273 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 229 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 0 (mapToPair at SparkApplication.java:54) finished in 0.314 s
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 1)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 912.2 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.2 MB)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54698 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at sortByKey at SparkApplication.java:57)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, ANY, 5142 bytes)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 3, localhost, partition 1, ANY, 5142 bytes)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 3)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3). 2394 bytes result sent to driver
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 2324 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 3) in 77 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 82 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (sortByKey at SparkApplication.java:57) finished in 0.084 s
2016-10-26 16:19:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: sortByKey at SparkApplication.java:57, took 0.596600 s
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collect at SparkApplication.java:58
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 4 (reduceByKey at SparkApplication.java:57)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (collect at SparkApplication.java:58) with 2 output partitions
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (collect at SparkApplication.java:58)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 912.2 MB)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54698 (size: 2.5 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 3 (ShuffledRDD[4] at reduceByKey at SparkApplication.java:57)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1882 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 38 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1882 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 44 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (reduceByKey at SparkApplication.java:57) finished in 0.045 s
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:57), which has no missing parents
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 3.5 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.0 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54698 (size: 2.0 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[7] at sortByKey at SparkApplication.java:57)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 2 tasks
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 4.0 (TID 7, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 4.0 (TID 7)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 4186 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 36 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7). 3896 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 4.0 (TID 7) in 39 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (collect at SparkApplication.java:58) finished in 0.041 s
2016-10-26 16:19:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: collect at SparkApplication.java:58, took 0.118917 s
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: sortByKey at SparkApplication.java:60
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 0 is 159 bytes
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.MapOutputTrackerMaster [Logging.scala:54] : Size of output statuses for shuffle 1 is 159 bytes
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (sortByKey at SparkApplication.java:60) with 2 output partitions
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (sortByKey at SparkApplication.java:60)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 6)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:60), which has no missing parents
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 5.0 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54698 (size: 2.8 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at sortByKey at SparkApplication.java:60)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 2 tasks
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, ANY, 5143 bytes)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 7.0 (TID 9, localhost, partition 1, ANY, 5143 bytes)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 7.0 (TID 9)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9). 2255 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 7.0 (TID 9) in 16 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8). 2235 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 8) in 21 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (sortByKey at SparkApplication.java:60) finished in 0.022 s
2016-10-26 16:19:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: sortByKey at SparkApplication.java:60, took 0.040345 s
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: foreach at SparkApplication.java:61
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 8 (mapToPair at SparkApplication.java:60)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (foreach at SparkApplication.java:61) with 2 output partitions
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 11 (foreach at SparkApplication.java:61)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 10)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 10)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:60), which has no missing parents
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 4.5 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54698 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[8] at mapToPair at SparkApplication.java:60)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 10.0 with 2 tasks
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 10.0 (TID 10, localhost, partition 0, ANY, 5130 bytes)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 10.0 (TID 11, localhost, partition 1, ANY, 5130 bytes)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 10.0 (TID 11)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 10.0 (TID 10)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 1 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11). 1882 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 10.0 (TID 11) in 18 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10). 1882 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 10.0 (TID 10) in 21 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 10.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 10 (mapToPair at SparkApplication.java:60) finished in 0.022 s
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 11)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:60), which has no missing parents
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 912.1 MB)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54698 (size: 2.4 KB, free: 912.3 MB)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[11] at sortByKey at SparkApplication.java:60)
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 11.0 with 2 tasks
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 11.0 (TID 12, localhost, partition 0, ANY, 5141 bytes)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 11.0 (TID 13, localhost, partition 1, ANY, 5141 bytes)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 11.0 (TID 13)
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 11.0 (TID 12)
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 2 non-empty blocks out of 2 blocks
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 0 ms
2016-10-26 16:19:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12). 1553 bytes result sent to driver
2016-10-26 16:19:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13). 1553 bytes result sent to driver
2016-10-26 16:19:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 11.0 (TID 12) in 20 ms on localhost (1/2)
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 11.0 (TID 13) in 19 ms on localhost (2/2)
2016-10-26 16:19:10 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 11.0, whose tasks have all completed, from pool 
2016-10-26 16:19:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 11 (foreach at SparkApplication.java:61) finished in 0.021 s
2016-10-26 16:19:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: foreach at SparkApplication.java:61, took 0.070665 s
2016-10-26 16:19:10 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@44bddc96{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:19:10 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:19:10 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:19:10 INFO  [main] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:19:10 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:19:10 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54699.
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:19:10 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-524e0994-7d24-454a-9351-081abfb1bba6
2016-10-26 16:19:10 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:19:10 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:19:10 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:19:10 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@50932268{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:19:10 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @5451ms
2016-10-26 16:19:10 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:19:10 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://localhost:4040
2016-10-26 16:19:10 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:19:10 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54700.
2016-10-26 16:19:10 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on localhost:54700
2016-10-26 16:19:10 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, localhost, 54700)
2016-10-26 16:19:10 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager localhost:54700 with 912.3 MB RAM, BlockManagerId(driver, localhost, 54700)
2016-10-26 16:19:10 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, localhost, 54700)
2016-10-26 16:19:10 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:19:10 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:19:11 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:19:12 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:19:12 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:19:12 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:19:14 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 16:19:15 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 16:19:15 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 16:19:16 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 16:19:16 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/20eb5930-e797-44b8-9f1c-9f63f0cdb031_resources
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/20eb5930-e797-44b8-9f1c-9f63f0cdb031
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/20eb5930-e797-44b8-9f1c-9f63f0cdb031
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/20eb5930-e797-44b8-9f1c-9f63f0cdb031/_tmp_space.db
2016-10-26 16:19:16 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/c196e61c-234a-49e0-a1c7-082b45929f12_resources
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/c196e61c-234a-49e0-a1c7-082b45929f12
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/c196e61c-234a-49e0-a1c7-082b45929f12
2016-10-26 16:19:16 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/c196e61c-234a-49e0-a1c7-082b45929f12/_tmp_space.db
2016-10-26 16:19:16 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:19:17 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 16:19:17 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 16:19:17 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:492)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:472)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:79)

2016-10-26 16:19:17 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:19:17 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:19:17 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-26 16:19:17 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:19:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:19:17 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:19:17 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on localhost:54700 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:19:17 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:83
2016-10-26 16:19:17 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4197663 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:19:18 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 219.156863 ms
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:83
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:83) with 1 output partitions
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:83)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:83), which has no missing parents
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.1 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on localhost:54700 (size: 3.6 KB, free: 912.3 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:83)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.893366 ms
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-26 16:19:18 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 83 ms on localhost (1/1)
2016-10-26 16:19:18 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:83) finished in 0.087 s
2016-10-26 16:19:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:83, took 0.114385 s
2016-10-26 16:19:18 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.254593 ms
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.8 KB, free 912.0 MB)
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on localhost:54700 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:86
2016-10-26 16:19:18 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:86
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:86) with 1 output partitions
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:86)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:86), which has no missing parents
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on localhost:54700 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:86)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5451 bytes)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+1036
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-26 16:19:18 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 23 ms on localhost (1/1)
2016-10-26 16:19:18 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:86) finished in 0.024 s
2016-10-26 16:19:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:86, took 0.036625 s
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on localhost:54700 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:89
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4195340 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:89
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:89) with 1 output partitions
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:89)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:89), which has no missing parents
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on localhost:54700 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:89)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 2)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2). 1670 bytes result sent to driver
2016-10-26 16:19:18 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 2) in 28 ms on localhost (1/1)
2016-10-26 16:19:18 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:89) finished in 0.029 s
2016-10-26 16:19:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:89, took 0.040356 s
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.8 KB, free 911.7 MB)
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on localhost:54700 (size: 14.9 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:92
2016-10-26 16:19:18 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:92
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:92) with 1 output partitions
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:92)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:92), which has no missing parents
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on localhost:54700 (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:92)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 3)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+243
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3). 1650 bytes result sent to driver
2016-10-26 16:19:18 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 3) in 11 ms on localhost (1/1)
2016-10-26 16:19:18 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:92) finished in 0.012 s
2016-10-26 16:19:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:92, took 0.020322 s
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 133.4 KB, free 911.6 MB)
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.5 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on localhost:54700 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:95
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:95
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:95) with 1 output partitions
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:95)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:95), which has no missing parents
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on localhost:54700 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:95)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 4)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.8084 ms
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4). 1410 bytes result sent to driver
2016-10-26 16:19:18 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 4) in 26 ms on localhost (1/1)
2016-10-26 16:19:18 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:95) finished in 0.027 s
2016-10-26 16:19:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:95, took 0.035608 s
2016-10-26 16:19:18 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.749538 ms
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 133.4 KB, free 911.4 MB)
2016-10-26 16:19:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.4 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on localhost:54700 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from show at SparkApplication.java:96
2016-10-26 16:19:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:19:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:96
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (show at SparkApplication.java:96) with 1 output partitions
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:96)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:96), which has no missing parents
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 7.1 KB, free 911.4 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.4 MB)
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on localhost:54700 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:96)
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 16:19:18 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 5, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 5)
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:19:18 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 5). 1336 bytes result sent to driver
2016-10-26 16:19:18 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 5) in 7 ms on localhost (1/1)
2016-10-26 16:19:18 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 16:19:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:96) finished in 0.009 s
2016-10-26 16:19:18 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: show at SparkApplication.java:96, took 0.015847 s
2016-10-26 16:19:19 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:19:19 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 16:19:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:19:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:19:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:19:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:19:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 133.4 KB, free 911.2 MB)
2016-10-26 16:19:19 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.2 MB)
2016-10-26 16:19:19 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on localhost:54700 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:19:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from show at SparkApplication.java:100
2016-10-26 16:19:19 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194547 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:19:19 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:100
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (show at SparkApplication.java:100) with 1 output partitions
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (show at SparkApplication.java:100)
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:100), which has no missing parents
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 7.2 KB, free 911.2 MB)
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.2 MB)
2016-10-26 16:19:19 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on localhost:54700 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:100)
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 16:19:19 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:19:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 6)
2016-10-26 16:19:19 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:19:19 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 16:19:19 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 6) in 8 ms on localhost (1/1)
2016-10-26 16:19:19 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 16:19:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (show at SparkApplication.java:100) finished in 0.010 s
2016-10-26 16:19:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 finished: show at SparkApplication.java:100, took 0.017338 s
2016-10-26 16:19:19 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:19:19 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:19:19 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@50932268{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:19:19 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://localhost:4040
2016-10-26 16:19:19 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:19:19 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:19:19 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:19:19 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:19:19 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:19:19 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:19:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:19:19 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-27352bf8-cda4-44ec-8069-07cc867ef75a
2016-10-26 16:25:27 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:25:27 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:25:27 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:25:27 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:25:27 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:25:27 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:25:27 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:25:27 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54737.
2016-10-26 16:25:28 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:25:28 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:25:28 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-13e8d925-1e7f-4aa3-a706-f13452256e5f
2016-10-26 16:25:28 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:25:28 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:25:28 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2387ms
2016-10-26 16:25:28 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:25:28 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:25:28 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2551ms
2016-10-26 16:25:28 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:25:28 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:25:28 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:25:28 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54738.
2016-10-26 16:25:28 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54738
2016-10-26 16:25:28 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54738)
2016-10-26 16:25:28 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54738 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54738)
2016-10-26 16:25:28 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54738)
2016-10-26 16:25:28 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:25:29 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:25:30 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:25:30 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:25:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:25:30 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:25:32 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 16:25:33 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 16:25:33 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 16:25:34 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 16:25:34 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/cede883a-3beb-4aec-9247-b1b22738a1ed_resources
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/cede883a-3beb-4aec-9247-b1b22738a1ed
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/cede883a-3beb-4aec-9247-b1b22738a1ed
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/cede883a-3beb-4aec-9247-b1b22738a1ed/_tmp_space.db
2016-10-26 16:25:34 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/425b4702-6e35-43f4-b034-e95de98709de_resources
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/425b4702-6e35-43f4-b034-e95de98709de
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/425b4702-6e35-43f4-b034-e95de98709de
2016-10-26 16:25:34 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/425b4702-6e35-43f4-b034-e95de98709de/_tmp_space.db
2016-10-26 16:25:34 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:25:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 16:25:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 16:25:35 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy16.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:492)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:472)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:30)

2016-10-26 16:25:35 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:25:35 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:25:35 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-26 16:25:35 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:25:35 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:25:35 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:25:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54738 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:25:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:34
2016-10-26 16:25:35 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:25:36 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 350.498412 ms
2016-10-26 16:25:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:34
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:34) with 1 output partitions
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:34)
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:34), which has no missing parents
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.1 MB)
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-26 16:25:36 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54738 (size: 3.6 KB, free: 912.3 MB)
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:34)
2016-10-26 16:25:36 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-26 16:25:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-26 16:25:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 19.102027 ms
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-26 16:25:37 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 177 ms on localhost (1/1)
2016-10-26 16:25:37 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:34) finished in 0.197 s
2016-10-26 16:25:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:34, took 0.360338 s
2016-10-26 16:25:37 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.296005 ms
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.8 KB, free 912.0 MB)
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54738 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:37
2016-10-26 16:25:37 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:37
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:37) with 2 output partitions
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:37)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:37), which has no missing parents
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54738 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:37)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5450 bytes)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1, PROCESS_LOCAL, 5450 bytes)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 2)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+518
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:518+518
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 2). 1616 bytes result sent to driver
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-26 16:25:37 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 2) in 30 ms on localhost (1/2)
2016-10-26 16:25:37 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 44 ms on localhost (2/2)
2016-10-26 16:25:37 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:37) finished in 0.045 s
2016-10-26 16:25:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:37, took 0.058371 s
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54738 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:40
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:40
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:40) with 1 output partitions
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:40)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:40), which has no missing parents
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54738 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:40)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1670 bytes result sent to driver
2016-10-26 16:25:37 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 23 ms on localhost (1/1)
2016-10-26 16:25:37 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:40) finished in 0.024 s
2016-10-26 16:25:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:40, took 0.035443 s
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.8 KB, free 911.7 MB)
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54738 (size: 14.9 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:43
2016-10-26 16:25:37 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:43
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:43) with 2 output partitions
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:43)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:43), which has no missing parents
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54738 (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:43)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 16:25:37 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1650 bytes result sent to driver
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1650 bytes result sent to driver
2016-10-26 16:25:37 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 13 ms on localhost (1/2)
2016-10-26 16:25:37 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 16 ms on localhost (2/2)
2016-10-26 16:25:37 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:43) finished in 0.017 s
2016-10-26 16:25:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:43, took 0.030700 s
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 133.4 KB, free 911.6 MB)
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.5 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:54738 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:46
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:46
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:46) with 1 output partitions
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:46)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:46), which has no missing parents
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:54738 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:46)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.898284 ms
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 16:25:37 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 23 ms on localhost (1/1)
2016-10-26 16:25:37 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:46) finished in 0.024 s
2016-10-26 16:25:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:46, took 0.034562 s
2016-10-26 16:25:37 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.890187 ms
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 133.4 KB, free 911.4 MB)
2016-10-26 16:25:37 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.4 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:54738 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from show at SparkApplication.java:47
2016-10-26 16:25:37 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:25:37 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:47
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (show at SparkApplication.java:47) with 1 output partitions
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:47)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:47), which has no missing parents
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 7.1 KB, free 911.4 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.4 MB)
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:54738 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:47)
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 16:25:37 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 7)
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:25:37 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 7). 1336 bytes result sent to driver
2016-10-26 16:25:37 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 7) in 10 ms on localhost (1/1)
2016-10-26 16:25:37 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 16:25:37 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:47) finished in 0.010 s
2016-10-26 16:25:37 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: show at SparkApplication.java:47, took 0.019250 s
2016-10-26 16:25:37 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:25:38 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 16:25:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:25:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:25:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:25:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:25:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 133.4 KB, free 911.2 MB)
2016-10-26 16:25:38 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.2 MB)
2016-10-26 16:25:38 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:54738 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:25:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from show at SparkApplication.java:51
2016-10-26 16:25:38 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:25:38 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:51
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (show at SparkApplication.java:51) with 1 output partitions
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (show at SparkApplication.java:51)
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:51), which has no missing parents
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 7.2 KB, free 911.2 MB)
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.2 MB)
2016-10-26 16:25:38 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:54738 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:51)
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 16:25:38 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:25:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 8)
2016-10-26 16:25:38 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:25:38 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 8). 1410 bytes result sent to driver
2016-10-26 16:25:38 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 8) in 12 ms on localhost (1/1)
2016-10-26 16:25:38 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 16:25:38 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (show at SparkApplication.java:51) finished in 0.013 s
2016-10-26 16:25:38 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 finished: show at SparkApplication.java:51, took 0.021930 s
2016-10-26 16:25:38 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:25:38 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:25:38 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:25:38 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:25:38 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:25:38 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:25:38 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:25:38 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:25:38 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:25:38 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:25:38 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:25:38 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-f136d923-07ef-4c41-aac7-295472498b4b
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:27:43 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:27:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54757.
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:27:43 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-b659acc4-c8d2-46fd-88ad-516b6e270150
2016-10-26 16:27:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:27:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:27:43 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1810ms
2016-10-26 16:27:43 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:27:43 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:27:43 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @1920ms
2016-10-26 16:27:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:27:43 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:27:44 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:27:44 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54758.
2016-10-26 16:27:44 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54758
2016-10-26 16:27:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54758)
2016-10-26 16:27:44 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54758 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54758)
2016-10-26 16:27:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54758)
2016-10-26 16:27:44 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:27:44 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:27:45 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:27:45 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:27:45 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:27:45 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:27:47 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 16:27:48 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 16:27:48 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 16:27:48 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/b82d2167-2701-4f42-95aa-ad3e3a645653_resources
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/b82d2167-2701-4f42-95aa-ad3e3a645653
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/b82d2167-2701-4f42-95aa-ad3e3a645653
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/b82d2167-2701-4f42-95aa-ad3e3a645653/_tmp_space.db
2016-10-26 16:27:48 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/570f44e8-1e69-4156-bd0d-3d567141aecf_resources
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/570f44e8-1e69-4156-bd0d-3d567141aecf
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/570f44e8-1e69-4156-bd0d-3d567141aecf
2016-10-26 16:27:48 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/570f44e8-1e69-4156-bd0d-3d567141aecf/_tmp_space.db
2016-10-26 16:27:48 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:27:49 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 16:27:49 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 16:27:49 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy16.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:492)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:472)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:30)

2016-10-26 16:27:49 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:49 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:49 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-26 16:27:49 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:49 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:27:49 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:27:49 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:27:49 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:34
2016-10-26 16:27:49 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:50 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 213.778621 ms
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:34
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:34) with 1 output partitions
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:34)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:34), which has no missing parents
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.1 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54758 (size: 3.6 KB, free: 912.3 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:34)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.87385 ms
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-26 16:27:50 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 125 ms on localhost (1/1)
2016-10-26 16:27:50 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:34) finished in 0.138 s
2016-10-26 16:27:50 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:34, took 0.235206 s
2016-10-26 16:27:50 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.53001 ms
2016-10-26 16:27:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.8 KB, free 912.0 MB)
2016-10-26 16:27:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54758 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:37
2016-10-26 16:27:50 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:37
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:37) with 2 output partitions
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:37)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:37), which has no missing parents
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54758 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:37)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5450 bytes)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1, PROCESS_LOCAL, 5450 bytes)
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 2)
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:518+518
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+518
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 2). 1616 bytes result sent to driver
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-26 16:27:50 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 2) in 30 ms on localhost (1/2)
2016-10-26 16:27:50 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 40 ms on localhost (2/2)
2016-10-26 16:27:50 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:37) finished in 0.040 s
2016-10-26 16:27:50 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:37, took 0.051921 s
2016-10-26 16:27:50 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:50 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:50 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-26 16:27:50 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 16:27:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:40
2016-10-26 16:27:50 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:40
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:40) with 1 output partitions
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:40)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:40), which has no missing parents
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54758 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:40)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1670 bytes result sent to driver
2016-10-26 16:27:50 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 16 ms on localhost (1/1)
2016-10-26 16:27:50 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:40) finished in 0.017 s
2016-10-26 16:27:50 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:40, took 0.029503 s
2016-10-26 16:27:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.8 KB, free 911.7 MB)
2016-10-26 16:27:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54758 (size: 14.9 KB, free: 912.2 MB)
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:43
2016-10-26 16:27:50 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:27:50 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:43
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:43) with 2 output partitions
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:43)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:43), which has no missing parents
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54758 (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:43)
2016-10-26 16:27:50 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:27:50 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 16:27:50 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 16:27:50 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 16:27:51 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1650 bytes result sent to driver
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1650 bytes result sent to driver
2016-10-26 16:27:51 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 12 ms on localhost (1/2)
2016-10-26 16:27:51 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 12 ms on localhost (2/2)
2016-10-26 16:27:51 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:43) finished in 0.014 s
2016-10-26 16:27:51 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:43, took 0.022621 s
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 133.4 KB, free 911.6 MB)
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.5 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:46
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:46
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:46) with 1 output partitions
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:46)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:46), which has no missing parents
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:54758 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:46)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.717014 ms
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 16:27:51 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 29 ms on localhost (1/1)
2016-10-26 16:27:51 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:46) finished in 0.029 s
2016-10-26 16:27:51 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:46, took 0.040429 s
2016-10-26 16:27:51 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.675997 ms
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 133.4 KB, free 911.4 MB)
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.4 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from show at SparkApplication.java:47
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:47
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (show at SparkApplication.java:47) with 1 output partitions
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:47)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:47), which has no missing parents
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 7.1 KB, free 911.4 MB)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.4 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:54758 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:47)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 7)
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 7). 1336 bytes result sent to driver
2016-10-26 16:27:51 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 7) in 11 ms on localhost (1/1)
2016-10-26 16:27:51 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:47) finished in 0.013 s
2016-10-26 16:27:51 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: show at SparkApplication.java:47, took 0.022506 s
2016-10-26 16:27:51 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:27:51 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 133.4 KB, free 911.2 MB)
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from show at SparkApplication.java:51
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:51
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (show at SparkApplication.java:51) with 1 output partitions
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (show at SparkApplication.java:51)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:51), which has no missing parents
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 7.2 KB, free 911.2 MB)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:54758 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:51)
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 8)
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:27:51 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 8). 1410 bytes result sent to driver
2016-10-26 16:27:51 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 8) in 13 ms on localhost (1/1)
2016-10-26 16:27:51 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 16:27:51 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (show at SparkApplication.java:51) finished in 0.013 s
2016-10-26 16:27:51 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 finished: show at SparkApplication.java:51, took 0.025662 s
2016-10-26 16:27:51 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 16:27:51 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:27:51 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:27:51 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:27:51 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:27:51 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 16:27:51 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:27:51 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:27:51 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:27:51 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:27:51 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:54758 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:27:51 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_13_piece0 on 172.16.106.190:54758 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:54758 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:27:51 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 0
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 1
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:54758 in memory (size: 3.6 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:54758 in memory (size: 14.9 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:54758 in memory (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:54758 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:54758 in memory (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:54758 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:54758 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:54758 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 224
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 225
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:54758 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 270
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 271
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_11_piece0 on 172.16.106.190:54758 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_12_piece0 on 172.16.106.190:54758 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 316
2016-10-26 16:27:51 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 317
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:27:51 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:27:51 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:27:51 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from saveAsTable at SparkApplication.java:56
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:51 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:27:52 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:27:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:56
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 7 (saveAsTable at SparkApplication.java:56) with 1 output partitions
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (saveAsTable at SparkApplication.java:56)
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[24] at saveAsTable at SparkApplication.java:56), which has no missing parents
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_15 stored as values in memory (estimated size 54.2 KB, free 912.1 MB)
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.4 KB, free 912.1 MB)
2016-10-26 16:27:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_15_piece0 in memory on 172.16.106.190:54758 (size: 20.4 KB, free: 912.3 MB)
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 15 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[24] at saveAsTable at SparkApplication.java:56)
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 16:27:52 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 9)
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261627_0007_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261627_0007_m_000000
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261627_0007_m_000000_0: Committed
2016-10-26 16:27:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 9). 1309 bytes result sent to driver
2016-10-26 16:27:52 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 7.0 (TID 9) in 713 ms on localhost (1/1)
2016-10-26 16:27:52 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 16:27:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (saveAsTable at SparkApplication.java:56) finished in 0.714 s
2016-10-26 16:27:52 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 7 finished: saveAsTable at SparkApplication.java:56, took 0.733095 s
2016-10-26 16:27:52 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261627_0000 committed.
2016-10-26 16:27:52 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 16:27:52 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:27:52 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:27:52 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:27:52 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:27:53 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477470472, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 16:27:53 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477470472, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 16:27:53 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 16:27:53 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 16:27:53 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 16:27:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:27:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:27:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:27:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:27:53 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_16 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 16:27:53 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.9 MB)
2016-10-26 16:27:53 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_16_piece0 in memory on 172.16.106.190:54758 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:27:53 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 16 from collectAsList at SparkApplication.java:62
2016-10-26 16:27:53 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:27:53 ERROR [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private Object[] deserializetoobject_values;
/* 010 */   private org.apache.spark.sql.types.StructType deserializetoobject_schema;
/* 011 */   private UnsafeRow deserializetoobject_result;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;
/* 014 */   private UnsafeRow mapelements_result;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;
/* 017 */   private UnsafeRow serializefromobject_result;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;
/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;
/* 020 */
/* 021 */   public GeneratedIterator(Object[] references) {
/* 022 */     this.references = references;
/* 023 */   }
/* 024 */
/* 025 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 026 */     partitionIndex = index;
/* 027 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 028 */     scan_input = inputs[0];
/* 029 */
/* 030 */     this.deserializetoobject_schema = (org.apache.spark.sql.types.StructType) references[1];
/* 031 */     deserializetoobject_result = new UnsafeRow(1);
/* 032 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);
/* 033 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);
/* 034 */     mapelements_result = new UnsafeRow(1);
/* 035 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);
/* 036 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);
/* 037 */     serializefromobject_result = new UnsafeRow(3);
/* 038 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);
/* 039 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 3);
/* 040 */   }
/* 041 */
/* 042 */   protected void processNext() throws java.io.IOException {
/* 043 */     while (scan_input.hasNext()) {
/* 044 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 045 */       scan_numOutputRows.add(1);
/* 046 */       boolean scan_isNull3 = scan_row.isNullAt(0);
/* 047 */       UTF8String scan_value3 = scan_isNull3 ? null : (scan_row.getUTF8String(0));
/* 048 */       boolean scan_isNull4 = scan_row.isNullAt(1);
/* 049 */       UTF8String scan_value4 = scan_isNull4 ? null : (scan_row.getUTF8String(1));
/* 050 */       boolean scan_isNull5 = scan_row.isNullAt(2);
/* 051 */       UTF8String scan_value5 = scan_isNull5 ? null : (scan_row.getUTF8String(2));
/* 052 */
/* 053 */       deserializetoobject_values = new Object[3];
/* 054 */
/* 055 */       boolean deserializetoobject_isNull1 = scan_isNull3;
/* 056 */       final java.lang.String deserializetoobject_value1 = deserializetoobject_isNull1 ? null : (java.lang.String) scan_value3.toString();
/* 057 */       deserializetoobject_isNull1 = deserializetoobject_value1 == null;
/* 058 */       if (deserializetoobject_isNull1) {
/* 059 */         deserializetoobject_values[0] = null;
/* 060 */       } else {
/* 061 */         deserializetoobject_values[0] = deserializetoobject_value1;
/* 062 */       }
/* 063 */
/* 064 */       boolean deserializetoobject_isNull3 = scan_isNull4;
/* 065 */       final java.lang.String deserializetoobject_value3 = deserializetoobject_isNull3 ? null : (java.lang.String) scan_value4.toString();
/* 066 */       deserializetoobject_isNull3 = deserializetoobject_value3 == null;
/* 067 */       if (deserializetoobject_isNull3) {
/* 068 */         deserializetoobject_values[1] = null;
/* 069 */       } else {
/* 070 */         deserializetoobject_values[1] = deserializetoobject_value3;
/* 071 */       }
/* 072 */
/* 073 */       boolean deserializetoobject_isNull5 = scan_isNull5;
/* 074 */       final java.lang.String deserializetoobject_value5 = deserializetoobject_isNull5 ? null : (java.lang.String) scan_value5.toString();
/* 075 */       deserializetoobject_isNull5 = deserializetoobject_value5 == null;
/* 076 */       if (deserializetoobject_isNull5) {
/* 077 */         deserializetoobject_values[2] = null;
/* 078 */       } else {
/* 079 */         deserializetoobject_values[2] = deserializetoobject_value5;
/* 080 */       }
/* 081 */
/* 082 */       final org.apache.spark.sql.Row deserializetoobject_value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(deserializetoobject_values, deserializetoobject_schema);
/* 083 */
/* 084 */       Object mapelements_obj = ((Expression) references[2]).eval(null);
/* 085 */       org.apache.spark.api.java.function.MapFunction mapelements_value1 = (org.apache.spark.api.java.function.MapFunction) mapelements_obj;
/* 086 */
/* 087 */       boolean mapelements_isNull = false || false;
/* 088 */
/* 089 */       hx.stream.spark.SparkApplication$Person mapelements_value = null;
/* 090 */       try {
/* 091 */         mapelements_value = mapelements_isNull ? null : (hx.stream.spark.SparkApplication$Person) mapelements_value1.call(deserializetoobject_value);
/* 092 */       } catch (Exception e) {
/* 093 */         org.apache.spark.unsafe.Platform.throwException(e);
/* 094 */       }
/* 095 */
/* 096 */       mapelements_isNull = mapelements_value == null;
/* 097 */
/* 098 */       boolean serializefromobject_isNull = mapelements_isNull;
/* 099 */       final int serializefromobject_value = serializefromobject_isNull ? -1 : mapelements_value.getAge();
/* 100 */       boolean serializefromobject_isNull2 = mapelements_isNull;
/* 101 */       final int serializefromobject_value2 = serializefromobject_isNull2 ? -1 : mapelements_value.getId();
/* 102 */       boolean serializefromobject_isNull5 = mapelements_isNull;
/* 103 */       final java.lang.String serializefromobject_value5 = serializefromobject_isNull5 ? null : (java.lang.String) mapelements_value.getName();
/* 104 */       serializefromobject_isNull5 = serializefromobject_value5 == null;
/* 105 */       boolean serializefromobject_isNull4 = serializefromobject_isNull5;
/* 106 */       final UTF8String serializefromobject_value4 = serializefromobject_isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value5);
/* 107 */       serializefromobject_isNull4 = serializefromobject_value4 == null;
/* 108 */       serializefromobject_holder.reset();
/* 109 */
/* 110 */       serializefromobject_rowWriter.zeroOutNullBytes();
/* 111 */
/* 112 */       if (serializefromobject_isNull) {
/* 113 */         serializefromobject_rowWriter.setNullAt(0);
/* 114 */       } else {
/* 115 */         serializefromobject_rowWriter.write(0, serializefromobject_value);
/* 116 */       }
/* 117 */
/* 118 */       if (serializefromobject_isNull2) {
/* 119 */         serializefromobject_rowWriter.setNullAt(1);
/* 120 */       } else {
/* 121 */         serializefromobject_rowWriter.write(1, serializefromobject_value2);
/* 122 */       }
/* 123 */
/* 124 */       if (serializefromobject_isNull4) {
/* 125 */         serializefromobject_rowWriter.setNullAt(2);
/* 126 */       } else {
/* 127 */         serializefromobject_rowWriter.write(2, serializefromobject_value4);
/* 128 */       }
/* 129 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());
/* 130 */       append(serializefromobject_result);
/* 131 */       if (shouldStop()) return;
/* 132 */     }
/* 133 */   }
/* 134 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1273)
	at org.codehaus.janino.UnitCompiler.access$1500(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitWhileStatement(UnitCompiler.java:940)
	at org.codehaus.janino.Java$WhileStatement.accept(Java.java:2244)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:350)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2176)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2175)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2175)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2174)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2174)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:62)
2016-10-26 16:27:53 WARN  [main] o.a.s.s.e.WholeStageCodegenExec [Logging.scala:66] : Whole-stage codegen disabled for this plan:
 *SerializeFromObject [input[0, hx.stream.spark.SparkApplication$Person, true].getAge AS age#71, input[0, hx.stream.spark.SparkApplication$Person, true].getId AS id#72, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, hx.stream.spark.SparkApplication$Person, true].getName, true) AS name#73]
+- *MapElements hx.stream.spark.SparkApplication$$Lambda$8/452384342@16f15a4, obj#70: hx.stream.spark.SparkApplication$Person
   +- *DeserializeToObject createexternalrow(age#12.toString, id#13.toString, name#14.toString, StructField(age,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true)), obj#69: org.apache.spark.sql.Row
      +- *Scan json [age#12,id#13,name#14] Format: JSON, InputPaths: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resou..., PushedFilters: [], ReadSchema: struct<age:string,id:string,name:string>

2016-10-26 16:27:53 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:62
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 8 (collectAsList at SparkApplication.java:62) with 1 output partitions
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 8 (collectAsList at SparkApplication.java:62)
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 8 (MapPartitionsRDD[32] at collectAsList at SparkApplication.java:62), which has no missing parents
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_17 stored as values in memory (estimated size 11.2 KB, free 911.9 MB)
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.9 KB, free 911.9 MB)
2016-10-26 16:27:53 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_17_piece0 in memory on 172.16.106.190:54758 (size: 5.9 KB, free: 912.2 MB)
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 17 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[32] at collectAsList at SparkApplication.java:62)
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 8.0 with 1 tasks
2016-10-26 16:27:53 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0, PROCESS_LOCAL, 5940 bytes)
2016-10-26 16:27:53 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 8.0 (TID 10)
2016-10-26 16:27:53 ERROR [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 16:27:53 ERROR [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 8.0 (TID 10)
java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 common frames omitted
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 common frames omitted
2016-10-26 16:27:53 WARN  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 8.0 (TID 10, localhost): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 more
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 more

2016-10-26 16:27:53 ERROR [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 8.0 failed 1 times; aborting job
2016-10-26 16:27:53 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 8.0, whose tasks have all completed, from pool 
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 8
2016-10-26 16:27:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 8 (collectAsList at SparkApplication.java:62) failed in 0.067 s
2016-10-26 16:27:53 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 8 failed: collectAsList at SparkApplication.java:62, took 0.076738 s
2016-10-26 16:27:53 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:27:53 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:27:53 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:27:53 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:27:53 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:27:53 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:27:53 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:27:53 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:27:53 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:27:53 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:27:53 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-cd29aa29-2c4c-49ab-a8bc-2dbcc495f71b
2016-10-26 16:30:14 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:30:15 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:30:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:30:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:30:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:30:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:30:15 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:30:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54768.
2016-10-26 16:30:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:30:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:30:16 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-ef17c05a-dbf3-4062-af95-ed7408e9a7ba
2016-10-26 16:30:16 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:30:16 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:30:16 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2740ms
2016-10-26 16:30:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:30:16 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:30:16 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2854ms
2016-10-26 16:30:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:30:16 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:30:16 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:30:16 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54769.
2016-10-26 16:30:16 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54769
2016-10-26 16:30:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54769)
2016-10-26 16:30:16 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54769 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54769)
2016-10-26 16:30:16 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54769)
2016-10-26 16:30:16 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:30:16 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:30:17 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:30:17 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:30:18 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:30:18 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:30:19 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 16:30:20 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 16:30:20 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 16:30:21 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 16:30:21 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/5b9e3f1d-b44d-441f-8436-f6c4b4ea16b7_resources
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/5b9e3f1d-b44d-441f-8436-f6c4b4ea16b7
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/5b9e3f1d-b44d-441f-8436-f6c4b4ea16b7
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/5b9e3f1d-b44d-441f-8436-f6c4b4ea16b7/_tmp_space.db
2016-10-26 16:30:21 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/fe666384-1397-41d2-84a3-6fe1c716b52d_resources
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/fe666384-1397-41d2-84a3-6fe1c716b52d
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/fe666384-1397-41d2-84a3-6fe1c716b52d
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/fe666384-1397-41d2-84a3-6fe1c716b52d/_tmp_space.db
2016-10-26 16:30:21 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:30:21 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 16:30:21 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 16:30:21 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy15.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy16.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:492)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:472)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:30)

2016-10-26 16:30:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:30:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:30:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<value: string>
2016-10-26 16:30:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:30:22 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:30:22 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:30:22 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54769 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:30:22 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from first at SparkApplication.java:34
2016-10-26 16:30:22 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:30:23 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 304.782067 ms
2016-10-26 16:30:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: first at SparkApplication.java:34
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (first at SparkApplication.java:34) with 1 output partitions
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (first at SparkApplication.java:34)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:34), which has no missing parents
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 6.5 KB, free 912.1 MB)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 912.1 MB)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54769 (size: 3.6 KB, free: 912.3 MB)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at first at SparkApplication.java:34)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 1 tasks
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5793 bytes)
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///usr/local/Cellar/apache-spark/1.6.1/README.md, range: 0-3359, partition values: [empty row]
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.844958 ms
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1285 bytes result sent to driver
2016-10-26 16:30:23 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 156 ms on localhost (1/1)
2016-10-26 16:30:23 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (first at SparkApplication.java:34) finished in 0.172 s
2016-10-26 16:30:23 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: first at SparkApplication.java:34, took 0.331993 s
2016-10-26 16:30:23 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.131076 ms
2016-10-26 16:30:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.8 KB, free 912.0 MB)
2016-10-26 16:30:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.0 MB)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54769 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:30:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from json at SparkApplication.java:37
2016-10-26 16:30:23 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:30:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:37
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (json at SparkApplication.java:37) with 2 output partitions
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (json at SparkApplication.java:37)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:37), which has no missing parents
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 4.3 KB, free 912.0 MB)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.0 MB)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54769 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at json at SparkApplication.java:37)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 2 tasks
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5450 bytes)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1, PROCESS_LOCAL, 5450 bytes)
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 1)
2016-10-26 16:30:23 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 1.0 (TID 2)
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:0+518
2016-10-26 16:30:23 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json:518+518
2016-10-26 16:30:23 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:30:23 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:30:23 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:30:23 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 2). 1616 bytes result sent to driver
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1). 1616 bytes result sent to driver
2016-10-26 16:30:23 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 1.0 (TID 2) in 34 ms on localhost (1/2)
2016-10-26 16:30:23 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 1) in 45 ms on localhost (2/2)
2016-10-26 16:30:23 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (json at SparkApplication.java:37) finished in 0.047 s
2016-10-26 16:30:23 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: json at SparkApplication.java:37, took 0.060803 s
2016-10-26 16:30:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:30:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:30:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<_corrupt_record: string>
2016-10-26 16:30:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:30:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 16:30:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54769 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:30:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:40
2016-10-26 16:30:23 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:30:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:40
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:40) with 1 output partitions
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:40)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:40), which has no missing parents
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 911.8 MB)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.8 MB)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54769 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:40)
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5857 bytes)
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/response.json, range: 0-1036, partition values: [empty row]
2016-10-26 16:30:23 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1670 bytes result sent to driver
2016-10-26 16:30:23 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 24 ms on localhost (1/1)
2016-10-26 16:30:23 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 16:30:23 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:40) finished in 0.025 s
2016-10-26 16:30:23 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:40, took 0.041013 s
2016-10-26 16:30:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.8 KB, free 911.7 MB)
2016-10-26 16:30:23 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KB, free 911.7 MB)
2016-10-26 16:30:23 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54769 (size: 14.9 KB, free: 912.2 MB)
2016-10-26 16:30:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from json at SparkApplication.java:43
2016-10-26 16:30:24 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:43
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (json at SparkApplication.java:43) with 2 output partitions
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 3 (json at SparkApplication.java:43)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:43), which has no missing parents
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 4.3 KB, free 911.7 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.6 KB, free 911.7 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54769 (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at json at SparkApplication.java:43)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 2 tasks
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 3.0 (TID 5, localhost, partition 1, PROCESS_LOCAL, 5449 bytes)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 16:30:24 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 3.0 (TID 5)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 16:30:24 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 16:30:24 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5). 1650 bytes result sent to driver
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1650 bytes result sent to driver
2016-10-26 16:30:24 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 3.0 (TID 5) in 13 ms on localhost (1/2)
2016-10-26 16:30:24 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 16 ms on localhost (2/2)
2016-10-26 16:30:24 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 3 (json at SparkApplication.java:43) finished in 0.017 s
2016-10-26 16:30:24 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: json at SparkApplication.java:43, took 0.029520 s
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:30:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 133.4 KB, free 911.6 MB)
2016-10-26 16:30:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.5 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:54769 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from show at SparkApplication.java:46
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:46
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:46) with 1 output partitions
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (show at SparkApplication.java:46)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:46), which has no missing parents
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 7.2 KB, free 911.5 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.5 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:54769 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at show at SparkApplication.java:46)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 6)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.388569 ms
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 16:30:24 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 6) in 27 ms on localhost (1/1)
2016-10-26 16:30:24 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (show at SparkApplication.java:46) finished in 0.027 s
2016-10-26 16:30:24 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:46, took 0.040048 s
2016-10-26 16:30:24 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.869116 ms
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:30:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 133.4 KB, free 911.4 MB)
2016-10-26 16:30:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.4 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:54769 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from show at SparkApplication.java:47
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:47
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (show at SparkApplication.java:47) with 1 output partitions
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:47)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:47), which has no missing parents
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 7.1 KB, free 911.4 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.0 KB, free 911.4 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:54769 (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at show at SparkApplication.java:47)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 7)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 7). 1336 bytes result sent to driver
2016-10-26 16:30:24 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 7) in 12 ms on localhost (1/1)
2016-10-26 16:30:24 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:47) finished in 0.013 s
2016-10-26 16:30:24 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: show at SparkApplication.java:47, took 0.024300 s
2016-10-26 16:30:24 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:30:24 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:30:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 133.4 KB, free 911.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:54769 in memory (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:54769 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 0
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 1
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:54769 in memory (size: 3.6 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:54769 in memory (size: 14.9 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:54769 in memory (size: 2.6 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:54769 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:54769 in memory (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:54769 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:54769 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 224
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 225
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:54769 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:54769 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 270
2016-10-26 16:30:24 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 271
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_11_piece0 on 172.16.106.190:54769 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:54769 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from show at SparkApplication.java:51
2016-10-26 16:30:24 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:30:24 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:51
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (show at SparkApplication.java:51) with 1 output partitions
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (show at SparkApplication.java:51)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:51), which has no missing parents
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:54769 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[21] at show at SparkApplication.java:51)
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 16:30:24 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 8)
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:30:24 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 8). 1410 bytes result sent to driver
2016-10-26 16:30:24 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 8) in 14 ms on localhost (1/1)
2016-10-26 16:30:24 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 16:30:24 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (show at SparkApplication.java:51) finished in 0.014 s
2016-10-26 16:30:24 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 finished: show at SparkApplication.java:51, took 0.024542 s
2016-10-26 16:30:24 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 16:30:24 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:30:24 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:30:24 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:30:24 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:30:25 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 16:30:25 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:30:25 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:30:25 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:30:25 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:30:25 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:30:25 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:30:25 INFO  [dispatcher-event-loop-3] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:30:25 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:30:25 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:30:25 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:30:25 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:30:25 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:30:25 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:30:25 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-845b2302-dcfc-471b-813d-4f521613c72b
2016-10-26 16:50:17 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:50:18 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:50:18 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54933.
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:50:18 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-6bd87e16-4753-4bb2-8b7e-cc8c21ea5f2b
2016-10-26 16:50:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:50:18 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:50:19 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2403ms
2016-10-26 16:50:19 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:50:19 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:50:19 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2558ms
2016-10-26 16:50:19 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:50:19 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:50:19 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:50:19 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54934.
2016-10-26 16:50:19 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54934
2016-10-26 16:50:19 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54934)
2016-10-26 16:50:19 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54934 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54934)
2016-10-26 16:50:19 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54934)
2016-10-26 16:50:19 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:50:19 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:50:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 16:50:21 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 16:50:21 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54934 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:50:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:31
2016-10-26 16:50:21 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:50:21 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:31
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:31) with 2 output partitions
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:31)
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:31), which has no missing parents
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 16:50:21 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54934 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:31)
2016-10-26 16:50:21 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 16:50:21 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 16:50:21 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 16:50:21 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:50:21 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 16:50:22 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 16:50:22 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 16:50:22 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:50:22 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:50:22 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:50:22 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:50:22 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:50:22 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:50:22 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:50:22 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-10-26 16:50:22 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-10-26 16:50:22 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 286 ms on localhost (1/2)
2016-10-26 16:50:22 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 233 ms on localhost (2/2)
2016-10-26 16:50:22 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:50:22 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:31) finished in 0.309 s
2016-10-26 16:50:22 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:31, took 0.417094 s
2016-10-26 16:50:22 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:54934 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:50:22 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:54934 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:50:22 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:50:22 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:50:22 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:50:22 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:50:24 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 16:50:25 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 16:50:25 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 16:50:26 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 16:50:26 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/430aa178-21a4-455f-8064-8afb4fa50605_resources
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/430aa178-21a4-455f-8064-8afb4fa50605
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/430aa178-21a4-455f-8064-8afb4fa50605
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/430aa178-21a4-455f-8064-8afb4fa50605/_tmp_space.db
2016-10-26 16:50:26 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/7412badc-4d7c-4b16-9384-81f8c461417a_resources
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/7412badc-4d7c-4b16-9384-81f8c461417a
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/7412badc-4d7c-4b16-9384-81f8c461417a
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/7412badc-4d7c-4b16-9384-81f8c461417a/_tmp_space.db
2016-10-26 16:50:26 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:50:26 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 16:50:26 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 16:50:26 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:31)

2016-10-26 16:50:27 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:50:27 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:50:27 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:50:27 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:50:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:50:27 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:50:27 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54934 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:27 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkApplication.java:34
2016-10-26 16:50:27 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:50:27 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 255.120149 ms
2016-10-26 16:50:27 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:34
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkApplication.java:34) with 1 output partitions
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkApplication.java:34)
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:34), which has no missing parents
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 16:50:27 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54934 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:34)
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 16:50:27 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:50:27 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 16:50:27 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:50:27 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.870579 ms
2016-10-26 16:50:27 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-10-26 16:50:27 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 70 ms on localhost (1/1)
2016-10-26 16:50:27 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:50:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkApplication.java:34) finished in 0.070 s
2016-10-26 16:50:27 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkApplication.java:34, took 0.096136 s
2016-10-26 16:50:28 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.611645 ms
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:50:28 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 16:50:28 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54934 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:36
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:36
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:36) with 1 output partitions
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:36)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:36), which has no missing parents
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54934 (size: 4.0 KB, free: 912.3 MB)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:36)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.944125 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1336 bytes result sent to driver
2016-10-26 16:50:28 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 25 ms on localhost (1/1)
2016-10-26 16:50:28 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:36) finished in 0.026 s
2016-10-26 16:50:28 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:36, took 0.036082 s
2016-10-26 16:50:28 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.251321 ms
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:50:28 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 16:50:28 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54934 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkApplication.java:37
2016-10-26 16:50:28 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:50:28 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkApplication.java:37
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkApplication.java:37)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkApplication.java:37) with 1 output partitions
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkApplication.java:37)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:37), which has no missing parents
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54934 (size: 7.8 KB, free: 912.2 MB)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:37)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.883219 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 41.703174 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.209918 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.085164 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.635769 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-10-26 16:50:28 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 289 ms on localhost (1/1)
2016-10-26 16:50:28 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkApplication.java:37) finished in 0.290 s
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:37), which has no missing parents
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 911.8 MB)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.8 MB)
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:54934 (size: 8.0 KB, free: 912.2 MB)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:37)
2016-10-26 16:50:28 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 16:50:28 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 5 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 29.471377 ms
2016-10-26 16:50:28 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 20.410019 ms
2016-10-26 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.521833 ms
2016-10-26 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.215858 ms
2016-10-26 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-10-26 16:50:29 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 188 ms on localhost (1/1)
2016-10-26 16:50:29 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkApplication.java:37) finished in 0.188 s
2016-10-26 16:50:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkApplication.java:37, took 0.569530 s
2016-10-26 16:50:29 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.843567 ms
2016-10-26 16:50:29 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 6.263289 ms
2016-10-26 16:50:29 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.937053 ms
2016-10-26 16:50:29 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:50:29 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 16:50:29 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:50:29 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:50:29 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:50:29 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:50:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 133.4 KB, free 911.7 MB)
2016-10-26 16:50:29 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.7 MB)
2016-10-26 16:50:29 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:54934 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:50:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkApplication.java:41
2016-10-26 16:50:29 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:50:29 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:41
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:41) with 1 output partitions
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:41)
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:41), which has no missing parents
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 911.6 MB)
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.6 MB)
2016-10-26 16:50:29 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:54934 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:41)
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 16:50:29 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-10-26 16:50:29 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:50:29 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 16:50:29 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 12 ms on localhost (1/1)
2016-10-26 16:50:29 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 16:50:29 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:41) finished in 0.013 s
2016-10-26 16:50:29 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:41, took 0.021931 s
2016-10-26 16:50:29 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:50:29 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 16:50:29 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:29 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: drop_table : db=default tbl=person
2016-10-26 16:50:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=person	
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:54934 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:54934 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:54934 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:54934 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:54934 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:54934 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 159
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 160
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 161
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 162
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 163
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned shuffle 0
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:54934 in memory (size: 7.8 KB, free: 912.3 MB)
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:54934 in memory (size: 8.0 KB, free: 912.3 MB)
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 252
2016-10-26 16:50:30 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:54934 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 253
2016-10-26 16:50:30 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 254
2016-10-26 16:50:31 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] : deleting  file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 16:50:31 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2016-10-26 16:50:31 INFO  [main] o.a.hadoop.fs.TrashPolicyDefault [TrashPolicyDefault.java:92] : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2016-10-26 16:50:31 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] : Deleted the diretory file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_tables: db=default pat=*
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
2016-10-26 16:50:31 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 7.659916 ms
2016-10-26 16:50:31 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:50:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:50:31 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:50:31 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:50:31 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:54934 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from saveAsTable at SparkApplication.java:59
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:50:31 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:50:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:59
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTable at SparkApplication.java:59) with 1 output partitions
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (saveAsTable at SparkApplication.java:59)
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:59), which has no missing parents
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 54.2 KB, free 912.1 MB)
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.4 KB, free 912.1 MB)
2016-10-26 16:50:31 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:54934 (size: 20.4 KB, free: 912.3 MB)
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:59)
2016-10-26 16:50:31 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 16:50:31 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 7)
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 16:50:31 INFO  [Executor task launch worker-1] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 16:50:32 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:50:32 INFO  [Executor task launch worker-1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261650_0006_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261650_0006_m_000000
2016-10-26 16:50:32 INFO  [Executor task launch worker-1] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261650_0006_m_000000_0: Committed
2016-10-26 16:50:32 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7). 1295 bytes result sent to driver
2016-10-26 16:50:32 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7) in 734 ms on localhost (1/1)
2016-10-26 16:50:32 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 16:50:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (saveAsTable at SparkApplication.java:59) finished in 0.735 s
2016-10-26 16:50:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTable at SparkApplication.java:59, took 0.757808 s
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261650_0000 committed.
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 16:50:32 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:32 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:32 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:50:32 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:50:32 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477471832, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 16:50:32 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477471832, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 16:50:32 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 16:50:32 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 16:50:32 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:50:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 16:50:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.9 MB)
2016-10-26 16:50:32 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:54934 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:50:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from collectAsList at SparkApplication.java:65
2016-10-26 16:50:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:50:32 ERROR [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private Object[] deserializetoobject_values;
/* 010 */   private org.apache.spark.sql.types.StructType deserializetoobject_schema;
/* 011 */   private UnsafeRow deserializetoobject_result;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;
/* 014 */   private UnsafeRow mapelements_result;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;
/* 017 */   private UnsafeRow serializefromobject_result;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;
/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;
/* 020 */
/* 021 */   public GeneratedIterator(Object[] references) {
/* 022 */     this.references = references;
/* 023 */   }
/* 024 */
/* 025 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 026 */     partitionIndex = index;
/* 027 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 028 */     scan_input = inputs[0];
/* 029 */
/* 030 */     this.deserializetoobject_schema = (org.apache.spark.sql.types.StructType) references[1];
/* 031 */     deserializetoobject_result = new UnsafeRow(1);
/* 032 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);
/* 033 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);
/* 034 */     mapelements_result = new UnsafeRow(1);
/* 035 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);
/* 036 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);
/* 037 */     serializefromobject_result = new UnsafeRow(3);
/* 038 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);
/* 039 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 3);
/* 040 */   }
/* 041 */
/* 042 */   protected void processNext() throws java.io.IOException {
/* 043 */     while (scan_input.hasNext()) {
/* 044 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 045 */       scan_numOutputRows.add(1);
/* 046 */       boolean scan_isNull3 = scan_row.isNullAt(0);
/* 047 */       UTF8String scan_value3 = scan_isNull3 ? null : (scan_row.getUTF8String(0));
/* 048 */       boolean scan_isNull4 = scan_row.isNullAt(1);
/* 049 */       UTF8String scan_value4 = scan_isNull4 ? null : (scan_row.getUTF8String(1));
/* 050 */       boolean scan_isNull5 = scan_row.isNullAt(2);
/* 051 */       UTF8String scan_value5 = scan_isNull5 ? null : (scan_row.getUTF8String(2));
/* 052 */
/* 053 */       deserializetoobject_values = new Object[3];
/* 054 */
/* 055 */       boolean deserializetoobject_isNull1 = scan_isNull3;
/* 056 */       final java.lang.String deserializetoobject_value1 = deserializetoobject_isNull1 ? null : (java.lang.String) scan_value3.toString();
/* 057 */       deserializetoobject_isNull1 = deserializetoobject_value1 == null;
/* 058 */       if (deserializetoobject_isNull1) {
/* 059 */         deserializetoobject_values[0] = null;
/* 060 */       } else {
/* 061 */         deserializetoobject_values[0] = deserializetoobject_value1;
/* 062 */       }
/* 063 */
/* 064 */       boolean deserializetoobject_isNull3 = scan_isNull4;
/* 065 */       final java.lang.String deserializetoobject_value3 = deserializetoobject_isNull3 ? null : (java.lang.String) scan_value4.toString();
/* 066 */       deserializetoobject_isNull3 = deserializetoobject_value3 == null;
/* 067 */       if (deserializetoobject_isNull3) {
/* 068 */         deserializetoobject_values[1] = null;
/* 069 */       } else {
/* 070 */         deserializetoobject_values[1] = deserializetoobject_value3;
/* 071 */       }
/* 072 */
/* 073 */       boolean deserializetoobject_isNull5 = scan_isNull5;
/* 074 */       final java.lang.String deserializetoobject_value5 = deserializetoobject_isNull5 ? null : (java.lang.String) scan_value5.toString();
/* 075 */       deserializetoobject_isNull5 = deserializetoobject_value5 == null;
/* 076 */       if (deserializetoobject_isNull5) {
/* 077 */         deserializetoobject_values[2] = null;
/* 078 */       } else {
/* 079 */         deserializetoobject_values[2] = deserializetoobject_value5;
/* 080 */       }
/* 081 */
/* 082 */       final org.apache.spark.sql.Row deserializetoobject_value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(deserializetoobject_values, deserializetoobject_schema);
/* 083 */
/* 084 */       Object mapelements_obj = ((Expression) references[2]).eval(null);
/* 085 */       org.apache.spark.api.java.function.MapFunction mapelements_value1 = (org.apache.spark.api.java.function.MapFunction) mapelements_obj;
/* 086 */
/* 087 */       boolean mapelements_isNull = false || false;
/* 088 */
/* 089 */       hx.stream.spark.SparkApplication$Person mapelements_value = null;
/* 090 */       try {
/* 091 */         mapelements_value = mapelements_isNull ? null : (hx.stream.spark.SparkApplication$Person) mapelements_value1.call(deserializetoobject_value);
/* 092 */       } catch (Exception e) {
/* 093 */         org.apache.spark.unsafe.Platform.throwException(e);
/* 094 */       }
/* 095 */
/* 096 */       mapelements_isNull = mapelements_value == null;
/* 097 */
/* 098 */       boolean serializefromobject_isNull = mapelements_isNull;
/* 099 */       final int serializefromobject_value = serializefromobject_isNull ? -1 : mapelements_value.getAge();
/* 100 */       boolean serializefromobject_isNull2 = mapelements_isNull;
/* 101 */       final int serializefromobject_value2 = serializefromobject_isNull2 ? -1 : mapelements_value.getId();
/* 102 */       boolean serializefromobject_isNull5 = mapelements_isNull;
/* 103 */       final java.lang.String serializefromobject_value5 = serializefromobject_isNull5 ? null : (java.lang.String) mapelements_value.getName();
/* 104 */       serializefromobject_isNull5 = serializefromobject_value5 == null;
/* 105 */       boolean serializefromobject_isNull4 = serializefromobject_isNull5;
/* 106 */       final UTF8String serializefromobject_value4 = serializefromobject_isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value5);
/* 107 */       serializefromobject_isNull4 = serializefromobject_value4 == null;
/* 108 */       serializefromobject_holder.reset();
/* 109 */
/* 110 */       serializefromobject_rowWriter.zeroOutNullBytes();
/* 111 */
/* 112 */       if (serializefromobject_isNull) {
/* 113 */         serializefromobject_rowWriter.setNullAt(0);
/* 114 */       } else {
/* 115 */         serializefromobject_rowWriter.write(0, serializefromobject_value);
/* 116 */       }
/* 117 */
/* 118 */       if (serializefromobject_isNull2) {
/* 119 */         serializefromobject_rowWriter.setNullAt(1);
/* 120 */       } else {
/* 121 */         serializefromobject_rowWriter.write(1, serializefromobject_value2);
/* 122 */       }
/* 123 */
/* 124 */       if (serializefromobject_isNull4) {
/* 125 */         serializefromobject_rowWriter.setNullAt(2);
/* 126 */       } else {
/* 127 */         serializefromobject_rowWriter.write(2, serializefromobject_value4);
/* 128 */       }
/* 129 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());
/* 130 */       append(serializefromobject_result);
/* 131 */       if (shouldStop()) return;
/* 132 */     }
/* 133 */   }
/* 134 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1273)
	at org.codehaus.janino.UnitCompiler.access$1500(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitWhileStatement(UnitCompiler.java:940)
	at org.codehaus.janino.Java$WhileStatement.accept(Java.java:2244)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:350)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2176)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2175)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2175)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2174)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2174)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:65)
2016-10-26 16:50:32 WARN  [main] o.a.s.s.e.WholeStageCodegenExec [Logging.scala:66] : Whole-stage codegen disabled for this plan:
 *SerializeFromObject [input[0, hx.stream.spark.SparkApplication$Person, true].getAge AS age#164, input[0, hx.stream.spark.SparkApplication$Person, true].getId AS id#165, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, hx.stream.spark.SparkApplication$Person, true].getName, true) AS name#166]
+- *MapElements hx.stream.spark.SparkApplication$$Lambda$9/44341075@4d50f682, obj#163: hx.stream.spark.SparkApplication$Person
   +- *DeserializeToObject createexternalrow(age#0.toString, id#1.toString, name#2.toString, StructField(age,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true)), obj#162: org.apache.spark.sql.Row
      +- *Scan json [age#0,id#1,name#2] Format: JSON, InputPaths: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resou..., PushedFilters: [], ReadSchema: struct<age:string,id:string,name:string>

2016-10-26 16:50:33 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:65
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (collectAsList at SparkApplication.java:65) with 1 output partitions
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (collectAsList at SparkApplication.java:65)
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:65), which has no missing parents
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 11.2 KB, free 911.9 MB)
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.9 KB, free 911.9 MB)
2016-10-26 16:50:33 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:54934 (size: 5.9 KB, free: 912.2 MB)
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:65)
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 16:50:33 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5940 bytes)
2016-10-26 16:50:33 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 16:50:33 ERROR [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 16:50:33 ERROR [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 7.0 (TID 8)
java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 common frames omitted
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 common frames omitted
2016-10-26 16:50:33 WARN  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 7.0 (TID 8, localhost): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 more
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 more

2016-10-26 16:50:33 ERROR [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 7.0 failed 1 times; aborting job
2016-10-26 16:50:33 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 7
2016-10-26 16:50:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (collectAsList at SparkApplication.java:65) failed in 0.069 s
2016-10-26 16:50:33 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 failed: collectAsList at SparkApplication.java:65, took 0.080047 s
2016-10-26 16:50:33 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:50:33 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:50:33 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:50:33 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:50:33 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:50:33 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:50:33 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:50:33 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:50:33 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:50:33 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:50:33 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-96e817c1-da71-4aad-9e65-e08d266e6522
2016-10-26 16:51:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:51:43 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:51:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54947.
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:51:43 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-14ddf79c-e4dd-46e1-8ab9-accf7610c3bc
2016-10-26 16:51:43 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:51:43 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:51:43 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1818ms
2016-10-26 16:51:43 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:51:43 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@18fb69ab{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:51:43 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @1940ms
2016-10-26 16:51:43 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:51:43 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:51:44 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:51:44 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54948.
2016-10-26 16:51:44 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54948
2016-10-26 16:51:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54948)
2016-10-26 16:51:44 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54948 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54948)
2016-10-26 16:51:44 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54948)
2016-10-26 16:51:44 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:51:44 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:51:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 16:51:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 16:51:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54948 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:51:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:31
2016-10-26 16:51:45 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:51:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:31
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:31) with 2 output partitions
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:31)
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:31), which has no missing parents
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 16:51:45 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54948 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:31)
2016-10-26 16:51:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 16:51:45 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 16:51:45 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 16:51:45 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:51:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 16:51:46 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 16:51:46 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 16:51:46 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:51:46 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:51:46 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:51:46 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:51:46 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:51:46 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:51:46 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1810 bytes result sent to driver
2016-10-26 16:51:46 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1810 bytes result sent to driver
2016-10-26 16:51:46 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 242 ms on localhost (1/2)
2016-10-26 16:51:46 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 210 ms on localhost (2/2)
2016-10-26 16:51:46 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:51:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:31) finished in 0.268 s
2016-10-26 16:51:46 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:31, took 0.340531 s
2016-10-26 16:51:46 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:51:46 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:51:46 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:51:46 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:51:46 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:51:46 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@18fb69ab{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:51:46 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:51:46 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:51:46 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:51:46 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:51:46 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:51:46 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:51:46 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:51:46 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:51:46 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-a60a219f-3bbc-409a-9bf0-a9c4053f2edb
2016-10-26 16:54:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 16:54:33 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 16:54:33 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 54987.
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 16:54:33 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-bbf29621-01fc-4868-83fb-7ba7db33ac7e
2016-10-26 16:54:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 16:54:33 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 16:54:33 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1814ms
2016-10-26 16:54:33 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 16:54:33 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:54:33 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @1943ms
2016-10-26 16:54:33 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 16:54:33 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 16:54:34 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 16:54:34 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54988.
2016-10-26 16:54:34 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:54988
2016-10-26 16:54:34 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 54988)
2016-10-26 16:54:34 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:54988 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 54988)
2016-10-26 16:54:34 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 54988)
2016-10-26 16:54:34 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 16:54:34 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 16:54:35 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 16:54:35 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 16:54:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:54988 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:54:35 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:32
2016-10-26 16:54:35 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 16:54:36 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:32
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:32) with 2 output partitions
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:32)
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32), which has no missing parents
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 16:54:36 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:54988 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32)
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 16:54:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 16:54:36 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 16:54:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 16:54:36 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 16:54:36 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:54:36 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 16:54:36 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 16:54:36 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-10-26 16:54:36 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-10-26 16:54:36 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 224 ms on localhost (1/2)
2016-10-26 16:54:36 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 272 ms on localhost (2/2)
2016-10-26 16:54:36 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 16:54:36 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:32) finished in 0.285 s
2016-10-26 16:54:36 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:32, took 0.360161 s
2016-10-26 16:54:36 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 16:54:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 16:54:36 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:54988 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 16:54:36 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:54988 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 16:54:36 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 16:54:36 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 16:54:38 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 16:54:39 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 16:54:39 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 16:54:39 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 16:54:39 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 16:54:39 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 16:54:40 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 16:54:40 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/260f5dbe-473f-4a5d-8304-5655f89fc5dc_resources
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/260f5dbe-473f-4a5d-8304-5655f89fc5dc
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/260f5dbe-473f-4a5d-8304-5655f89fc5dc
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/260f5dbe-473f-4a5d-8304-5655f89fc5dc/_tmp_space.db
2016-10-26 16:54:40 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/1a91b7a1-09f2-4103-95f4-d127d2113fea_resources
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/1a91b7a1-09f2-4103-95f4-d127d2113fea
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/1a91b7a1-09f2-4103-95f4-d127d2113fea
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/1a91b7a1-09f2-4103-95f4-d127d2113fea/_tmp_space.db
2016-10-26 16:54:40 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 16:54:40 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 16:54:40 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 16:54:40 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:32)

2016-10-26 16:54:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:54:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:54:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:54:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:54:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:54:40 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:54:40 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:54988 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:40 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkApplication.java:35
2016-10-26 16:54:40 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:54:41 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 225.153087 ms
2016-10-26 16:54:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:35
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkApplication.java:35) with 1 output partitions
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkApplication.java:35)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35), which has no missing parents
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 16:54:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:54988 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 16:54:41 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 25.869719 ms
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-10-26 16:54:41 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 107 ms on localhost (1/1)
2016-10-26 16:54:41 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkApplication.java:35) finished in 0.109 s
2016-10-26 16:54:41 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkApplication.java:35, took 0.131134 s
2016-10-26 16:54:41 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 21.236047 ms
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:54:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 16:54:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 16:54:41 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:54988 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:37
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:54:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:37
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:37) with 1 output partitions
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:37)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37), which has no missing parents
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-10-26 16:54:41 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:54988 (size: 4.0 KB, free: 912.3 MB)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 16:54:41 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.21398 ms
2016-10-26 16:54:41 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1423 bytes result sent to driver
2016-10-26 16:54:41 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 29 ms on localhost (1/1)
2016-10-26 16:54:41 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:37) finished in 0.030 s
2016-10-26 16:54:41 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:37, took 0.042724 s
2016-10-26 16:54:41 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.009461 ms
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:54:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 16:54:41 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 16:54:41 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:54988 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:54:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkApplication.java:38
2016-10-26 16:54:41 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:54:41 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkApplication.java:38
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkApplication.java:38)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkApplication.java:38) with 1 output partitions
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkApplication.java:38)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 16:54:41 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:54988 (size: 7.8 KB, free: 912.2 MB)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.690023 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 40.58895 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 7.650538 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.850749 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.845326 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-10-26 16:54:42 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 236 ms on localhost (1/1)
2016-10-26 16:54:42 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkApplication.java:38) finished in 0.237 s
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 911.8 MB)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.8 MB)
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:54988 (size: 8.0 KB, free: 912.2 MB)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.396464 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 22.663713 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 20.606919 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.352442 ms
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-10-26 16:54:42 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 170 ms on localhost (1/1)
2016-10-26 16:54:42 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkApplication.java:38) finished in 0.171 s
2016-10-26 16:54:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkApplication.java:38, took 0.498565 s
2016-10-26 16:54:42 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.467705 ms
2016-10-26 16:54:42 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.350534 ms
2016-10-26 16:54:42 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.993105 ms
2016-10-26 16:54:42 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:54:42 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 16:54:42 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:54:42 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:54:42 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:54:42 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:54:42 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 133.4 KB, free 911.7 MB)
2016-10-26 16:54:42 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.7 MB)
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:54988 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:54:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkApplication.java:42
2016-10-26 16:54:42 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:54:42 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:42
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:42) with 1 output partitions
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:42)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42), which has no missing parents
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 911.6 MB)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.6 MB)
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:54988 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42)
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 16:54:42 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:54:42 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 16:54:42 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 11 ms on localhost (1/1)
2016-10-26 16:54:42 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 16:54:42 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:42) finished in 0.012 s
2016-10-26 16:54:42 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:42, took 0.024063 s
2016-10-26 16:54:42 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:54:42 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 16:54:42 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:42 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:43 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:43 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: drop_table : db=default tbl=person
2016-10-26 16:54:43 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=person	
2016-10-26 16:54:44 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] : deleting  file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 16:54:44 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2016-10-26 16:54:44 INFO  [main] o.a.hadoop.fs.TrashPolicyDefault [TrashPolicyDefault.java:92] : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2016-10-26 16:54:44 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] : Deleted the diretory file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_tables: db=default pat=*
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
2016-10-26 16:54:44 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 6.486645 ms
2016-10-26 16:54:44 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 252
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:54988 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:54988 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:54988 in memory (size: 4.0 KB, free: 912.2 MB)
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:54988 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 159
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 160
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 161
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 162
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 163
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned shuffle 0
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:54988 in memory (size: 7.8 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:54988 in memory (size: 8.0 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:54988 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 253
2016-10-26 16:54:44 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 254
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:54988 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:54:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 16:54:44 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:54988 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from saveAsTable at SparkApplication.java:60
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:54:44 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:54:44 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:60
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTable at SparkApplication.java:60) with 1 output partitions
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (saveAsTable at SparkApplication.java:60)
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60), which has no missing parents
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 54.2 KB, free 912.0 MB)
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.4 KB, free 911.9 MB)
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:54988 (size: 20.4 KB, free: 912.3 MB)
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60)
2016-10-26 16:54:44 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 16:54:44 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 7)
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 16:54:44 INFO  [Executor task launch worker-1] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 16:54:45 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 16:54:45 INFO  [Executor task launch worker-1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261654_0006_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261654_0006_m_000000
2016-10-26 16:54:45 INFO  [Executor task launch worker-1] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261654_0006_m_000000_0: Committed
2016-10-26 16:54:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7). 1309 bytes result sent to driver
2016-10-26 16:54:45 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7) in 479 ms on localhost (1/1)
2016-10-26 16:54:45 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (saveAsTable at SparkApplication.java:60) finished in 0.479 s
2016-10-26 16:54:45 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTable at SparkApplication.java:60, took 0.501920 s
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261654_0000 committed.
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 16:54:45 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:45 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:45 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 16:54:45 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 16:54:45 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-10-26 16:54:45 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-10-26 16:54:45 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:54988 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:45 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477472085, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 16:54:45 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477472085, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 16:54:45 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_11_piece0 on 172.16.106.190:54988 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:45 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 300
2016-10-26 16:54:45 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 301
2016-10-26 16:54:45 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 16:54:45 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_12_piece0 on 172.16.106.190:54988 in memory (size: 20.4 KB, free: 912.3 MB)
2016-10-26 16:54:45 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 16:54:45 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 16:54:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 16:54:45 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 16:54:45 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:54988 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 16:54:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from collectAsList at SparkApplication.java:66
2016-10-26 16:54:45 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 16:54:45 ERROR [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private Object[] deserializetoobject_values;
/* 010 */   private org.apache.spark.sql.types.StructType deserializetoobject_schema;
/* 011 */   private UnsafeRow deserializetoobject_result;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;
/* 014 */   private UnsafeRow mapelements_result;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;
/* 017 */   private UnsafeRow serializefromobject_result;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;
/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;
/* 020 */
/* 021 */   public GeneratedIterator(Object[] references) {
/* 022 */     this.references = references;
/* 023 */   }
/* 024 */
/* 025 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 026 */     partitionIndex = index;
/* 027 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 028 */     scan_input = inputs[0];
/* 029 */
/* 030 */     this.deserializetoobject_schema = (org.apache.spark.sql.types.StructType) references[1];
/* 031 */     deserializetoobject_result = new UnsafeRow(1);
/* 032 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);
/* 033 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);
/* 034 */     mapelements_result = new UnsafeRow(1);
/* 035 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);
/* 036 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);
/* 037 */     serializefromobject_result = new UnsafeRow(3);
/* 038 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);
/* 039 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 3);
/* 040 */   }
/* 041 */
/* 042 */   protected void processNext() throws java.io.IOException {
/* 043 */     while (scan_input.hasNext()) {
/* 044 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 045 */       scan_numOutputRows.add(1);
/* 046 */       boolean scan_isNull3 = scan_row.isNullAt(0);
/* 047 */       UTF8String scan_value3 = scan_isNull3 ? null : (scan_row.getUTF8String(0));
/* 048 */       boolean scan_isNull4 = scan_row.isNullAt(1);
/* 049 */       UTF8String scan_value4 = scan_isNull4 ? null : (scan_row.getUTF8String(1));
/* 050 */       boolean scan_isNull5 = scan_row.isNullAt(2);
/* 051 */       UTF8String scan_value5 = scan_isNull5 ? null : (scan_row.getUTF8String(2));
/* 052 */
/* 053 */       deserializetoobject_values = new Object[3];
/* 054 */
/* 055 */       boolean deserializetoobject_isNull1 = scan_isNull3;
/* 056 */       final java.lang.String deserializetoobject_value1 = deserializetoobject_isNull1 ? null : (java.lang.String) scan_value3.toString();
/* 057 */       deserializetoobject_isNull1 = deserializetoobject_value1 == null;
/* 058 */       if (deserializetoobject_isNull1) {
/* 059 */         deserializetoobject_values[0] = null;
/* 060 */       } else {
/* 061 */         deserializetoobject_values[0] = deserializetoobject_value1;
/* 062 */       }
/* 063 */
/* 064 */       boolean deserializetoobject_isNull3 = scan_isNull4;
/* 065 */       final java.lang.String deserializetoobject_value3 = deserializetoobject_isNull3 ? null : (java.lang.String) scan_value4.toString();
/* 066 */       deserializetoobject_isNull3 = deserializetoobject_value3 == null;
/* 067 */       if (deserializetoobject_isNull3) {
/* 068 */         deserializetoobject_values[1] = null;
/* 069 */       } else {
/* 070 */         deserializetoobject_values[1] = deserializetoobject_value3;
/* 071 */       }
/* 072 */
/* 073 */       boolean deserializetoobject_isNull5 = scan_isNull5;
/* 074 */       final java.lang.String deserializetoobject_value5 = deserializetoobject_isNull5 ? null : (java.lang.String) scan_value5.toString();
/* 075 */       deserializetoobject_isNull5 = deserializetoobject_value5 == null;
/* 076 */       if (deserializetoobject_isNull5) {
/* 077 */         deserializetoobject_values[2] = null;
/* 078 */       } else {
/* 079 */         deserializetoobject_values[2] = deserializetoobject_value5;
/* 080 */       }
/* 081 */
/* 082 */       final org.apache.spark.sql.Row deserializetoobject_value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(deserializetoobject_values, deserializetoobject_schema);
/* 083 */
/* 084 */       Object mapelements_obj = ((Expression) references[2]).eval(null);
/* 085 */       org.apache.spark.api.java.function.MapFunction mapelements_value1 = (org.apache.spark.api.java.function.MapFunction) mapelements_obj;
/* 086 */
/* 087 */       boolean mapelements_isNull = false || false;
/* 088 */
/* 089 */       hx.stream.spark.SparkApplication$Person mapelements_value = null;
/* 090 */       try {
/* 091 */         mapelements_value = mapelements_isNull ? null : (hx.stream.spark.SparkApplication$Person) mapelements_value1.call(deserializetoobject_value);
/* 092 */       } catch (Exception e) {
/* 093 */         org.apache.spark.unsafe.Platform.throwException(e);
/* 094 */       }
/* 095 */
/* 096 */       mapelements_isNull = mapelements_value == null;
/* 097 */
/* 098 */       boolean serializefromobject_isNull = mapelements_isNull;
/* 099 */       final int serializefromobject_value = serializefromobject_isNull ? -1 : mapelements_value.getAge();
/* 100 */       boolean serializefromobject_isNull2 = mapelements_isNull;
/* 101 */       final int serializefromobject_value2 = serializefromobject_isNull2 ? -1 : mapelements_value.getId();
/* 102 */       boolean serializefromobject_isNull5 = mapelements_isNull;
/* 103 */       final java.lang.String serializefromobject_value5 = serializefromobject_isNull5 ? null : (java.lang.String) mapelements_value.getName();
/* 104 */       serializefromobject_isNull5 = serializefromobject_value5 == null;
/* 105 */       boolean serializefromobject_isNull4 = serializefromobject_isNull5;
/* 106 */       final UTF8String serializefromobject_value4 = serializefromobject_isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value5);
/* 107 */       serializefromobject_isNull4 = serializefromobject_value4 == null;
/* 108 */       serializefromobject_holder.reset();
/* 109 */
/* 110 */       serializefromobject_rowWriter.zeroOutNullBytes();
/* 111 */
/* 112 */       if (serializefromobject_isNull) {
/* 113 */         serializefromobject_rowWriter.setNullAt(0);
/* 114 */       } else {
/* 115 */         serializefromobject_rowWriter.write(0, serializefromobject_value);
/* 116 */       }
/* 117 */
/* 118 */       if (serializefromobject_isNull2) {
/* 119 */         serializefromobject_rowWriter.setNullAt(1);
/* 120 */       } else {
/* 121 */         serializefromobject_rowWriter.write(1, serializefromobject_value2);
/* 122 */       }
/* 123 */
/* 124 */       if (serializefromobject_isNull4) {
/* 125 */         serializefromobject_rowWriter.setNullAt(2);
/* 126 */       } else {
/* 127 */         serializefromobject_rowWriter.write(2, serializefromobject_value4);
/* 128 */       }
/* 129 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());
/* 130 */       append(serializefromobject_result);
/* 131 */       if (shouldStop()) return;
/* 132 */     }
/* 133 */   }
/* 134 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1273)
	at org.codehaus.janino.UnitCompiler.access$1500(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitWhileStatement(UnitCompiler.java:940)
	at org.codehaus.janino.Java$WhileStatement.accept(Java.java:2244)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:350)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2176)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2175)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2175)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2174)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2174)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:66)
2016-10-26 16:54:45 WARN  [main] o.a.s.s.e.WholeStageCodegenExec [Logging.scala:66] : Whole-stage codegen disabled for this plan:
 *SerializeFromObject [input[0, hx.stream.spark.SparkApplication$Person, true].getAge AS age#164, input[0, hx.stream.spark.SparkApplication$Person, true].getId AS id#165, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, hx.stream.spark.SparkApplication$Person, true].getName, true) AS name#166]
+- *MapElements hx.stream.spark.SparkApplication$$Lambda$9/2088388998@6d7a343f, obj#163: hx.stream.spark.SparkApplication$Person
   +- *DeserializeToObject createexternalrow(age#0.toString, id#1.toString, name#2.toString, StructField(age,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true)), obj#162: org.apache.spark.sql.Row
      +- *Scan json [age#0,id#1,name#2] Format: JSON, InputPaths: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resou..., PushedFilters: [], ReadSchema: struct<age:string,id:string,name:string>

2016-10-26 16:54:45 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:66
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (collectAsList at SparkApplication.java:66) with 1 output partitions
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (collectAsList at SparkApplication.java:66)
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:66), which has no missing parents
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 11.2 KB, free 912.1 MB)
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.9 KB, free 912.1 MB)
2016-10-26 16:54:45 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:54988 (size: 5.9 KB, free: 912.3 MB)
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:66)
2016-10-26 16:54:45 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 16:54:45 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5940 bytes)
2016-10-26 16:54:45 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 16:54:45 ERROR [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 16:54:45 ERROR [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 7.0 (TID 8)
java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 common frames omitted
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 common frames omitted
2016-10-26 16:54:46 WARN  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 7.0 (TID 8, localhost): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 more
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 more

2016-10-26 16:54:46 ERROR [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 7.0 failed 1 times; aborting job
2016-10-26 16:54:46 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 16:54:46 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 7
2016-10-26 16:54:46 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (collectAsList at SparkApplication.java:66) failed in 0.086 s
2016-10-26 16:54:46 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 failed: collectAsList at SparkApplication.java:66, took 0.098713 s
2016-10-26 16:54:46 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 16:54:46 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 16:54:46 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 16:54:46 INFO  [dispatcher-event-loop-1] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 16:54:46 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 16:54:46 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 16:54:46 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 16:54:46 INFO  [dispatcher-event-loop-1] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 16:54:46 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 16:54:46 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 16:54:46 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-8b7b8815-488c-4a0d-b190-030f817d4e16
2016-10-26 17:00:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 17:00:52 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 17:00:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 17:00:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 17:00:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 17:00:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 17:00:52 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 17:00:53 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 55027.
2016-10-26 17:00:53 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 17:00:53 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 17:00:53 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-04d48277-af0e-4a99-b070-4f9cb50dd279
2016-10-26 17:00:53 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 17:00:53 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 17:00:53 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1765ms
2016-10-26 17:00:53 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 17:00:53 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:00:53 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @1889ms
2016-10-26 17:00:53 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 17:00:53 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 17:00:53 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 17:00:53 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55028.
2016-10-26 17:00:53 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:55028
2016-10-26 17:00:53 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 55028)
2016-10-26 17:00:53 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:55028 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 55028)
2016-10-26 17:00:53 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 55028)
2016-10-26 17:00:53 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 17:00:53 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 17:00:55 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 17:00:55 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 17:00:55 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:55028 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:00:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:32
2016-10-26 17:00:55 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 17:00:55 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:32
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:32) with 2 output partitions
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:32)
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32), which has no missing parents
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 17:00:55 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:55028 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32)
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 17:00:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:00:55 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:00:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 17:00:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 17:00:55 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 17:00:55 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 17:00:55 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 17:00:55 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 17:00:55 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 17:00:55 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 17:00:55 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 17:00:55 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 17:00:55 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-10-26 17:00:55 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-10-26 17:00:55 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 212 ms on localhost (1/2)
2016-10-26 17:00:55 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 254 ms on localhost (2/2)
2016-10-26 17:00:55 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 17:00:55 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:32) finished in 0.269 s
2016-10-26 17:00:55 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:32, took 0.388886 s
2016-10-26 17:00:55 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:55028 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:00:56 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 17:00:56 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 17:00:56 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 17:00:56 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 17:00:56 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:55028 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:00:57 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 17:00:58 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 17:00:58 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 17:00:59 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 17:00:59 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/46a2c0a1-e266-4be7-9bf9-b334ff382c1b_resources
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/46a2c0a1-e266-4be7-9bf9-b334ff382c1b
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/46a2c0a1-e266-4be7-9bf9-b334ff382c1b
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/46a2c0a1-e266-4be7-9bf9-b334ff382c1b/_tmp_space.db
2016-10-26 17:00:59 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/9cfc8859-195f-4788-876d-f992a61a08dc_resources
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/9cfc8859-195f-4788-876d-f992a61a08dc
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/9cfc8859-195f-4788-876d-f992a61a08dc
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/9cfc8859-195f-4788-876d-f992a61a08dc/_tmp_space.db
2016-10-26 17:00:59 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:00:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 17:00:59 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 17:00:59 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:32)

2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:00 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:01:00 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:01:00 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:55028 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:00 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkApplication.java:35
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:00 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 219.571059 ms
2016-10-26 17:01:00 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:35
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkApplication.java:35) with 1 output partitions
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkApplication.java:35)
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35), which has no missing parents
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 17:01:00 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:55028 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35)
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 17:01:00 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:01:00 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 17:01:00 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:00 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 21.152466 ms
2016-10-26 17:01:00 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-10-26 17:01:00 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 78 ms on localhost (1/1)
2016-10-26 17:01:00 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkApplication.java:35) finished in 0.079 s
2016-10-26 17:01:00 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 17:01:00 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkApplication.java:35, took 0.111867 s
2016-10-26 17:01:00 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 34.603119 ms
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:00 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:01:00 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:01:00 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:55028 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:00 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:37
2016-10-26 17:01:00 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:37
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:37) with 1 output partitions
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:37)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37), which has no missing parents
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:55028 (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.74322 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1336 bytes result sent to driver
2016-10-26 17:01:01 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 29 ms on localhost (1/1)
2016-10-26 17:01:01 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:37) finished in 0.031 s
2016-10-26 17:01:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:37, took 0.041005 s
2016-10-26 17:01:01 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.46139 ms
2016-10-26 17:01:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-10-26 17:01:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 17:01:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:55028 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkApplication.java:38
2016-10-26 17:01:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkApplication.java:38
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkApplication.java:38)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkApplication.java:38) with 1 output partitions
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkApplication.java:38)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:55028 (size: 7.8 KB, free: 912.2 MB)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 23.643506 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 36.673485 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 7.718344 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.652526 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.390757 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-10-26 17:01:01 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 226 ms on localhost (1/1)
2016-10-26 17:01:01 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkApplication.java:38) finished in 0.227 s
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 911.8 MB)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.8 MB)
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:55028 (size: 8.0 KB, free: 912.2 MB)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38)
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 17:01:01 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 4 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 25.443777 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 23.042073 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.533706 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.87873 ms
2016-10-26 17:01:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-10-26 17:01:01 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 147 ms on localhost (1/1)
2016-10-26 17:01:01 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 17:01:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkApplication.java:38) finished in 0.148 s
2016-10-26 17:01:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkApplication.java:38, took 0.459625 s
2016-10-26 17:01:01 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 7.554046 ms
2016-10-26 17:01:01 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.243646 ms
2016-10-26 17:01:01 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.012582 ms
2016-10-26 17:01:01 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:01:02 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 17:01:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:01:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 133.4 KB, free 911.7 MB)
2016-10-26 17:01:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.7 MB)
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:55028 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:02 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkApplication.java:42
2016-10-26 17:01:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:02 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:42
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:42) with 1 output partitions
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:42)
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42), which has no missing parents
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 911.6 MB)
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.6 MB)
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:55028 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42)
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:55028 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:55028 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:02 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:55028 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:55028 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:55028 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:01:02 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 159
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 160
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 161
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 162
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 163
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned shuffle 0
2016-10-26 17:01:02 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:55028 in memory (size: 7.8 KB, free: 912.3 MB)
2016-10-26 17:01:02 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 19 ms on localhost (1/1)
2016-10-26 17:01:02 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 17:01:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:42) finished in 0.020 s
2016-10-26 17:01:02 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:42, took 0.058745 s
2016-10-26 17:01:02 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:55028 in memory (size: 8.0 KB, free: 912.3 MB)
2016-10-26 17:01:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 252
2016-10-26 17:01:02 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:01:02 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:02 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: drop_table : db=default tbl=person
2016-10-26 17:01:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=person	
2016-10-26 17:01:03 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] : deleting  file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:01:03 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2016-10-26 17:01:03 INFO  [main] o.a.hadoop.fs.TrashPolicyDefault [TrashPolicyDefault.java:92] : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2016-10-26 17:01:03 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] : Deleted the diretory file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_tables: db=default pat=*
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
2016-10-26 17:01:03 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.106352 ms
2016-10-26 17:01:03 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:03 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:03 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:01:03 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:03 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:01:04 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:55028 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from saveAsTable at SparkApplication.java:60
2016-10-26 17:01:04 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:04 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:01:04 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:01:04 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:60
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTable at SparkApplication.java:60) with 1 output partitions
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (saveAsTable at SparkApplication.java:60)
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60), which has no missing parents
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 54.2 KB, free 911.9 MB)
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.4 KB, free 911.9 MB)
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:55028 (size: 20.4 KB, free: 912.2 MB)
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60)
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 7)
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261701_0006_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261701_0006_m_000000
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261701_0006_m_000000_0: Committed
2016-10-26 17:01:04 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7). 1222 bytes result sent to driver
2016-10-26 17:01:04 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7) in 447 ms on localhost (1/1)
2016-10-26 17:01:04 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 17:01:04 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (saveAsTable at SparkApplication.java:60) finished in 0.449 s
2016-10-26 17:01:04 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTable at SparkApplication.java:60, took 0.475086 s
2016-10-26 17:01:04 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261701_0000 committed.
2016-10-26 17:01:04 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 17:01:04 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:04 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:04 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:04 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:04 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477472464, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 17:01:04 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477472464, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 17:01:04 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 17:01:04 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 17:01:04 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_12_piece0 on 172.16.106.190:55028 in memory (size: 20.4 KB, free: 912.3 MB)
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:55028 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:04 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 253
2016-10-26 17:01:04 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 254
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:55028 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:01:04 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_11_piece0 on 172.16.106.190:55028 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:04 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 300
2016-10-26 17:01:04 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 301
2016-10-26 17:01:05 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:05 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:05 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:01:05 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:05 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:01:05 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:01:05 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:55028 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:05 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from collectAsList at SparkApplication.java:66
2016-10-26 17:01:05 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:05 ERROR [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private Object[] deserializetoobject_values;
/* 010 */   private org.apache.spark.sql.types.StructType deserializetoobject_schema;
/* 011 */   private UnsafeRow deserializetoobject_result;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;
/* 014 */   private UnsafeRow mapelements_result;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;
/* 017 */   private UnsafeRow serializefromobject_result;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;
/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;
/* 020 */
/* 021 */   public GeneratedIterator(Object[] references) {
/* 022 */     this.references = references;
/* 023 */   }
/* 024 */
/* 025 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 026 */     partitionIndex = index;
/* 027 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 028 */     scan_input = inputs[0];
/* 029 */
/* 030 */     this.deserializetoobject_schema = (org.apache.spark.sql.types.StructType) references[1];
/* 031 */     deserializetoobject_result = new UnsafeRow(1);
/* 032 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);
/* 033 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);
/* 034 */     mapelements_result = new UnsafeRow(1);
/* 035 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);
/* 036 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);
/* 037 */     serializefromobject_result = new UnsafeRow(3);
/* 038 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);
/* 039 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 3);
/* 040 */   }
/* 041 */
/* 042 */   protected void processNext() throws java.io.IOException {
/* 043 */     while (scan_input.hasNext()) {
/* 044 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 045 */       scan_numOutputRows.add(1);
/* 046 */       boolean scan_isNull3 = scan_row.isNullAt(0);
/* 047 */       UTF8String scan_value3 = scan_isNull3 ? null : (scan_row.getUTF8String(0));
/* 048 */       boolean scan_isNull4 = scan_row.isNullAt(1);
/* 049 */       UTF8String scan_value4 = scan_isNull4 ? null : (scan_row.getUTF8String(1));
/* 050 */       boolean scan_isNull5 = scan_row.isNullAt(2);
/* 051 */       UTF8String scan_value5 = scan_isNull5 ? null : (scan_row.getUTF8String(2));
/* 052 */
/* 053 */       deserializetoobject_values = new Object[3];
/* 054 */
/* 055 */       boolean deserializetoobject_isNull1 = scan_isNull3;
/* 056 */       final java.lang.String deserializetoobject_value1 = deserializetoobject_isNull1 ? null : (java.lang.String) scan_value3.toString();
/* 057 */       deserializetoobject_isNull1 = deserializetoobject_value1 == null;
/* 058 */       if (deserializetoobject_isNull1) {
/* 059 */         deserializetoobject_values[0] = null;
/* 060 */       } else {
/* 061 */         deserializetoobject_values[0] = deserializetoobject_value1;
/* 062 */       }
/* 063 */
/* 064 */       boolean deserializetoobject_isNull3 = scan_isNull4;
/* 065 */       final java.lang.String deserializetoobject_value3 = deserializetoobject_isNull3 ? null : (java.lang.String) scan_value4.toString();
/* 066 */       deserializetoobject_isNull3 = deserializetoobject_value3 == null;
/* 067 */       if (deserializetoobject_isNull3) {
/* 068 */         deserializetoobject_values[1] = null;
/* 069 */       } else {
/* 070 */         deserializetoobject_values[1] = deserializetoobject_value3;
/* 071 */       }
/* 072 */
/* 073 */       boolean deserializetoobject_isNull5 = scan_isNull5;
/* 074 */       final java.lang.String deserializetoobject_value5 = deserializetoobject_isNull5 ? null : (java.lang.String) scan_value5.toString();
/* 075 */       deserializetoobject_isNull5 = deserializetoobject_value5 == null;
/* 076 */       if (deserializetoobject_isNull5) {
/* 077 */         deserializetoobject_values[2] = null;
/* 078 */       } else {
/* 079 */         deserializetoobject_values[2] = deserializetoobject_value5;
/* 080 */       }
/* 081 */
/* 082 */       final org.apache.spark.sql.Row deserializetoobject_value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(deserializetoobject_values, deserializetoobject_schema);
/* 083 */
/* 084 */       Object mapelements_obj = ((Expression) references[2]).eval(null);
/* 085 */       org.apache.spark.api.java.function.MapFunction mapelements_value1 = (org.apache.spark.api.java.function.MapFunction) mapelements_obj;
/* 086 */
/* 087 */       boolean mapelements_isNull = false || false;
/* 088 */
/* 089 */       hx.stream.spark.SparkApplication$Person mapelements_value = null;
/* 090 */       try {
/* 091 */         mapelements_value = mapelements_isNull ? null : (hx.stream.spark.SparkApplication$Person) mapelements_value1.call(deserializetoobject_value);
/* 092 */       } catch (Exception e) {
/* 093 */         org.apache.spark.unsafe.Platform.throwException(e);
/* 094 */       }
/* 095 */
/* 096 */       mapelements_isNull = mapelements_value == null;
/* 097 */
/* 098 */       boolean serializefromobject_isNull = mapelements_isNull;
/* 099 */       final int serializefromobject_value = serializefromobject_isNull ? -1 : mapelements_value.getAge();
/* 100 */       boolean serializefromobject_isNull2 = mapelements_isNull;
/* 101 */       final int serializefromobject_value2 = serializefromobject_isNull2 ? -1 : mapelements_value.getId();
/* 102 */       boolean serializefromobject_isNull5 = mapelements_isNull;
/* 103 */       final java.lang.String serializefromobject_value5 = serializefromobject_isNull5 ? null : (java.lang.String) mapelements_value.getName();
/* 104 */       serializefromobject_isNull5 = serializefromobject_value5 == null;
/* 105 */       boolean serializefromobject_isNull4 = serializefromobject_isNull5;
/* 106 */       final UTF8String serializefromobject_value4 = serializefromobject_isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value5);
/* 107 */       serializefromobject_isNull4 = serializefromobject_value4 == null;
/* 108 */       serializefromobject_holder.reset();
/* 109 */
/* 110 */       serializefromobject_rowWriter.zeroOutNullBytes();
/* 111 */
/* 112 */       if (serializefromobject_isNull) {
/* 113 */         serializefromobject_rowWriter.setNullAt(0);
/* 114 */       } else {
/* 115 */         serializefromobject_rowWriter.write(0, serializefromobject_value);
/* 116 */       }
/* 117 */
/* 118 */       if (serializefromobject_isNull2) {
/* 119 */         serializefromobject_rowWriter.setNullAt(1);
/* 120 */       } else {
/* 121 */         serializefromobject_rowWriter.write(1, serializefromobject_value2);
/* 122 */       }
/* 123 */
/* 124 */       if (serializefromobject_isNull4) {
/* 125 */         serializefromobject_rowWriter.setNullAt(2);
/* 126 */       } else {
/* 127 */         serializefromobject_rowWriter.write(2, serializefromobject_value4);
/* 128 */       }
/* 129 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());
/* 130 */       append(serializefromobject_result);
/* 131 */       if (shouldStop()) return;
/* 132 */     }
/* 133 */   }
/* 134 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1273)
	at org.codehaus.janino.UnitCompiler.access$1500(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitWhileStatement(UnitCompiler.java:940)
	at org.codehaus.janino.Java$WhileStatement.accept(Java.java:2244)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:350)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2176)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2175)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2175)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2174)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2174)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:66)
2016-10-26 17:01:05 WARN  [main] o.a.s.s.e.WholeStageCodegenExec [Logging.scala:66] : Whole-stage codegen disabled for this plan:
 *SerializeFromObject [input[0, hx.stream.spark.SparkApplication$Person, true].getAge AS age#164, input[0, hx.stream.spark.SparkApplication$Person, true].getId AS id#165, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, hx.stream.spark.SparkApplication$Person, true].getName, true) AS name#166]
+- *MapElements hx.stream.spark.SparkApplication$$Lambda$9/819799010@7d7888a5, obj#163: hx.stream.spark.SparkApplication$Person
   +- *DeserializeToObject createexternalrow(age#0.toString, id#1.toString, name#2.toString, StructField(age,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true)), obj#162: org.apache.spark.sql.Row
      +- *Scan json [age#0,id#1,name#2] Format: JSON, InputPaths: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resou..., PushedFilters: [], ReadSchema: struct<age:string,id:string,name:string>

2016-10-26 17:01:05 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:66
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (collectAsList at SparkApplication.java:66) with 1 output partitions
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (collectAsList at SparkApplication.java:66)
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:66), which has no missing parents
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 11.2 KB, free 912.1 MB)
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.9 KB, free 912.1 MB)
2016-10-26 17:01:05 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:55028 (size: 5.9 KB, free: 912.3 MB)
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:66)
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 17:01:05 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5940 bytes)
2016-10-26 17:01:05 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 17:01:05 ERROR [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 17:01:05 ERROR [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 7.0 (TID 8)
java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 common frames omitted
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 common frames omitted
2016-10-26 17:01:05 WARN  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 7.0 (TID 8, localhost): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 more
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 more

2016-10-26 17:01:05 ERROR [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 7.0 failed 1 times; aborting job
2016-10-26 17:01:05 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 7
2016-10-26 17:01:05 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (collectAsList at SparkApplication.java:66) failed in 0.069 s
2016-10-26 17:01:05 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 failed: collectAsList at SparkApplication.java:66, took 0.082929 s
2016-10-26 17:01:05 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 17:01:05 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@55787112{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:01:05 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 17:01:05 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 17:01:05 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 17:01:05 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 17:01:05 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 17:01:05 INFO  [dispatcher-event-loop-0] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 17:01:05 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 17:01:05 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 17:01:05 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-9e480fa2-6a60-4b95-a348-ed9d5f6c6d87
2016-10-26 17:01:49 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 17:01:49 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 17:01:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 17:01:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 17:01:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 17:01:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 17:01:49 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 17:01:50 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 55032.
2016-10-26 17:01:50 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 17:01:50 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 17:01:50 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-907c05c2-8cd6-400a-8999-baae3d8181c1
2016-10-26 17:01:50 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 17:01:50 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 17:01:50 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @1815ms
2016-10-26 17:01:50 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 17:01:50 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@7854fbb4{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:01:50 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @1943ms
2016-10-26 17:01:50 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 17:01:50 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 17:01:50 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 17:01:50 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55033.
2016-10-26 17:01:50 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:55033
2016-10-26 17:01:50 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 55033)
2016-10-26 17:01:50 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:55033 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 55033)
2016-10-26 17:01:50 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 55033)
2016-10-26 17:01:51 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 17:01:51 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 17:01:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 17:01:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 17:01:52 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:55033 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:01:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:32
2016-10-26 17:01:52 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 17:01:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:32
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:32) with 2 output partitions
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:32)
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32), which has no missing parents
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 17:01:52 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:55033 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32)
2016-10-26 17:01:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 17:01:52 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:01:52 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:01:52 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 17:01:52 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 17:01:53 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 17:01:53 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 17:01:53 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 17:01:53 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 17:01:53 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 17:01:53 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 17:01:53 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 17:01:53 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 17:01:53 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-10-26 17:01:53 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-10-26 17:01:53 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 209 ms on localhost (1/2)
2016-10-26 17:01:53 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 249 ms on localhost (2/2)
2016-10-26 17:01:53 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 17:01:53 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:32) finished in 0.263 s
2016-10-26 17:01:53 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:32, took 0.345704 s
2016-10-26 17:01:53 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 17:01:53 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 17:01:53 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 17:01:53 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 17:01:54 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:55033 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:01:54 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:55033 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:01:55 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 17:01:56 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 17:01:56 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 17:01:56 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/2b1a9663-0b66-4524-89d0-af934d7ff2ed_resources
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/2b1a9663-0b66-4524-89d0-af934d7ff2ed
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/2b1a9663-0b66-4524-89d0-af934d7ff2ed
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/2b1a9663-0b66-4524-89d0-af934d7ff2ed/_tmp_space.db
2016-10-26 17:01:56 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/cc162de4-5b01-4593-9960-0873bcf7d3cf_resources
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/cc162de4-5b01-4593-9960-0873bcf7d3cf
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/cc162de4-5b01-4593-9960-0873bcf7d3cf
2016-10-26 17:01:56 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/cc162de4-5b01-4593-9960-0873bcf7d3cf/_tmp_space.db
2016-10-26 17:01:56 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:01:57 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 17:01:57 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 17:01:57 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:32)

2016-10-26 17:01:57 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:57 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:57 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:01:57 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:57 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:01:57 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:01:57 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:55033 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:57 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkApplication.java:35
2016-10-26 17:01:57 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:58 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 254.637074 ms
2016-10-26 17:01:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:35
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkApplication.java:35) with 1 output partitions
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkApplication.java:35)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35), which has no missing parents
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:55033 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 23.994506 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-10-26 17:01:58 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 81 ms on localhost (1/1)
2016-10-26 17:01:58 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkApplication.java:35) finished in 0.082 s
2016-10-26 17:01:58 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkApplication.java:35, took 0.114019 s
2016-10-26 17:01:58 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 21.339761 ms
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:01:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:55033 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:37
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:37
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:37) with 1 output partitions
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:37)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37), which has no missing parents
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:55033 (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.86024 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1336 bytes result sent to driver
2016-10-26 17:01:58 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 35 ms on localhost (1/1)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:37) finished in 0.036 s
2016-10-26 17:01:58 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 17:01:58 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:37, took 0.045483 s
2016-10-26 17:01:58 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.508163 ms
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 17:01:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:55033 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkApplication.java:38
2016-10-26 17:01:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkApplication.java:38
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkApplication.java:38)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkApplication.java:38) with 1 output partitions
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkApplication.java:38)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:55033 (size: 7.8 KB, free: 912.2 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 31.492373 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 37.679811 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.121601 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.435054 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.411069 ms
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-10-26 17:01:58 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 252 ms on localhost (1/1)
2016-10-26 17:01:58 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkApplication.java:38) finished in 0.253 s
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 911.8 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.8 MB)
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:55033 (size: 8.0 KB, free: 912.2 MB)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38)
2016-10-26 17:01:58 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 17:01:58 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-10-26 17:01:58 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 10 ms
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.91214 ms
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 19.362423 ms
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.976843 ms
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 14.928364 ms
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-10-26 17:01:59 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 164 ms on localhost (1/1)
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkApplication.java:38) finished in 0.166 s
2016-10-26 17:01:59 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 17:01:59 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkApplication.java:38, took 0.517599 s
2016-10-26 17:01:59 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.840229 ms
2016-10-26 17:01:59 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 12.614624 ms
2016-10-26 17:01:59 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.392394 ms
2016-10-26 17:01:59 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:01:59 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 17:01:59 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:01:59 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:01:59 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:01:59 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:01:59 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 133.4 KB, free 911.7 MB)
2016-10-26 17:01:59 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.7 MB)
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:55033 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:59 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkApplication.java:42
2016-10-26 17:01:59 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:01:59 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:42
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:42) with 1 output partitions
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:42)
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42), which has no missing parents
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 911.6 MB)
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.6 MB)
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:55033 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42)
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 163
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:55033 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:55033 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:55033 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:55033 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:55033 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:01:59 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 159
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 160
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 161
2016-10-26 17:01:59 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 16 ms on localhost (1/1)
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 162
2016-10-26 17:01:59 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 17:01:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:42) finished in 0.017 s
2016-10-26 17:01:59 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:42, took 0.048585 s
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned shuffle 0
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:55033 in memory (size: 7.8 KB, free: 912.3 MB)
2016-10-26 17:01:59 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:01:59 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:55033 in memory (size: 8.0 KB, free: 912.3 MB)
2016-10-26 17:01:59 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 252
2016-10-26 17:01:59 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 17:01:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:59 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:01:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:01:59 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:01:59 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:01:59 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:00 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: drop_table : db=default tbl=person
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=person	
2016-10-26 17:02:00 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] : deleting  file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:02:00 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2016-10-26 17:02:00 INFO  [main] o.a.hadoop.fs.TrashPolicyDefault [TrashPolicyDefault.java:92] : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2016-10-26 17:02:00 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] : Deleted the diretory file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_tables: db=default pat=*
2016-10-26 17:02:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
2016-10-26 17:02:01 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.88358 ms
2016-10-26 17:02:01 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:02:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:02:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:02:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:02:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:02:01 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:02:01 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:55033 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:02:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from saveAsTable at SparkApplication.java:60
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:02:01 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:60
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTable at SparkApplication.java:60) with 1 output partitions
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (saveAsTable at SparkApplication.java:60)
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60), which has no missing parents
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 54.2 KB, free 911.9 MB)
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.4 KB, free 911.9 MB)
2016-10-26 17:02:01 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:55033 (size: 20.4 KB, free: 912.2 MB)
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60)
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 17:02:01 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 7)
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261702_0006_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261702_0006_m_000000
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261702_0006_m_000000_0: Committed
2016-10-26 17:02:01 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7). 1309 bytes result sent to driver
2016-10-26 17:02:01 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7) in 431 ms on localhost (1/1)
2016-10-26 17:02:01 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 17:02:01 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (saveAsTable at SparkApplication.java:60) finished in 0.432 s
2016-10-26 17:02:01 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTable at SparkApplication.java:60, took 0.455000 s
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261702_0000 committed.
2016-10-26 17:02:01 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 17:02:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:01 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:02:01 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_12_piece0 on 172.16.106.190:55033 in memory (size: 20.4 KB, free: 912.3 MB)
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:55033 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:02:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 253
2016-10-26 17:02:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 254
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:55033 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_11_piece0 on 172.16.106.190:55033 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:02:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 300
2016-10-26 17:02:02 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 301
2016-10-26 17:02:02 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477472521, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 17:02:02 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477472521, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 17:02:02 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 17:02:02 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 17:02:02 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 17:02:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:02:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:02:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:02:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:02:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:02:02 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:55033 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:02:02 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from collectAsList at SparkApplication.java:66
2016-10-26 17:02:02 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:02:02 ERROR [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 006 */   private Object[] references;
/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric scan_numOutputRows;
/* 008 */   private scala.collection.Iterator scan_input;
/* 009 */   private Object[] deserializetoobject_values;
/* 010 */   private org.apache.spark.sql.types.StructType deserializetoobject_schema;
/* 011 */   private UnsafeRow deserializetoobject_result;
/* 012 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;
/* 013 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;
/* 014 */   private UnsafeRow mapelements_result;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;
/* 016 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;
/* 017 */   private UnsafeRow serializefromobject_result;
/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;
/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;
/* 020 */
/* 021 */   public GeneratedIterator(Object[] references) {
/* 022 */     this.references = references;
/* 023 */   }
/* 024 */
/* 025 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 026 */     partitionIndex = index;
/* 027 */     this.scan_numOutputRows = (org.apache.spark.sql.execution.metric.SQLMetric) references[0];
/* 028 */     scan_input = inputs[0];
/* 029 */
/* 030 */     this.deserializetoobject_schema = (org.apache.spark.sql.types.StructType) references[1];
/* 031 */     deserializetoobject_result = new UnsafeRow(1);
/* 032 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);
/* 033 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);
/* 034 */     mapelements_result = new UnsafeRow(1);
/* 035 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);
/* 036 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);
/* 037 */     serializefromobject_result = new UnsafeRow(3);
/* 038 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);
/* 039 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 3);
/* 040 */   }
/* 041 */
/* 042 */   protected void processNext() throws java.io.IOException {
/* 043 */     while (scan_input.hasNext()) {
/* 044 */       InternalRow scan_row = (InternalRow) scan_input.next();
/* 045 */       scan_numOutputRows.add(1);
/* 046 */       boolean scan_isNull3 = scan_row.isNullAt(0);
/* 047 */       UTF8String scan_value3 = scan_isNull3 ? null : (scan_row.getUTF8String(0));
/* 048 */       boolean scan_isNull4 = scan_row.isNullAt(1);
/* 049 */       UTF8String scan_value4 = scan_isNull4 ? null : (scan_row.getUTF8String(1));
/* 050 */       boolean scan_isNull5 = scan_row.isNullAt(2);
/* 051 */       UTF8String scan_value5 = scan_isNull5 ? null : (scan_row.getUTF8String(2));
/* 052 */
/* 053 */       deserializetoobject_values = new Object[3];
/* 054 */
/* 055 */       boolean deserializetoobject_isNull1 = scan_isNull3;
/* 056 */       final java.lang.String deserializetoobject_value1 = deserializetoobject_isNull1 ? null : (java.lang.String) scan_value3.toString();
/* 057 */       deserializetoobject_isNull1 = deserializetoobject_value1 == null;
/* 058 */       if (deserializetoobject_isNull1) {
/* 059 */         deserializetoobject_values[0] = null;
/* 060 */       } else {
/* 061 */         deserializetoobject_values[0] = deserializetoobject_value1;
/* 062 */       }
/* 063 */
/* 064 */       boolean deserializetoobject_isNull3 = scan_isNull4;
/* 065 */       final java.lang.String deserializetoobject_value3 = deserializetoobject_isNull3 ? null : (java.lang.String) scan_value4.toString();
/* 066 */       deserializetoobject_isNull3 = deserializetoobject_value3 == null;
/* 067 */       if (deserializetoobject_isNull3) {
/* 068 */         deserializetoobject_values[1] = null;
/* 069 */       } else {
/* 070 */         deserializetoobject_values[1] = deserializetoobject_value3;
/* 071 */       }
/* 072 */
/* 073 */       boolean deserializetoobject_isNull5 = scan_isNull5;
/* 074 */       final java.lang.String deserializetoobject_value5 = deserializetoobject_isNull5 ? null : (java.lang.String) scan_value5.toString();
/* 075 */       deserializetoobject_isNull5 = deserializetoobject_value5 == null;
/* 076 */       if (deserializetoobject_isNull5) {
/* 077 */         deserializetoobject_values[2] = null;
/* 078 */       } else {
/* 079 */         deserializetoobject_values[2] = deserializetoobject_value5;
/* 080 */       }
/* 081 */
/* 082 */       final org.apache.spark.sql.Row deserializetoobject_value = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(deserializetoobject_values, deserializetoobject_schema);
/* 083 */
/* 084 */       Object mapelements_obj = ((Expression) references[2]).eval(null);
/* 085 */       org.apache.spark.api.java.function.MapFunction mapelements_value1 = (org.apache.spark.api.java.function.MapFunction) mapelements_obj;
/* 086 */
/* 087 */       boolean mapelements_isNull = false || false;
/* 088 */
/* 089 */       hx.stream.spark.SparkApplication$Person mapelements_value = null;
/* 090 */       try {
/* 091 */         mapelements_value = mapelements_isNull ? null : (hx.stream.spark.SparkApplication$Person) mapelements_value1.call(deserializetoobject_value);
/* 092 */       } catch (Exception e) {
/* 093 */         org.apache.spark.unsafe.Platform.throwException(e);
/* 094 */       }
/* 095 */
/* 096 */       mapelements_isNull = mapelements_value == null;
/* 097 */
/* 098 */       boolean serializefromobject_isNull = mapelements_isNull;
/* 099 */       final int serializefromobject_value = serializefromobject_isNull ? -1 : mapelements_value.getAge();
/* 100 */       boolean serializefromobject_isNull2 = mapelements_isNull;
/* 101 */       final int serializefromobject_value2 = serializefromobject_isNull2 ? -1 : mapelements_value.getId();
/* 102 */       boolean serializefromobject_isNull5 = mapelements_isNull;
/* 103 */       final java.lang.String serializefromobject_value5 = serializefromobject_isNull5 ? null : (java.lang.String) mapelements_value.getName();
/* 104 */       serializefromobject_isNull5 = serializefromobject_value5 == null;
/* 105 */       boolean serializefromobject_isNull4 = serializefromobject_isNull5;
/* 106 */       final UTF8String serializefromobject_value4 = serializefromobject_isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(serializefromobject_value5);
/* 107 */       serializefromobject_isNull4 = serializefromobject_value4 == null;
/* 108 */       serializefromobject_holder.reset();
/* 109 */
/* 110 */       serializefromobject_rowWriter.zeroOutNullBytes();
/* 111 */
/* 112 */       if (serializefromobject_isNull) {
/* 113 */         serializefromobject_rowWriter.setNullAt(0);
/* 114 */       } else {
/* 115 */         serializefromobject_rowWriter.write(0, serializefromobject_value);
/* 116 */       }
/* 117 */
/* 118 */       if (serializefromobject_isNull2) {
/* 119 */         serializefromobject_rowWriter.setNullAt(1);
/* 120 */       } else {
/* 121 */         serializefromobject_rowWriter.write(1, serializefromobject_value2);
/* 122 */       }
/* 123 */
/* 124 */       if (serializefromobject_isNull4) {
/* 125 */         serializefromobject_rowWriter.setNullAt(2);
/* 126 */       } else {
/* 127 */         serializefromobject_rowWriter.write(2, serializefromobject_value4);
/* 128 */       }
/* 129 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());
/* 130 */       append(serializefromobject_result);
/* 131 */       if (shouldStop()) return;
/* 132 */     }
/* 133 */   }
/* 134 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 99, Column 98: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1273)
	at org.codehaus.janino.UnitCompiler.access$1500(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitWhileStatement(UnitCompiler.java:940)
	at org.codehaus.janino.Java$WhileStatement.accept(Java.java:2244)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:350)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2176)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1$$anonfun$apply$14.apply(Dataset.scala:2175)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2175)
	at org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2174)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2545)
	at org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2174)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:66)
2016-10-26 17:02:02 WARN  [main] o.a.s.s.e.WholeStageCodegenExec [Logging.scala:66] : Whole-stage codegen disabled for this plan:
 *SerializeFromObject [input[0, hx.stream.spark.SparkApplication$Person, true].getAge AS age#164, input[0, hx.stream.spark.SparkApplication$Person, true].getId AS id#165, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, hx.stream.spark.SparkApplication$Person, true].getName, true) AS name#166]
+- *MapElements hx.stream.spark.SparkApplication$$Lambda$9/1172289188@5b697993, obj#163: hx.stream.spark.SparkApplication$Person
   +- *DeserializeToObject createexternalrow(age#0.toString, id#1.toString, name#2.toString, StructField(age,StringType,true), StructField(id,StringType,true), StructField(name,StringType,true)), obj#162: org.apache.spark.sql.Row
      +- *Scan json [age#0,id#1,name#2] Format: JSON, InputPaths: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resou..., PushedFilters: [], ReadSchema: struct<age:string,id:string,name:string>

2016-10-26 17:02:02 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:66
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (collectAsList at SparkApplication.java:66) with 1 output partitions
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (collectAsList at SparkApplication.java:66)
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:66), which has no missing parents
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 11.2 KB, free 912.1 MB)
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.9 KB, free 912.1 MB)
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:55033 (size: 5.9 KB, free: 912.3 MB)
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at collectAsList at SparkApplication.java:66)
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5940 bytes)
2016-10-26 17:02:02 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 17:02:02 ERROR [Executor task launch worker-0] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:91] : failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 17:02:02 ERROR [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 7.0 (TID 8)
java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 common frames omitted
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 common frames omitted
2016-10-26 17:02:02 WARN  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 7.0 (TID 8, localhost): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at org.spark_project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at org.spark_project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at org.spark_project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at org.spark_project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:837)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:397)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:356)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:821)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:125)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:124)
	at org.apache.spark.sql.execution.SerializeFromObjectExec$$anonfun$5.apply(objects.scala:123)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private UnsafeRow result;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder holder;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter rowWriter;
/* 011 */
/* 012 */
/* 013 */   public SpecificUnsafeProjection(Object[] references) {
/* 014 */     this.references = references;
/* 015 */     result = new UnsafeRow(3);
/* 016 */     this.holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(result, 32);
/* 017 */     this.rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(holder, 3);
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     holder.reset();
/* 027 */
/* 028 */     rowWriter.zeroOutNullBytes();
/* 029 */
/* 030 */
/* 031 */     boolean isNull1 = i.isNullAt(0);
/* 032 */     hx.stream.spark.SparkApplication$Person value1 = isNull1 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 033 */
/* 034 */     boolean isNull = isNull1;
/* 035 */     final int value = isNull ? -1 : value1.getAge();
/* 036 */     if (isNull) {
/* 037 */       rowWriter.setNullAt(0);
/* 038 */     } else {
/* 039 */       rowWriter.write(0, value);
/* 040 */     }
/* 041 */
/* 042 */
/* 043 */     boolean isNull3 = i.isNullAt(0);
/* 044 */     hx.stream.spark.SparkApplication$Person value3 = isNull3 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 045 */
/* 046 */     boolean isNull2 = isNull3;
/* 047 */     final int value2 = isNull2 ? -1 : value3.getId();
/* 048 */     if (isNull2) {
/* 049 */       rowWriter.setNullAt(1);
/* 050 */     } else {
/* 051 */       rowWriter.write(1, value2);
/* 052 */     }
/* 053 */
/* 054 */
/* 055 */     boolean isNull6 = i.isNullAt(0);
/* 056 */     hx.stream.spark.SparkApplication$Person value6 = isNull6 ? null : ((hx.stream.spark.SparkApplication$Person)i.get(0, null));
/* 057 */
/* 058 */     boolean isNull5 = isNull6;
/* 059 */     final java.lang.String value5 = isNull5 ? null : (java.lang.String) value6.getName();
/* 060 */     isNull5 = value5 == null;
/* 061 */     boolean isNull4 = isNull5;
/* 062 */     final UTF8String value4 = isNull4 ? null : org.apache.spark.unsafe.types.UTF8String.fromString(value5);
/* 063 */     isNull4 = value4 == null;
/* 064 */     if (isNull4) {
/* 065 */       rowWriter.setNullAt(2);
/* 066 */     } else {
/* 067 */       rowWriter.write(2, value4);
/* 068 */     }
/* 069 */     result.setTotalSize(holder.totalSize());
/* 070 */     return result;
/* 071 */   }
/* 072 */ }

	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:889)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:941)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:938)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	... 27 more
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 35, Column 47: No applicable constructor/method found for zero actual parameters; candidates are: "public int hx.stream.spark.SparkApplication$Person.getAge()"
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:7559)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7429)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:7333)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3873)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3571)
	at org.codehaus.janino.UnitCompiler.access$6600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitConditionalExpression(UnitCompiler.java:3260)
	at org.codehaus.janino.Java$ConditionalExpression.accept(Java.java:3441)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1845)
	at org.codehaus.janino.UnitCompiler.access$2000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitLocalVariableDeclarationStatement(UnitCompiler.java:945)
	at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:2508)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:883)
	... 31 more

2016-10-26 17:02:02 ERROR [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 7.0 failed 1 times; aborting job
2016-10-26 17:02:02 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 7
2016-10-26 17:02:02 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (collectAsList at SparkApplication.java:66) failed in 0.087 s
2016-10-26 17:02:02 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 failed: collectAsList at SparkApplication.java:66, took 0.100212 s
2016-10-26 17:02:02 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 17:02:02 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@7854fbb4{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:02:02 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-2] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 17:02:02 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 17:02:02 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 17:02:02 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 17:02:02 INFO  [dispatcher-event-loop-2] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 17:02:02 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 17:02:02 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 17:02:02 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-f1eb6f30-9c52-4f96-8415-69f03c855af1
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 17:04:06 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 17:04:06 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 55051.
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 17:04:06 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-941c31ef-49da-4a37-b309-8394a4852ba1
2016-10-26 17:04:06 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 17:04:06 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 17:04:07 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2539ms
2016-10-26 17:04:07 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 17:04:07 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@2459319c{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:04:07 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2688ms
2016-10-26 17:04:07 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 17:04:07 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 17:04:07 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 17:04:07 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55052.
2016-10-26 17:04:07 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:55052
2016-10-26 17:04:07 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 55052)
2016-10-26 17:04:07 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:55052 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 55052)
2016-10-26 17:04:07 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 55052)
2016-10-26 17:04:07 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 17:04:07 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 17:04:09 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 17:04:09 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 17:04:09 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:55052 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:04:09 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:32
2016-10-26 17:04:09 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 17:04:09 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:32
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:32) with 2 output partitions
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:32)
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32), which has no missing parents
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 17:04:09 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:55052 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:32)
2016-10-26 17:04:09 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 17:04:09 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:04:09 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:04:09 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 17:04:09 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 17:04:10 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 17:04:10 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 17:04:10 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 17:04:10 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 17:04:10 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 17:04:10 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 17:04:10 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 17:04:10 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-10-26 17:04:10 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-10-26 17:04:10 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 233 ms on localhost (1/2)
2016-10-26 17:04:10 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 321 ms on localhost (2/2)
2016-10-26 17:04:10 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 17:04:10 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:32) finished in 0.344 s
2016-10-26 17:04:10 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:32, took 0.431691 s
2016-10-26 17:04:10 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 17:04:10 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 17:04:10 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 17:04:10 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 17:04:11 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:55052 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:04:11 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:55052 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:04:12 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 17:04:13 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 17:04:13 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 17:04:14 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 17:04:14 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/3c3ae0c9-5edc-4f8b-b62a-433aa267c02a_resources
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/3c3ae0c9-5edc-4f8b-b62a-433aa267c02a
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/3c3ae0c9-5edc-4f8b-b62a-433aa267c02a
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/3c3ae0c9-5edc-4f8b-b62a-433aa267c02a/_tmp_space.db
2016-10-26 17:04:14 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/0cd5ee05-0953-464d-8d3d-f78403b688d0_resources
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/0cd5ee05-0953-464d-8d3d-f78403b688d0
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/0cd5ee05-0953-464d-8d3d-f78403b688d0
2016-10-26 17:04:14 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/0cd5ee05-0953-464d-8d3d-f78403b688d0/_tmp_space.db
2016-10-26 17:04:14 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:04:15 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 17:04:15 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 17:04:15 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:32)

2016-10-26 17:04:15 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:04:15 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:04:15 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:04:15 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:04:15 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:04:15 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:04:15 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:55052 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:04:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkApplication.java:35
2016-10-26 17:04:15 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:04:15 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 207.45367 ms
2016-10-26 17:04:15 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:35
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkApplication.java:35) with 1 output partitions
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkApplication.java:35)
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35), which has no missing parents
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 17:04:15 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:55052 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:35)
2016-10-26 17:04:15 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 17:04:15 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:04:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 17:04:15 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:04:15 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.750938 ms
2016-10-26 17:04:15 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-10-26 17:04:16 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 73 ms on localhost (1/1)
2016-10-26 17:04:16 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 17:04:16 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkApplication.java:35) finished in 0.074 s
2016-10-26 17:04:16 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkApplication.java:35, took 0.099708 s
2016-10-26 17:04:16 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.696892 ms
2016-10-26 17:05:12 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:05:12 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:05:12 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 17:05:12 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:05:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:05:12 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:05:12 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:55052 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:05:12 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:37
2016-10-26 17:05:12 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:05:13 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:37
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:37) with 1 output partitions
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:37)
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37), which has no missing parents
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-10-26 17:05:13 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:55052 (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:37)
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 17:05:13 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:05:13 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 17:05:13 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:05:13 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.084379 ms
2016-10-26 17:05:13 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1336 bytes result sent to driver
2016-10-26 17:05:13 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 23 ms on localhost (1/1)
2016-10-26 17:05:13 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:37) finished in 0.023 s
2016-10-26 17:05:13 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 17:05:13 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:37, took 0.032243 s
2016-10-26 17:05:13 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 104.0999 ms
2016-10-26 17:06:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:06:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:06:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-10-26 17:06:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:06:30 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 17:06:30 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 17:06:30 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:55052 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:06:30 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkApplication.java:38
2016-10-26 17:06:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:06:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkApplication.java:38
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkApplication.java:38)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkApplication.java:38) with 1 output partitions
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkApplication.java:38)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-10-26 17:06:31 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:55052 (size: 7.8 KB, free: 912.2 MB)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:38)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 17:06:31 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.126395 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 40.126623 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.796782 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 17.799175 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.738425 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-10-26 17:06:31 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 278 ms on localhost (1/1)
2016-10-26 17:06:31 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkApplication.java:38) finished in 0.280 s
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38), which has no missing parents
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 911.8 MB)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.8 MB)
2016-10-26 17:06:31 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:55052 (size: 8.0 KB, free: 912.2 MB)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:38)
2016-10-26 17:06:31 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 17:06:31 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 10 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 29.923982 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 23.111247 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 19.982302 ms
2016-10-26 17:06:31 INFO  [Executor task launch worker-2] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.346531 ms
2016-10-26 17:06:32 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-10-26 17:06:32 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 193 ms on localhost (1/1)
2016-10-26 17:06:32 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 17:06:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkApplication.java:38) finished in 0.193 s
2016-10-26 17:06:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkApplication.java:38, took 0.592144 s
2016-10-26 17:06:32 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 185.302605 ms
2016-10-26 17:06:32 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 152.58298 ms
2016-10-26 17:06:33 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 110.146261 ms
2016-10-26 17:06:38 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:06:51 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 17:06:52 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:06:52 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:06:52 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:06:52 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:06:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 133.4 KB, free 911.7 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:55052 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:55052 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 17:06:52 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.7 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:55052 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:55052 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:55052 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 159
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 160
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 161
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 162
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 163
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned shuffle 0
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:55052 in memory (size: 7.8 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:55052 in memory (size: 8.0 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 252
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:55052 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-10-26 17:06:52 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-10-26 17:06:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkApplication.java:42
2016-10-26 17:06:52 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:06:52 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:42
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:42) with 1 output partitions
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:42)
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42), which has no missing parents
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:55052 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:42)
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 17:06:52 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:06:52 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-10-26 17:06:52 INFO  [Executor task launch worker-2] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:06:52 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 17:06:52 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 12 ms on localhost (1/1)
2016-10-26 17:06:52 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 17:06:52 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:42) finished in 0.013 s
2016-10-26 17:06:52 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:42, took 0.021560 s
2016-10-26 17:07:23 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:07:30 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 17:07:30 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:07:30 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:07:33 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:07:33 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:07:33 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:07:33 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:07:33 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 17:07:33 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:07:33 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:07:33 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:07:33 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:07:33 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:07:33 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:07:33 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:07:33 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:07:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:07:34 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:07:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:07:34 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:07:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: drop_table : db=default tbl=person
2016-10-26 17:07:34 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=person	
2016-10-26 17:07:41 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] : deleting  file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:07:41 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2016-10-26 17:07:41 INFO  [main] o.a.hadoop.fs.TrashPolicyDefault [TrashPolicyDefault.java:92] : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2016-10-26 17:07:41 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] : Deleted the diretory file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:10:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_tables: db=default pat=*
2016-10-26 17:10:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
2016-10-26 17:10:11 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 118.995904 ms
2016-10-26 17:10:17 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:10:17 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:10:17 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:10:17 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:17 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:17 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:17 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:17 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:10:17 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:10:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:10:18 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:10:18 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:55052 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:10:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from saveAsTable at SparkApplication.java:60
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:10:18 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:10:18 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:60
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTable at SparkApplication.java:60) with 1 output partitions
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (saveAsTable at SparkApplication.java:60)
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60), which has no missing parents
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 54.2 KB, free 911.9 MB)
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.4 KB, free 911.9 MB)
2016-10-26 17:10:18 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:55052 (size: 20.4 KB, free: 912.2 MB)
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:60)
2016-10-26 17:10:18 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 17:10:18 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 17:10:18 INFO  [Executor task launch worker-3] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 7)
2016-10-26 17:10:18 INFO  [Executor task launch worker-3] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 17:10:18 INFO  [Executor task launch worker-3] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 17:10:18 INFO  [Executor task launch worker-3] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 17:10:18 INFO  [Executor task launch worker-3] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 17:10:18 INFO  [Executor task launch worker-3] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:10:19 INFO  [Executor task launch worker-3] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 17:10:19 INFO  [Executor task launch worker-3] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 17:10:19 INFO  [Executor task launch worker-3] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:10:19 INFO  [Executor task launch worker-3] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261710_0006_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261710_0006_m_000000
2016-10-26 17:10:19 INFO  [Executor task launch worker-3] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261710_0006_m_000000_0: Committed
2016-10-26 17:10:19 INFO  [Executor task launch worker-3] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7). 1309 bytes result sent to driver
2016-10-26 17:10:19 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7) in 562 ms on localhost (1/1)
2016-10-26 17:10:19 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 17:10:19 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (saveAsTable at SparkApplication.java:60) finished in 0.563 s
2016-10-26 17:10:19 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTable at SparkApplication.java:60, took 0.588616 s
2016-10-26 17:10:19 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261710_0000 committed.
2016-10-26 17:10:19 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 17:10:19 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:19 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:19 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:10:19 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:10:20 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477473019, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 17:10:20 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477473019, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 17:10:20 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 17:10:20 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 17:10:20 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 17:10:20 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:55052 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:10:20 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 253
2016-10-26 17:10:20 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 254
2016-10-26 17:10:20 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:55052 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:10:20 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_11_piece0 on 172.16.106.190:55052 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:10:20 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 300
2016-10-26 17:10:20 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 301
2016-10-26 17:10:20 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_12_piece0 on 172.16.106.190:55052 in memory (size: 20.4 KB, free: 912.3 MB)
2016-10-26 17:11:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:11:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:11:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:11:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:11:31 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:11:31 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:11:31 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:55052 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:11:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from collectAsList at SparkApplication.java:66
2016-10-26 17:11:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:11:32 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 216.59187 ms
2016-10-26 17:11:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:66
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (collectAsList at SparkApplication.java:66) with 1 output partitions
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (collectAsList at SparkApplication.java:66)
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[27] at collectAsList at SparkApplication.java:66), which has no missing parents
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 13.8 KB, free 912.1 MB)
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.1 KB, free 912.1 MB)
2016-10-26 17:11:32 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:55052 (size: 6.1 KB, free: 912.3 MB)
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at collectAsList at SparkApplication.java:66)
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 17:11:32 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5940 bytes)
2016-10-26 17:11:32 INFO  [Executor task launch worker-4] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 17:11:32 INFO  [Executor task launch worker-4] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:11:32 ERROR [Executor task launch worker-4] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 7.0 (TID 8)
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at org.apache.spark.sql.Row$class.getInt(Row.scala:217)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(rows.scala:192)
	at hx.stream.spark.SparkApplication.lambda$1(SparkApplication.java:64)
	at hx.stream.spark.SparkApplication$$Lambda$10/1126588281.call(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 17:11:32 WARN  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 7.0 (TID 8, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at org.apache.spark.sql.Row$class.getInt(Row.scala:217)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(rows.scala:192)
	at hx.stream.spark.SparkApplication.lambda$1(SparkApplication.java:64)
	at hx.stream.spark.SparkApplication$$Lambda$10/1126588281.call(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2016-10-26 17:11:32 ERROR [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 7.0 failed 1 times; aborting job
2016-10-26 17:11:32 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 7
2016-10-26 17:11:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (collectAsList at SparkApplication.java:66) failed in 0.098 s
2016-10-26 17:11:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 failed: collectAsList at SparkApplication.java:66, took 0.113450 s
2016-10-26 17:12:23 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Running Spark version 2.0.0
2016-10-26 17:12:23 WARN  [main] o.a.hadoop.util.NativeCodeLoader [NativeCodeLoader.java:62] : Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2016-10-26 17:12:23 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls to: Benchun
2016-10-26 17:12:23 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls to: Benchun
2016-10-26 17:12:23 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing view acls groups to: 
2016-10-26 17:12:23 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : Changing modify acls groups to: 
2016-10-26 17:12:23 INFO  [main] org.apache.spark.SecurityManager [Logging.scala:54] : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Benchun); groups with view permissions: Set(); users  with modify permissions: Set(Benchun); groups with modify permissions: Set()
2016-10-26 17:12:24 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'sparkDriver' on port 55105.
2016-10-26 17:12:24 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering MapOutputTracker
2016-10-26 17:12:24 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering BlockManagerMaster
2016-10-26 17:12:24 INFO  [main] o.a.spark.storage.DiskBlockManager [Logging.scala:54] : Created local directory at /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/blockmgr-354c1bbf-5051-44d2-9e94-9eee1de1cf47
2016-10-26 17:12:24 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore started with capacity 912.3 MB
2016-10-26 17:12:24 INFO  [main] org.apache.spark.SparkEnv [Logging.scala:54] : Registering OutputCommitCoordinator
2016-10-26 17:12:24 INFO  [main] org.spark_project.jetty.util.log [Log.java:186] : Logging initialized @2150ms
2016-10-26 17:12:24 INFO  [main] o.spark_project.jetty.server.Server [Server.java:327] : jetty-9.2.z-SNAPSHOT
2016-10-26 17:12:24 INFO  [main] o.s.jetty.server.ServerConnector [AbstractConnector.java:266] : Started ServerConnector@2459319c{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:12:24 INFO  [main] o.spark_project.jetty.server.Server [Server.java:379] : Started @2314ms
2016-10-26 17:12:24 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'SparkUI' on port 4040.
2016-10-26 17:12:24 INFO  [main] org.apache.spark.ui.SparkUI [Logging.scala:54] : Bound SparkUI to 0.0.0.0, and started at http://172.16.106.190:4040
2016-10-26 17:12:24 INFO  [main] org.apache.spark.executor.Executor [Logging.scala:54] : Starting executor ID driver on host localhost
2016-10-26 17:12:24 INFO  [main] org.apache.spark.util.Utils [Logging.scala:54] : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55106.
2016-10-26 17:12:24 INFO  [main] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] : Server created on 172.16.106.190:55106
2016-10-26 17:12:24 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registering BlockManager BlockManagerId(driver, 172.16.106.190, 55106)
2016-10-26 17:12:24 INFO  [dispatcher-event-loop-2] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] : Registering block manager 172.16.106.190:55106 with 912.3 MB RAM, BlockManagerId(driver, 172.16.106.190, 55106)
2016-10-26 17:12:24 INFO  [main] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : Registered BlockManager BlockManagerId(driver, 172.16.106.190, 55106)
2016-10-26 17:12:25 WARN  [main] org.apache.spark.SparkContext [Logging.scala:66] : Use an existing SparkContext, some configuration may not take effect.
2016-10-26 17:12:25 INFO  [main] o.a.spark.sql.hive.HiveSharedState [Logging.scala:54] : Warehouse path is 'file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse'.
2016-10-26 17:12:26 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0 stored as values in memory (estimated size 133.8 KB, free 912.2 MB)
2016-10-26 17:12:26 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_0_piece0 stored as bytes in memory (estimated size 14.9 KB, free 912.2 MB)
2016-10-26 17:12:26 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_0_piece0 in memory on 172.16.106.190:55106 (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:12:26 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 0 from json at SparkApplication.java:31
2016-10-26 17:12:27 INFO  [main] o.a.hadoop.mapred.FileInputFormat [FileInputFormat.java:253] : Total input paths to process : 1
2016-10-26 17:12:27 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: json at SparkApplication.java:31
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 0 (json at SparkApplication.java:31) with 2 output partitions
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 0 (json at SparkApplication.java:31)
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:31), which has no missing parents
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 912.2 MB)
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 912.1 MB)
2016-10-26 17:12:27 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_1_piece0 in memory on 172.16.106.190:55106 (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 1 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at SparkApplication.java:31)
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 0.0 with 2 tasks
2016-10-26 17:12:27 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:12:27 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5448 bytes)
2016-10-26 17:12:27 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 0.0 (TID 0)
2016-10-26 17:12:27 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 1.0 in stage 0.0 (TID 1)
2016-10-26 17:12:27 INFO  [Executor task launch worker-1] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:121+122
2016-10-26 17:12:27 INFO  [Executor task launch worker-0] org.apache.spark.rdd.HadoopRDD [Logging.scala:54] : Input split: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json:0+121
2016-10-26 17:12:27 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.tip.id is deprecated. Instead, use mapreduce.task.id
2016-10-26 17:12:27 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
2016-10-26 17:12:27 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
2016-10-26 17:12:27 INFO  [Executor task launch worker-0] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
2016-10-26 17:12:27 INFO  [Executor task launch worker-1] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.job.id is deprecated. Instead, use mapreduce.job.id
2016-10-26 17:12:27 INFO  [Executor task launch worker-0] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0). 1737 bytes result sent to driver
2016-10-26 17:12:27 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1). 1737 bytes result sent to driver
2016-10-26 17:12:27 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 1.0 in stage 0.0 (TID 1) in 230 ms on localhost (1/2)
2016-10-26 17:12:27 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 0.0 (TID 0) in 279 ms on localhost (2/2)
2016-10-26 17:12:27 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2016-10-26 17:12:27 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 0 (json at SparkApplication.java:31) finished in 0.298 s
2016-10-26 17:12:27 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 0 finished: json at SparkApplication.java:31, took 0.404264 s
2016-10-26 17:12:27 INFO  [main] org.apache.spark.sql.hive.HiveUtils [Logging.scala:54] : Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
2016-10-26 17:12:27 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
2016-10-26 17:12:28 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:589] : 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2016-10-26 17:12:28 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:289] : ObjectStore, initialize called
2016-10-26 17:12:28 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_0_piece0 on 172.16.106.190:55106 in memory (size: 14.9 KB, free: 912.3 MB)
2016-10-26 17:12:28 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_1_piece0 on 172.16.106.190:55106 in memory (size: 2.6 KB, free: 912.3 MB)
2016-10-26 17:12:29 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:370] : Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2016-10-26 17:12:30 INFO  [main] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] : Using direct SQL, underlying DB is DERBY
2016-10-26 17:12:30 INFO  [main] o.a.h.hive.metastore.ObjectStore [ObjectStore.java:272] : Initialized ObjectStore
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:663] : Added admin role in metastore
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:672] : Added public role in metastore
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:712] : No user is added in admin role, since config is empty
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_all_databases
2016-10-26 17:12:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_all_databases	
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_functions: db=default pat=*
2016-10-26 17:12:31 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/5f179a10-4899-4d20-9df3-791c9ddeacd5_resources
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/5f179a10-4899-4d20-9df3-791c9ddeacd5
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/5f179a10-4899-4d20-9df3-791c9ddeacd5
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/5f179a10-4899-4d20-9df3-791c9ddeacd5/_tmp_space.db
2016-10-26 17:12:31 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/1884bb9f-41ee-4744-92ef-a185cd73940e_resources
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/1884bb9f-41ee-4744-92ef-a185cd73940e
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created local directory: /var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/Benchun/1884bb9f-41ee-4744-92ef-a185cd73940e
2016-10-26 17:12:31 INFO  [main] o.a.h.hive.ql.session.SessionState [SessionState.java:641] : Created HDFS directory: /tmp/hive/Benchun/1884bb9f-41ee-4744-92ef-a185cd73940e/_tmp_space.db
2016-10-26 17:12:31 INFO  [main] o.a.s.s.hive.client.HiveClientImpl [Logging.scala:54] : Warehouse location for Hive client (version 1.2.1) is file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse
2016-10-26 17:12:32 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})
2016-10-26 17:12:32 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse, parameters:{})	
2016-10-26 17:12:32 ERROR [main] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] : AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:262)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:208)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:290)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:43)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:287)
	at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:249)
	at hx.stream.spark.SparkApplication.main(SparkApplication.java:31)

2016-10-26 17:12:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:12:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:12:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:12:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:12:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:12:32 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:12:32 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_2_piece0 in memory on 172.16.106.190:55106 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:12:32 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 2 from show at SparkApplication.java:34
2016-10-26 17:12:32 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:12:33 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 256.965829 ms
2016-10-26 17:12:33 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:34
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 1 (show at SparkApplication.java:34) with 1 output partitions
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 1 (show at SparkApplication.java:34)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:34), which has no missing parents
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3 stored as values in memory (estimated size 7.2 KB, free 912.1 MB)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.1 KB, free 912.1 MB)
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_3_piece0 in memory on 172.16.106.190:55106 (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 3 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at SparkApplication.java:34)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 1.0 with 1 tasks
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 1.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 1.0 (TID 2)
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 15.543198 ms
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2). 1410 bytes result sent to driver
2016-10-26 17:12:33 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 1.0 (TID 2) in 61 ms on localhost (1/1)
2016-10-26 17:12:33 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 1 (show at SparkApplication.java:34) finished in 0.061 s
2016-10-26 17:12:33 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 1 finished: show at SparkApplication.java:34, took 0.091666 s
2016-10-26 17:12:33 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.075497 ms
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<name: string>
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:12:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:12:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.0 MB)
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_4_piece0 in memory on 172.16.106.190:55106 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:12:33 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 4 from show at SparkApplication.java:36
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:12:33 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:36
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 2 (show at SparkApplication.java:36) with 1 output partitions
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 2 (show at SparkApplication.java:36)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:36), which has no missing parents
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5 stored as values in memory (estimated size 7.1 KB, free 912.0 MB)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.0 KB, free 912.0 MB)
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_5_piece0 in memory on 172.16.106.190:55106 (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 5 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SparkApplication.java:36)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 2.0 with 1 tasks
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 2.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 2.0 (TID 3)
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.312377 ms
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3). 1336 bytes result sent to driver
2016-10-26 17:12:33 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 2.0 (TID 3) in 24 ms on localhost (1/1)
2016-10-26 17:12:33 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 2 (show at SparkApplication.java:36) finished in 0.025 s
2016-10-26 17:12:33 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 2 finished: show at SparkApplication.java:36, took 0.036913 s
2016-10-26 17:12:33 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.331858 ms
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<id: string>
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:12:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6 stored as values in memory (estimated size 133.4 KB, free 911.9 MB)
2016-10-26 17:12:33 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.8 MB)
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_6_piece0 in memory on 172.16.106.190:55106 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:12:33 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 6 from describe at SparkApplication.java:37
2016-10-26 17:12:33 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:12:33 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: describe at SparkApplication.java:37
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Registering RDD 12 (describe at SparkApplication.java:37)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 3 (describe at SparkApplication.java:37) with 1 output partitions
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 4 (describe at SparkApplication.java:37)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List(ShuffleMapStage 3)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List(ShuffleMapStage 3)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:37), which has no missing parents
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7 stored as values in memory (estimated size 16.1 KB, free 911.8 MB)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.8 KB, free 911.8 MB)
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_7_piece0 in memory on 172.16.106.190:55106 (size: 7.8 KB, free: 912.2 MB)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 7 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[12] at describe at SparkApplication.java:37)
2016-10-26 17:12:33 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 3.0 with 1 tasks
2016-10-26 17:12:33 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 3.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5844 bytes)
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 3.0 (TID 4)
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 37.023529 ms
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 42.234711 ms
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 13.705748 ms
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 24.850088 ms
2016-10-26 17:12:33 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 16.898575 ms
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4). 1792 bytes result sent to driver
2016-10-26 17:12:34 INFO  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 3.0 (TID 4) in 292 ms on localhost (1/1)
2016-10-26 17:12:34 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ShuffleMapStage 3 (describe at SparkApplication.java:37) finished in 0.300 s
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : looking for newly runnable stages
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : running: Set()
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : waiting: Set(ResultStage 4)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : failed: Set()
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:37), which has no missing parents
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8 stored as values in memory (estimated size 16.8 KB, free 911.8 MB)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.8 MB)
2016-10-26 17:12:34 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_8_piece0 in memory on 172.16.106.190:55106 (size: 8.0 KB, free: 912.2 MB)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 8 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at describe at SparkApplication.java:37)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 4.0 with 1 tasks
2016-10-26 17:12:34 INFO  [dispatcher-event-loop-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0, ANY, 5190 bytes)
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 4.0 (TID 5)
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Getting 1 non-empty blocks out of 1 blocks
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] : Started 0 remote fetches in 8 ms
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 33.150062 ms
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 25.746443 ms
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 23.832374 ms
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 10.74993 ms
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5). 1954 bytes result sent to driver
2016-10-26 17:12:34 INFO  [task-result-getter-0] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 4.0 (TID 5) in 179 ms on localhost (1/1)
2016-10-26 17:12:34 INFO  [task-result-getter-0] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 4 (describe at SparkApplication.java:37) finished in 0.180 s
2016-10-26 17:12:34 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 3 finished: describe at SparkApplication.java:37, took 0.589654 s
2016-10-26 17:12:34 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 11.476585 ms
2016-10-26 17:12:34 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 9.368892 ms
2016-10-26 17:12:34 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 8.448577 ms
2016-10-26 17:12:34 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:12:34 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: select * from person
2016-10-26 17:12:34 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:12:34 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:12:34 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:12:34 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:12:34 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9 stored as values in memory (estimated size 133.4 KB, free 911.7 MB)
2016-10-26 17:12:34 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.7 MB)
2016-10-26 17:12:34 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_9_piece0 in memory on 172.16.106.190:55106 (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:12:34 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 9 from show at SparkApplication.java:41
2016-10-26 17:12:34 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:12:34 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: show at SparkApplication.java:41
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 4 (show at SparkApplication.java:41) with 1 output partitions
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 5 (show at SparkApplication.java:41)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:41), which has no missing parents
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10 stored as values in memory (estimated size 7.2 KB, free 911.6 MB)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 911.6 MB)
2016-10-26 17:12:34 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_10_piece0 in memory on 172.16.106.190:55106 (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 10 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[19] at show at SparkApplication.java:41)
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 5.0 with 1 tasks
2016-10-26 17:12:34 INFO  [dispatcher-event-loop-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5855 bytes)
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 5.0 (TID 6)
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:12:34 INFO  [Executor task launch worker-1] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6). 1410 bytes result sent to driver
2016-10-26 17:12:34 INFO  [task-result-getter-2] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 5.0 (TID 6) in 21 ms on localhost (1/1)
2016-10-26 17:12:34 INFO  [task-result-getter-2] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2016-10-26 17:12:34 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 5 (show at SparkApplication.java:41) finished in 0.024 s
2016-10-26 17:12:34 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 4 finished: show at SparkApplication.java:41, took 0.037672 s
2016-10-26 17:12:34 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:12:34 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: drop table if exists person
2016-10-26 17:12:34 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:12:34 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: `person`
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: drop_table : db=default tbl=person
2016-10-26 17:12:35 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=drop_table : db=default tbl=person	
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 252
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_2_piece0 on 172.16.106.190:55106 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 66
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 67
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_3_piece0 on 172.16.106.190:55106 in memory (size: 4.1 KB, free: 912.2 MB)
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_4_piece0 on 172.16.106.190:55106 in memory (size: 14.7 KB, free: 912.2 MB)
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 112
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 113
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_5_piece0 on 172.16.106.190:55106 in memory (size: 4.0 KB, free: 912.3 MB)
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_6_piece0 on 172.16.106.190:55106 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 158
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 159
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 160
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 161
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 162
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 163
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned shuffle 0
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_7_piece0 on 172.16.106.190:55106 in memory (size: 7.8 KB, free: 912.3 MB)
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_10_piece0 on 172.16.106.190:55106 in memory (size: 4.1 KB, free: 912.3 MB)
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-1] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_8_piece0 on 172.16.106.190:55106 in memory (size: 8.0 KB, free: 912.3 MB)
2016-10-26 17:12:35 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Removed broadcast_9_piece0 on 172.16.106.190:55106 in memory (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 253
2016-10-26 17:12:35 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner [Logging.scala:54] : Cleaned accumulator 254
2016-10-26 17:12:36 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] : deleting  file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:12:36 INFO  [main] o.a.h.c.Configuration.deprecation [Configuration.java:840] : io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2016-10-26 17:12:36 INFO  [main] o.a.hadoop.fs.TrashPolicyDefault [TrashPolicyDefault.java:92] : Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2016-10-26 17:12:36 INFO  [main] hive.metastore.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] : Deleted the diretory file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person
2016-10-26 17:12:42 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:12:42 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:12:42 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:12:42 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:12:42 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:12:42 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:12:42 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_tables: db=default pat=*
2016-10-26 17:12:42 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
2016-10-26 17:13:30 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 18.147671 ms
2016-10-26 17:13:57 INFO  [main] o.a.s.sql.execution.SparkSqlParser [Logging.scala:54] : Parsing command: person
2016-10-26 17:13:57 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:13:57 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:13:57 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:13:57 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:13:57 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:13:57 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:13:57 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_table : db=default tbl=person
2016-10-26 17:13:57 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_table : db=default tbl=person	
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:13:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11 stored as values in memory (estimated size 133.4 KB, free 912.2 MB)
2016-10-26 17:13:58 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_11_piece0 stored as bytes in memory (estimated size 14.7 KB, free 912.2 MB)
2016-10-26 17:13:58 INFO  [dispatcher-event-loop-2] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_11_piece0 in memory on 172.16.106.190:55106 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:13:58 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 11 from saveAsTable at SparkApplication.java:65
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] : Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:13:58 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:13:59 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: saveAsTable at SparkApplication.java:65
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 5 (saveAsTable at SparkApplication.java:65) with 1 output partitions
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 6 (saveAsTable at SparkApplication.java:65)
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:65), which has no missing parents
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12 stored as values in memory (estimated size 54.2 KB, free 912.1 MB)
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.4 KB, free 912.1 MB)
2016-10-26 17:13:59 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_12_piece0 in memory on 172.16.106.190:55106 (size: 20.4 KB, free: 912.3 MB)
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 12 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at saveAsTable at SparkApplication.java:65)
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 6.0 with 1 tasks
2016-10-26 17:13:59 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 6.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5948 bytes)
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 6.0 (TID 7)
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.key.class is deprecated. Instead, use mapreduce.job.output.key.class
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.h.c.Configuration.deprecation [Configuration.java:840] : mapred.output.value.class is deprecated. Instead, use mapreduce.job.output.value.class
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] : Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "age",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary age (UTF8);
  optional binary id (UTF8);
  optional binary name (UTF8);
}

       
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.hadoop.io.compress.CodecPool [CodecPool.java:150] : Got brand-new compressor [.snappy]
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] : Saved output of task 'attempt_201610261713_0006_m_000000_0' to file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person/_temporary/0/task_201610261713_0006_m_000000
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] o.a.s.mapred.SparkHadoopMapRedUtil [Logging.scala:54] : attempt_201610261713_0006_m_000000_0: Committed
2016-10-26 17:13:59 INFO  [Executor task launch worker-2] org.apache.spark.executor.Executor [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7). 1382 bytes result sent to driver
2016-10-26 17:13:59 INFO  [task-result-getter-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Finished task 0.0 in stage 6.0 (TID 7) in 764 ms on localhost (1/1)
2016-10-26 17:13:59 INFO  [task-result-getter-3] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 6.0, whose tasks have all completed, from pool 
2016-10-26 17:13:59 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 6 (saveAsTable at SparkApplication.java:65) finished in 0.765 s
2016-10-26 17:13:59 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 5 finished: saveAsTable at SparkApplication.java:65, took 0.785623 s
2016-10-26 17:14:00 INFO  [main] o.a.s.s.e.d.DefaultWriterContainer [Logging.scala:54] : Job job_201610261713_0000 committed.
2016-10-26 17:14:00 INFO  [main] o.a.s.s.e.c.CreateDataSourceTableUtils [Logging.scala:54] : Persisting data source relation `person` with a single input path into Hive metastore in Hive compatible format. Input path: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person.
2016-10-26 17:14:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:14:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:14:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: get_database: default
2016-10-26 17:14:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=get_database: default	
2016-10-26 17:14:00 INFO  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:746] : 0: create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477473240, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2016-10-26 17:14:00 INFO  [main] o.a.h.h.m.HiveMetaStore.audit [HiveMetaStore.java:371] : ugi=Benchun	ip=unknown-ip-addr	cmd=create_table: Table(tableName:person, dbName:default, owner:Benchun, createTime:1477473240, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:age, type:string, comment:null), FieldSchema(name:id, type:string, comment:null), FieldSchema(name:name, type:string, comment:null)], location:file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=FALSE, spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"age","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2016-10-26 17:14:00 WARN  [main] o.a.h.hive.metastore.HiveMetaStore [HiveMetaStore.java:1383] : Location: file:/Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/spark-warehouse/person specified for non-external table:person
2016-10-26 17:14:00 INFO  [main] hive.log [MetaStoreUtils.java:217] : Updating table stats fast for person
2016-10-26 17:14:00 INFO  [main] hive.log [MetaStoreUtils.java:219] : Updated size of table person to 785
2016-10-26 17:15:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruning directories with: 
2016-10-26 17:15:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Post-Scan Filters: 
2016-10-26 17:15:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pruned Data Schema: struct<age: string, id: string, name: string ... 1 more fields>
2016-10-26 17:15:30 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Pushed Filters: 
2016-10-26 17:15:30 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13 stored as values in memory (estimated size 133.4 KB, free 912.0 MB)
2016-10-26 17:15:30 INFO  [main] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.7 KB, free 911.9 MB)
2016-10-26 17:15:30 INFO  [dispatcher-event-loop-3] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_13_piece0 in memory on 172.16.106.190:55106 (size: 14.7 KB, free: 912.3 MB)
2016-10-26 17:15:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 13 from collectAsList at SparkApplication.java:71
2016-10-26 17:15:31 INFO  [main] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2016-10-26 17:15:31 INFO  [main] o.a.s.s.c.e.codegen.CodeGenerator [Logging.scala:54] : Code generated in 214.943189 ms
2016-10-26 17:15:31 INFO  [main] org.apache.spark.SparkContext [Logging.scala:54] : Starting job: collectAsList at SparkApplication.java:71
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Got job 6 (collectAsList at SparkApplication.java:71) with 1 output partitions
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Final stage: ResultStage 7 (collectAsList at SparkApplication.java:71)
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Parents of final stage: List()
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Missing parents: List()
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting ResultStage 7 (MapPartitionsRDD[27] at collectAsList at SparkApplication.java:71), which has no missing parents
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14 stored as values in memory (estimated size 13.8 KB, free 911.9 MB)
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.1 KB, free 911.9 MB)
2016-10-26 17:15:31 INFO  [dispatcher-event-loop-0] o.a.spark.storage.BlockManagerInfo [Logging.scala:54] : Added broadcast_14_piece0 in memory on 172.16.106.190:55106 (size: 6.1 KB, free: 912.2 MB)
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext [Logging.scala:54] : Created broadcast 14 from broadcast at DAGScheduler.scala:1012
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at collectAsList at SparkApplication.java:71)
2016-10-26 17:15:31 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Adding task set 7.0 with 1 tasks
2016-10-26 17:15:31 INFO  [dispatcher-event-loop-3] o.a.spark.scheduler.TaskSetManager [Logging.scala:54] : Starting task 0.0 in stage 7.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5941 bytes)
2016-10-26 17:15:31 INFO  [Executor task launch worker-3] org.apache.spark.executor.Executor [Logging.scala:54] : Running task 0.0 in stage 7.0 (TID 8)
2016-10-26 17:15:31 INFO  [Executor task launch worker-3] o.a.s.s.e.datasources.FileScanRDD [Logging.scala:54] : Reading File path: file:///Users/Benchun/Documents/Engineering/eclipse/big-architecture/stream-pipeline/src/main/resources/person.json, range: 0-243, partition values: [empty row]
2016-10-26 17:15:31 ERROR [Executor task launch worker-3] org.apache.spark.executor.Executor [Logging.scala:91] : Exception in task 0.0 in stage 7.0 (TID 8)
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at org.apache.spark.sql.Row$class.getInt(Row.scala:217)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(rows.scala:192)
	at hx.stream.spark.SparkApplication.lambda$1(SparkApplication.java:69)
	at hx.stream.spark.SparkApplication$$Lambda$10/1252060284.call(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-10-26 17:15:32 WARN  [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:66] : Lost task 0.0 in stage 7.0 (TID 8, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at org.apache.spark.sql.Row$class.getInt(Row.scala:217)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(rows.scala:192)
	at hx.stream.spark.SparkApplication.lambda$1(SparkApplication.java:69)
	at hx.stream.spark.SparkApplication$$Lambda$10/1252060284.call(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2016-10-26 17:15:32 ERROR [task-result-getter-1] o.a.spark.scheduler.TaskSetManager [Logging.scala:70] : Task 0 in stage 7.0 failed 1 times; aborting job
2016-10-26 17:15:32 INFO  [task-result-getter-1] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2016-10-26 17:15:32 INFO  [dag-scheduler-event-loop] o.a.s.scheduler.TaskSchedulerImpl [Logging.scala:54] : Cancelling stage 7
2016-10-26 17:15:32 INFO  [dag-scheduler-event-loop] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : ResultStage 7 (collectAsList at SparkApplication.java:71) failed in 0.066 s
2016-10-26 17:15:32 INFO  [main] o.a.spark.scheduler.DAGScheduler [Logging.scala:54] : Job 6 failed: collectAsList at SparkApplication.java:71, took 0.077157 s
2016-10-26 17:16:24 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Invoking stop() from shutdown hook
2016-10-26 17:16:24 INFO  [Thread-1] o.s.jetty.server.ServerConnector [AbstractConnector.java:306] : Stopped ServerConnector@2459319c{HTTP/1.1}{0.0.0.0:4040}
2016-10-26 17:16:24 INFO  [Thread-1] org.apache.spark.ui.SparkUI [Logging.scala:54] : Stopped Spark web UI at http://172.16.106.190:4040
2016-10-26 17:16:24 INFO  [dispatcher-event-loop-0] o.a.s.MapOutputTrackerMasterEndpoint [Logging.scala:54] : MapOutputTrackerMasterEndpoint stopped!
2016-10-26 17:16:24 INFO  [Thread-1] o.a.s.storage.memory.MemoryStore [Logging.scala:54] : MemoryStore cleared
2016-10-26 17:16:24 INFO  [Thread-1] o.apache.spark.storage.BlockManager [Logging.scala:54] : BlockManager stopped
2016-10-26 17:16:24 INFO  [Thread-1] o.a.s.storage.BlockManagerMaster [Logging.scala:54] : BlockManagerMaster stopped
2016-10-26 17:16:24 INFO  [dispatcher-event-loop-3] o.a.s.s.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [Logging.scala:54] : OutputCommitCoordinator stopped!
2016-10-26 17:16:24 INFO  [Thread-1] org.apache.spark.SparkContext [Logging.scala:54] : Successfully stopped SparkContext
2016-10-26 17:16:24 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Shutdown hook called
2016-10-26 17:16:24 INFO  [Thread-1] o.a.spark.util.ShutdownHookManager [Logging.scala:54] : Deleting directory /private/var/folders/_z/4sz05tlx0c9_lmn2lplzh3w00000gn/T/spark-f3dd1a7e-6025-4cb1-a10e-3be51f99c246
